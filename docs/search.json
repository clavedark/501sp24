[
  {
    "objectID": "probability24.html",
    "href": "probability24.html",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns.\n\n\n\n\nProbability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value).\n\n\n\n\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita.\n\n\n\n\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely.\n\n\n\n\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war.\n\n\n\n\nThese four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information.\n\n\n\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24.html#why-probability-distributions",
    "href": "probability24.html#why-probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns."
  },
  {
    "objectID": "probability24.html#probability-distributions",
    "href": "probability24.html#probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Probability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?"
  },
  {
    "objectID": "probability24.html#things-we-want-to-know-about-x",
    "href": "probability24.html#things-we-want-to-know-about-x",
    "title": "Basic probability",
    "section": "",
    "text": "\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value)."
  },
  {
    "objectID": "probability24.html#types-of-variables",
    "href": "probability24.html#types-of-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Variables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "probability24.html#discrete-variables",
    "href": "probability24.html#discrete-variables",
    "title": "Basic probability",
    "section": "",
    "text": "May be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "probability24.html#continuous-variables",
    "href": "probability24.html#continuous-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Can be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "probability24.html#levels-of-measurement",
    "href": "probability24.html#levels-of-measurement",
    "title": "Basic probability",
    "section": "",
    "text": "These four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "probability24.html#levels-of-measurement-and-models",
    "href": "probability24.html#levels-of-measurement-and-models",
    "title": "Basic probability",
    "section": "",
    "text": "In general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24.html#pdf-and-cdf",
    "href": "probability24.html#pdf-and-cdf",
    "title": "Basic probability",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nProbability distributions can be described by\n\nProbability Density Function (PDF) which maps \\(X\\) onto the probability space describing the probabilities of every value of \\(X\\), \\(x\\).\nCumulative Distribution Function (CDF) which maps \\(X\\) onto the probability space describing the probability \\(X\\) is less than some value, \\(x\\)."
  },
  {
    "objectID": "probability24.html#pdf-density",
    "href": "probability24.html#pdf-density",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nDefinition\n\n\n\nThe Probability Density Function or PDF describes the probability a random variable takes on a particular value, and it does so for all values of that random variable."
  },
  {
    "objectID": "probability24.html#pdf-density-1",
    "href": "probability24.html#pdf-density-1",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nExample\n\n\n\nSuppose we toss a fair coin 1 time and want to describe the probability distribution of the outcome, \\(Y\\) which is a random variable. The possible outcomes are heads and tails, and the probability of each is .5. This is the PDF because it describes the probabilities associated with all possible outcomes of \\(Y\\)."
  },
  {
    "objectID": "probability24.html#pdf-density-2",
    "href": "probability24.html#pdf-density-2",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\nWhile establishing the probability of a value of a discrete variable is possible, establishing the probability of a particular value of a continuous variable is not.\nInstead, the continuous PDF describes the instantaneous rate of change in \\(Pr(X=x)\\) for every value of \\(x\\)."
  },
  {
    "objectID": "probability24.html#pdf-plots",
    "href": "probability24.html#pdf-plots",
    "title": "Basic probability",
    "section": "PDF plots",
    "text": "PDF plots"
  },
  {
    "objectID": "probability24.html#cdf",
    "href": "probability24.html#cdf",
    "title": "Basic probability",
    "section": "CDF",
    "text": "CDF\n\n\n\n\n\n\nDefinition\n\n\n\nFor a random variable, \\(Y\\), the probability \\(Y\\) is less than or equal to some particular value, \\(y\\) in the range of \\(Y\\) defines the cumulative density function.\nFor a continuous variable:\n\\[P(Y \\leq j) =\\int\\limits_{-\\infty}^{j} f(X)dX\\]\nFor a discrete variable:\n\\[P(Y \\leq j) = \\sum\\limits_{y\\leq j} P(Y=y)\n= 1- \\sum\\limits_{y&gt; j} P(Y=y)\\]"
  },
  {
    "objectID": "probability24.html#cdf-plots",
    "href": "probability24.html#cdf-plots",
    "title": "Basic probability",
    "section": "CDF plots",
    "text": "CDF plots"
  },
  {
    "objectID": "probability24.html#notation",
    "href": "probability24.html#notation",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\\(f(x)\\) denotes the PDF of \\(x\\).\n\\(F(x)\\) denotes the CDF of \\(x\\).\nSubstitute either the appropriate name, symbol, or function for \\(F\\) or \\(f\\) to indicate the distribution.\nLower case Greek letters denote PDFs: e.g. \\(f(x)= \\phi(x)\\) denotes the normal PDF.\nUpper case Greek letters denote CDFs: e.g. \\(F(x)=\\Phi(x)\\) denotes the normal CDF."
  },
  {
    "objectID": "probability24.html#notation-1",
    "href": "probability24.html#notation-1",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nExample\n\n\n\n\\(x\\) is distributed Normal, \\(N(\\mu, \\sigma^{2})\\):\n\\[F(x) = \\Phi_{\\mu, \\sigma^{2}}(x)  =    \\int  \\phi_{\\mu, \\sigma^{2}} (x) f(x)d(x) \\]\n\\[ f(x) = \\phi_{\\mu, \\sigma^{2}}(x) =\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{(x-\\mu)^{2}}{2 \\sigma^{2}} \\right) \\]\nNote the first derivative of the CDF is the PDF; the integral of the PDF is the CDF."
  },
  {
    "objectID": "probability24.html#bernoulli",
    "href": "probability24.html#bernoulli",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nSuppose a binary variable \\(x\\) that takes on only the values of zero and one (\\(x \\in {0, 1}\\)):\n\\[\nPr(X=1)=\\pi\\nonumber \\\\\nPr(x=0)= 1-Pr(x=1) \\\\\n= 1-\\pi   \n\\]"
  },
  {
    "objectID": "probability24.html#bernoulli-1",
    "href": "probability24.html#bernoulli-1",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe PDF is:\n\\[ f(x) = \\\\\n          \\pi    ~~~~~~~~ ~ ~ ~~~~ \\text{if } x=1\\\\\n         1-\\pi   ~ ~ ~ ~~~~\\text{if } x=0\n     \\]\nor\n\\[f(x) = \\pi^{x}(1-\\pi)^{1-x} \\]"
  },
  {
    "objectID": "probability24.html#bernoulli-2",
    "href": "probability24.html#bernoulli-2",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe CDF is:\n\\[F(x) =\\sum_{x} f(x) \\]\nand the expected value is\n\\[\nE(x) =\\sum_{x}x f(x)    \\\\\n= (1)(\\pi)+(0)(1-\\pi)   \\\\\n= \\pi\n\\]"
  },
  {
    "objectID": "probability24.html#binomial",
    "href": "probability24.html#binomial",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nBernoulli is important because it is the foundation for a lot of other distributions, including the binomial distribution. The binomial describes the success probability function (where \\(x=1\\) is a “success”) from a set of \\(n\\) independent Bernoulli trials. So, the binomial is the probability of successes (“ones”) in \\(n\\) independent Bernoulli trials with identical probabilities, \\(\\pi\\).\n\\(~\\)\nThere are two essential parts to the binomial PDF - the probability of success, and the number of ways a success can occur."
  },
  {
    "objectID": "probability24.html#binomial-1",
    "href": "probability24.html#binomial-1",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe probability of success is the Bernoulli probability:\n\\[\nf(x) = \\pi^{x}(1-\\pi)^{1-x}\n\\]\nand the number of ways success can occur (called “n-tuples”) is\n\\[  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} = \\frac{n!}{x!(n-x)!}\n\\]\nNotice the notation for the n-tuple."
  },
  {
    "objectID": "probability24.html#binomial-2",
    "href": "probability24.html#binomial-2",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe PDF combines these:\n\\[\nf(x)=\n  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} \\pi^{x}(1-\\pi)^{n-x}\n\\]\nThere are \\(n\\) trials, \\(x\\) is the number of successes, and \\(n-x\\) is the number of failures. Each event in the n-tuple arises with the Bernoulli probability."
  },
  {
    "objectID": "probability24.html#binomial-3",
    "href": "probability24.html#binomial-3",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are going to toss a fair coin 4 times and want to know how many ways we can have heads come up twice.\n\\[\n\\frac{4!}{2!(4-2)!} = 6\n\\] Now, suppose we want to know the probability exactly 2 of those 4 tosses is heads.\n\\[\nPr(x=2) =\\frac{4!}{2!(4-2)!} (.5)^{2}(.5)^{2} \\\\\n= 6(0.0625) \\\\\n= 0.375\n\\]"
  },
  {
    "objectID": "probability24.html#binomial-4",
    "href": "probability24.html#binomial-4",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nHere’s another example: suppose we flip 5 fair coins once each; what is the PDF of the number of heads we expect? In other words, what is the probability associated with each possible number of successes (heads), from 0 to 5?\n\\[\nP(x=0)  =  \\begin{pmatrix}\n    5  \\\\\n    0\n  \\end{pmatrix} \\cdot  (.5)^{0} \\cdot (.5) ^{5} = 1 \\cdot 1 \\cdot 0.03125=0.03125\\\\\n  \\] \\[\nP(x=1) =  \\begin{pmatrix}\n    5  \\\\\n    1\n  \\end{pmatrix} \\cdot (.5)^{1}\\cdot (.5)^{4} = 5 \\cdot .5 \\cdot 0.0625=0.15625 \\\\\n  \\] \\[P(x=2) =  \\begin{pmatrix}\n    5  \\\\\n    2\n  \\end{pmatrix} \\cdot (.5)^{2}\\cdot (.5)^{3} = 10 \\cdot .25 \\cdot 0.125=0.3125 \\\\\n\\] \\[\n\\vdots \\vdots \\vdots\n\\]"
  },
  {
    "objectID": "probability24.html#binomial-family-distributions",
    "href": "probability24.html#binomial-family-distributions",
    "title": "Basic probability",
    "section": "Binomial family distributions",
    "text": "Binomial family distributions\n\nthe geometric distribution describes repeated Bernoulli trials with probability of success \\(\\pi\\), until the first success.\nthe negative binomial describes the number of Bernoulli failures prior to the first success - it can be thought of as a counting process up to the first success.\nthe poisson describes Bernoulli trials where \\(\\pi\\) for any particular trial is very small."
  },
  {
    "objectID": "probability24.html#the-normal-distribution",
    "href": "probability24.html#the-normal-distribution",
    "title": "Basic probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe Normal distribution is the most widely used distribution in the social sciences.\n\nThe normal seems to “fit” a lot of the variables social scientists measure and use in models.\nEstimation techniques we often employ assume the disturbance term is normally distributed; one result is the coefficients are either normally distributed or t-distributed."
  },
  {
    "objectID": "probability24.html#normal-pdf",
    "href": "probability24.html#normal-pdf",
    "title": "Basic probability",
    "section": "Normal PDF",
    "text": "Normal PDF\nThe normal PDF: \\[\nPr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]\n\\]\nwhere two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\) describe the location and shape of the distribution, the mean and variance respectively; we indicate a normally distributed variable and its parameters as\n\\[\nY \\sim \\text{Normal}(\\mu,\\sigma^{2}) \\nonumber\n\\]\n\n\nDescribe these dataNormal?\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "probability24.html#standard-normal",
    "href": "probability24.html#standard-normal",
    "title": "Basic probability",
    "section": "Standard Normal",
    "text": "Standard Normal\nA useful special case of the Normal is the standard normal, \\(z \\sim \\text{Normal}(0,1)\\). The PDF for the standard normal is given by\n\\[\n\\phi(z)=\\frac{1}{\\sqrt{2 \\pi }} exp \\left[\\frac{-z^{2}}{2}\\right] \\nonumber\n\\]\nwhere the parameters themselves drop out since we’d be subtracting a mean of zero and dividing/multiplying by a variance of 1. The standard normal CDF is denoted \\(\\Phi(z)\\); the standard normal PDF is denoted \\(\\phi(z)\\)."
  },
  {
    "objectID": "probability24.html#normal-pdfs-different-moments",
    "href": "probability24.html#normal-pdfs-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs different moments",
    "text": "Normal PDFs different moments"
  },
  {
    "objectID": "probability24.html#models",
    "href": "probability24.html#models",
    "title": "Basic probability",
    "section": "Models",
    "text": "Models\n\nProbability models include unobserved disturbances; we make assumptions about the distributions of those errors, \\(\\epsilon\\).\nWhat we assume about the unobservables is always informed by what we know about the observables, mainly \\(y\\).\nA useful way to describe or summarize \\(y\\) is to characterize its observed distribution.\nThe distribution of \\(y\\) informs our assumption about the distribution of \\(\\epsilon\\)."
  },
  {
    "objectID": "probability24.html#why-this-matters-to-ols",
    "href": "probability24.html#why-this-matters-to-ols",
    "title": "Basic probability",
    "section": "Why this matters to OLS",
    "text": "Why this matters to OLS\nLinear regression is such an inferential model; we have a number of sources of uncertainty. We represent that uncertainty in the model via the disturbance term, \\(\\epsilon\\).\n\\(~\\)\nIn order to know things about \\(\\epsilon\\) and other parts of the model, we assume that the disturbances are normally distributed; \\(y\\) should be normal too."
  },
  {
    "objectID": "probability24.html#normality-and-centrality",
    "href": "probability24.html#normality-and-centrality",
    "title": "Basic probability",
    "section": "Normality and Centrality",
    "text": "Normality and Centrality\nWe rely on stuff being normally distributed, and central tendency being meaningful. The Central Limit Theorem facilitates this."
  },
  {
    "objectID": "probability24.html#central-limit-theorem",
    "href": "probability24.html#central-limit-theorem",
    "title": "Basic probability",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ {\\sum\\limits_{n \\rightarrow \\infty} (\\bar{X_i})} / {n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "probability24.html#clt---why-is-this-valuable",
    "href": "probability24.html#clt---why-is-this-valuable",
    "title": "Basic probability",
    "section": "CLT - Why is this valuable?",
    "text": "CLT - Why is this valuable?\n\nIf we assume repeated sampling and/or infinitely large samples, we can assume normality.\nWe know the properties of the Normal intimately well. We can use this knowledge to evaluate where some value of \\(x\\) lies on the Normal CDF, what the probability less than that value is - we can figure out what the probability of observing that value is (on the PDF).\nAs we’ll see, \\(\\widehat{\\beta}\\) is normally distributed in the OLS model (it is in MLE as well). The CLT and what we know about normality facilitate inference."
  },
  {
    "objectID": "probability24.html#inference",
    "href": "probability24.html#inference",
    "title": "Basic probability",
    "section": "Inference",
    "text": "Inference\nInference is our effort to measure and characterize our uncertainty about the model and its parameters.\n\nuncertainty is the most important thing we estimate in inferential models.\ncharacterizing uncertainty relies on probability theory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PLSC 501 - Spring 2024",
    "section": "",
    "text": "This is the course website for PLSC 501 - it is a data science course focused on data management, coding in R, and learning basic regression techniques in OLS and MLE.\n\nSyllabus\nSlides\nCode"
  },
  {
    "objectID": "matrix24.html",
    "href": "matrix24.html",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model.\n\n\n\n\nA single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A.\n\n\n\nThe dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar.\n\n\n\n\nA is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]\n\n\n\n\n\n\nA rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3).\n\n\n\nA vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24.html#matrix-notation",
    "href": "matrix24.html#matrix-notation",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model."
  },
  {
    "objectID": "matrix24.html#matrices",
    "href": "matrix24.html#matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A."
  },
  {
    "objectID": "matrix24.html#dimensions",
    "href": "matrix24.html#dimensions",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "The dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar."
  },
  {
    "objectID": "matrix24.html#symmetric-matrices",
    "href": "matrix24.html#symmetric-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#rectangular-matrices",
    "href": "matrix24.html#rectangular-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3)."
  },
  {
    "objectID": "matrix24.html#vectors",
    "href": "matrix24.html#vectors",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24.html#transposition",
    "href": "matrix24.html#transposition",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nThe transpose of a matrix A is the matrix flipped on its side such that its rows become columns and columns become rows; the transpose of A is denoted \\({\\mathbf A'}\\) and is referred to as “\\({\\mathbf A}\\) transpose.”\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right]\n~~~~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5 & 2\\\\\n\\end{array} \\right] \\]\nso \\({\\mathbf X}\\), a (3,2) matrix, becomes \\({\\mathbf X'}\\), a (2,3) matrix."
  },
  {
    "objectID": "matrix24.html#transposition-1",
    "href": "matrix24.html#transposition-1",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nIf \\({\\mathbf A}\\) is symmetric, then \\({\\mathbf A}={\\mathbf A'}\\) – take a look at the following symmetric matrix and you’ll see why:\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#transposition-2",
    "href": "matrix24.html#transposition-2",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nAlso note that the transpose of a transposed matrix results in the original matrix: \\((\\mathbf{X}')'=\\mathbf{X}\\).\nTransposing a row vector results in a column vector and vice versa:\n\\[\\mathbf{x} =  \\left[\n\\begin{array}{c}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n\\end{array} \\right]\n~~~~~~~~~\n\\mathbf{x'} =  \\left[\n\\begin{array}{cccc}\n-1 & -9 &4 &16\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#trace-of-a-matrix",
    "href": "matrix24.html#trace-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nThe trace of a matrix is the sum of the diagonal elements of a square matrix, and is denoted \\(tr(\\mathbf{A})=a_{11}+a_{22}+a_{33}\\ldots+a_{nn} = \\sum\\limits_{i=1}^{n}a_{ii}\\)\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nsuch that\n\\(tr(\\mathbf{A})=2+5+2=9\\)"
  },
  {
    "objectID": "matrix24.html#addition-subtraction",
    "href": "matrix24.html#addition-subtraction",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\n\nAddition and subtraction of matrices is analogous to the same scalar operations, but depend on the conformatibility of the matrices to be added or subtracted.\nTwo matrices are conformable for addition or subtraction iff they are of the same order.\nNote that conformability means something different for addition/subtraction than for multiplication."
  },
  {
    "objectID": "matrix24.html#addition-subtraction-1",
    "href": "matrix24.html#addition-subtraction-1",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider the following (2,2) matrices and the problem \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{C}\\):\n\\[ \\left[\n\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21}& a_{22} \\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{cc}\nb_{11} & b_{12}\\\\\nb_{21}& b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21}& a_{22}+b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\nc_{11} & c_{12}\\\\\nc_{21}& c_{22} \\\\\n\\end{array} \\right] \\]\nAddition and subtraction are only possible among matrices that share the same order; otherwise, they are nonconformable."
  },
  {
    "objectID": "matrix24.html#addition-subtraction-2",
    "href": "matrix24.html#addition-subtraction-2",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider a second example of addition, and note that subtraction would follow directly:\n\\[ \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{ccc}\n1 &5 &7\\\\\n3& 7& 1\\\\\n2& 4& 9\\\\\n\\end{array} \\right]\n=\n  \\left[\n\\begin{array}{ccc}\n3 &9 &15\\\\\n7& 12& 4\\\\\n10& 7& 11\\\\\n\\end{array} \\right] \\]\nMatrix addition adheres to the commutative and associative properties such that:\n\\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) (Commutative), and \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}= \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) (Associative)."
  },
  {
    "objectID": "matrix24.html#multiplication",
    "href": "matrix24.html#multiplication",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\nLet’s begin by multiplying a scalar and a matrix - the product is a matrix whose elements are the scalar multiplied by each element of the original matrix, so:\n\\[ \\beta\n\\left[\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\n\\beta x_{11} & \\beta x_{12}\\\\\n\\beta x_{21}&  \\beta x_{22} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#multiplication-1",
    "href": "matrix24.html#multiplication-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\n\nIn order to multiply two matrices (or vectors), they must be conformable for multiplication.\nTwo matrices are conformable for multiplication iff the number of columns in the first matrix is equal to the number of rows in the second matrix. That is, \\(\\mathbf{A_{i,j}}\\) and \\(\\mathbf{B_{j,k}}\\).\nAn easy way to think of this is to write the dimensions of the two matrices, (i,j),(j,k) - the inner dimensions are the same, so the matrix is conformable for multiplication - moreover, the outer dimensions (i,k) give the dimension of the product matrix."
  },
  {
    "objectID": "matrix24.html#multiplication---inner-product",
    "href": "matrix24.html#multiplication---inner-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\n\nTo illustrate multiplication, let’s start with a column vector, \\(\\mathbf{e}\\) whose order is (N,1) and take its inner product.\nThe inner product of a column vector is the transpose of the column vector (thus, a row vector) post-multiplied by the column vector, so \\({\\mathbf e'\\mathbf e}\\). The inner product is a scalar."
  },
  {
    "objectID": "matrix24.html#multiplication---inner-product-1",
    "href": "matrix24.html#multiplication---inner-product-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\nWhen we transpose the column vector \\({\\mathbf e}\\) of (N,1) order, we get a row vector \\({\\mathbf e'}\\) of (1,N) order. Inner dimensions match, so they are conformable.\n\\[\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n~~~~~\n\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right] \\] \\[=\n(-1 \\cdot -1)+(-9 \\cdot -9)+(4 \\cdot 4)+ (16 \\cdot 16)+ (-10 \\cdot -10) = 454\\]"
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing",
    "href": "matrix24.html#least-squares-foreshadowing",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nNote the inner product of a column vector is a scalar; the inner product of \\(\\mathbf{e}\\), is the sum of the squares of \\(\\mathbf{e}\\).\n\\[\\mathbf{e'e}= e_{1}e_{1}+e_{2}e_{2}+\\ldots e_{N}e_{N} = \\sum\\limits_{i=1}^{N}e_{i}^{2}\\]"
  },
  {
    "objectID": "matrix24.html#multiplication---outer-product",
    "href": "matrix24.html#multiplication---outer-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - outer product",
    "text": "Multiplication - outer product\nThe outer product of a column vector is the transpose of the column vector (thus, a row vector) pre-multiplied by the column vector, so \\({\\mathbf e\\mathbf e'}\\). The outer product is an (N,N) matrix.\n\\[\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right]\n\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n\\]\nLet’s multiply \\({\\mathbf e\\mathbf e'}\\), a column vector of (5,1) by a row vector of (1,5); we’ll obtain a square matrix of (5,5)."
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing-1",
    "href": "matrix24.html#least-squares-foreshadowing-1",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nLet \\(\\mathbf{e}\\) represent the residuals or errors in our regression. The outer product\n\\[{\\mathbf e\\mathbf e'} =  \\left[\n\\begin{array}{rrrrr}\n1 & 9 &-4 &-16 &10\\\\\n9 & 81 &-36 &-144 &90\\\\\n-4 & -36 &16 &64 &-40\\\\\n-16 & -144 &64 &256 &-160\\\\\n10 & 90 &-40 &-160 &100\\\\\n\\end{array} \\right] \\]\nis the variance-covariance matrix of \\(\\mathbf{e}\\). The squares on the main diagonal (which sums to the sum of squares) and the symmetry of the off-diagonal elements."
  },
  {
    "objectID": "matrix24.html#inverting-matrices-1",
    "href": "matrix24.html#inverting-matrices-1",
    "title": "Matrix algebra basics",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\n\nYou’ll have noticed that division has been conspicuously absent so far. In scalar algebra, we sometimes represent division in fractions, \\(\\frac{1}{2}\\).\nAnother way to represent the same quantity is \\(2^{-1}\\), or the inverse of 2. Of course, in scalar algebra, a number multiplied by its inverse equals 1, e.g., \\(2 \\cdot 2^{-1}=1\\) or \\(2 \\cdot \\frac{1}{2}=1\\). So, if we are given \\(4x=1\\) and want to solve for \\(x\\) what we really want to know is “what is the inverse of 4 such that \\(4 \\cdot 4^{-1}=1\\)?” We consider matrices in a similar manner."
  },
  {
    "objectID": "matrix24.html#inverting-square-matrices",
    "href": "matrix24.html#inverting-square-matrices",
    "title": "Matrix algebra basics",
    "section": "Inverting Square Matrices",
    "text": "Inverting Square Matrices\nImagine a square matrix, \\(\\mathbf{X}\\) and its inverse, denoted \\({\\mathbf{X^{-1}}}\\) (read as “X inverse”). Analogous to scalar algebra, a matrix multiplied by its inverse is equal to the identity matrix, \\(\\mathbf{I}\\).\nSo in order to find \\({\\mathbf{X^{-1}}}\\), we must find the matrix that, when multiplied by \\(\\mathbf{X}\\), produces \\(\\mathbf{I}\\). Thus, for a square matrix, its inverse (if it exists) is such that\n\\[\\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X^{-1}} \\mathbf{X}=\\mathbf{I}\\]\nNotice the commutative property at work here - this is because \\(\\mathbf{X}\\) is square, such that \\(n=m\\), so \\(\\mathbf{X}\\) and \\({\\mathbf{X^{-1}}}\\) have the same dimensions."
  },
  {
    "objectID": "matrix24.html#determinant",
    "href": "matrix24.html#determinant",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nAn important characteristic of square matrices is the determinant. The determinant, denoted \\(|X|\\), is a scalar; every square matrix has one. We evaluate the determinant by examining the cross-products of the matrice’s elements. This is simple in the 2x2 matrix, less so in larger matrices.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right| = x_{11}x_{22}-x_{12}x_{21}\\]"
  },
  {
    "objectID": "matrix24.html#determinant-1",
    "href": "matrix24.html#determinant-1",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nIn the 2x2 matrix, the determinant is the cross-product of the main diagonals minus the cross-product of the off-diagonals.\n\\[ |\\mathbf{X_{1}}|=\\left|\n\\begin{array}{cc}\n2 & 4\\\\\n6& 3 \\\\\n\\end{array} \\right| = 2 \\cdot 3-4 \\cdot 6 = -18\n\\]\nThe determinant is -18; \\(|\\mathbf{X_{1}}|=-18\\)."
  },
  {
    "objectID": "matrix24.html#determinant-2",
    "href": "matrix24.html#determinant-2",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nThis matrix has a determinant of zero - this is an important case.\n\\[ |\\mathbf{X_{2}}|=\\left|\n\\begin{array}{cc}\n3 & 6\\\\\n2& 4 \\\\\n\\end{array} \\right| = 3 \\cdot 4-6 \\cdot 2 =0\n\\] A matrix with determinant zero is singular. A singluar matrix has no inverse."
  },
  {
    "objectID": "matrix24.html#singular-matrices",
    "href": "matrix24.html#singular-matrices",
    "title": "Matrix algebra basics",
    "section": "Singular matrices",
    "text": "Singular matrices\nThere are some important conditions that will produce a determinant of zero and thus a singular matrix:\n\nIf all the elements of any row or column of a matrix are equal to zero, then the determinant is zero.\nIf two rows or columns of a matrix are identical, the determinant is zero.\nIf a row or column of a matrix is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing-2",
    "href": "matrix24.html#least-squares-foreshadowing-2",
    "title": "Matrix algebra basics",
    "section": "Least Squares Foreshadowing",
    "text": "Least Squares Foreshadowing\nFor the linear model in least squares, this is important because computing estimates of \\(\\beta\\) requires inverting a matrix. Let’s derive \\(\\beta\\) in matrix notation to see how:\nThe estimated model is:\n\\[\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}}\\] Minimizing \\(\\widehat{\\mathbf{\\epsilon}}'\\widehat{\\mathbf{\\epsilon}}\\) (skipping the math for now), we get\n\\[(\\mathbf{X'X}) \\widehat{\\beta}=\\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares",
    "href": "matrix24.html#foreshadowing-least-squares",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThe matrix \\((\\mathbf{X'X})\\) gives the sums of squares (on the main diagonal) and the sums of cross products (in the off-diagonals) of all the \\(\\mathbf{X}\\) variables; the matrix is symmetric. Since \\(\\beta\\) is the unknown vector in this equation, solve for \\(\\beta\\) by dividing both sides by \\((\\mathbf{X'X})\\) - in matrix terms, we premultiply each side by \\((\\mathbf{X'X)^{-1}}\\):\n\\[(\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) \\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares-1",
    "href": "matrix24.html#foreshadowing-least-squares-1",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nAs we know, \\((\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) = \\mathbf{I}\\). So,\n\\[\\mathbf{I}\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\] or \\[\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]\nEstimating \\(\\widehat{\\beta}\\) requires inverting \\(\\mathbf{(X'X)}\\) If the matrix \\(\\mathbf{(X'X)}\\) is singular, then \\(\\mathbf{(X'X)^{-1}}\\) does not exist, and we cannot estimate \\(\\widehat{\\beta}\\). Least Squares fails."
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares-2",
    "href": "matrix24.html#foreshadowing-least-squares-2",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThinking specifically of the \\(\\mathbf{X}\\) variables in the model, any of these conditions produce perfect collinearity - estimation fails because the matrix can’t invert.\n\nIf all the elements of any row or column of \\(\\mathbf{X}\\) are equal to zero, then the determinant is zero.\nIf two rows or columns of \\(\\mathbf{X}\\) are identical, the determinant is zero.\nIf a row or column of \\(\\mathbf{X}\\) is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24.html#examples",
    "href": "matrix24.html#examples",
    "title": "Matrix algebra basics",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2Example 3\n\n\n\\({\\mathbf X}\\) below has a row of zeros; compute the determinant using the difference of cross-products.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n0 & 0\\\\\n19& 12 \\\\\n\\end{array} \\right| = 0 \\cdot 12-0 \\cdot 19 = 0 \\]\nYou can see in \\({\\mathbf X}\\) that because all the cross-products involve multiplying by zero, the determinant will always be zero; this applies to larger matrices as well.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n5 & 7\\\\\n5& 7 \\\\\n\\end{array} \\right| = 5 \\cdot 7-5 \\cdot 7 =0 \\]\nIn \\({\\mathbf X}\\) it’s easy to see that the cross-products will always be equal to one another if the rows or columns are identical, and the difference between identical values is zero.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n64 & 48\\\\\n16& 12 \\\\\n\\end{array} \\right| = 64 \\cdot 12-48 \\cdot 16 = 768-768=0\\]\nFinally \\({\\mathbf X}\\) shows that if one row or column is a linear combination of other rows or columns, the determinant will be zero; in that matrix, the top row is 4 times the bottom row."
  },
  {
    "objectID": "matrix24.html#inversion",
    "href": "matrix24.html#inversion",
    "title": "Matrix algebra basics",
    "section": "Inversion",
    "text": "Inversion\nThe matrix we need to invert, \\(\\mathbf{X'X}\\) is rectangular, so inversion is more complex. I’m going to illustrate the method of cofactor expansion - the purpose here is to give you an idea what software does every time you estimate an OLS model."
  },
  {
    "objectID": "matrix24.html#minor-of-a-matrix",
    "href": "matrix24.html#minor-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Minor of a matrix",
    "text": "Minor of a matrix\nThe minor of a matrix is the determinant of a submatrix created by deleting the \\(i\\)th row and the \\(j\\)th column of the full matrix, where the minor is denoted by the element where the deleted rows intersect.\n\\[ \\mathbf{A} =  \\left[\n\\begin{array}{ccc}\na_{11} & a_{12} &a_{13}\\\\\na_{21}& a_{22} &a_{23}\\\\\na_{31} &a_{32} & a_{33}\\\\\n\\end{array} \\right] \\] \\[\\text{so the minor of } a_{11} \\text{ is }\n|\\mathbf{M_{11}}| =  \\left|\n\\begin{array}{cc}\na_{22} &a_{23}\\\\\na_{32} & a_{33}\\\\\n\\end{array} \\right|  =  a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\]"
  },
  {
    "objectID": "matrix24.html#cofactor-matrix",
    "href": "matrix24.html#cofactor-matrix",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nThe cofactor of an element (\\(c_{ij}\\)) is the minor with a positive or negative sign depending on whether \\(i+j\\) is odd (negative) or even (positive). This is given by:\n\\[\\theta_{ij}=(-1)^{i+j}|\\mathbf{M_{ij}}|\\]\nso, from the example finding the minor of \\(a_{11}\\), \\(i\\)=1 and \\(j=1\\); their sum is 2 which is even, so the cofactor is \\(+a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\). If we find the cofactor for every element, \\(a_{ij}\\) in a matrix and replace each element with its cofactor, the new matrix is called the cofactor matrix of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24.html#cofactor-matrix-1",
    "href": "matrix24.html#cofactor-matrix-1",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nWhat we have so far is the signed matrix of minors:\n\\[\\mathbf{A} = \\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#cofactor-expansion",
    "href": "matrix24.html#cofactor-expansion",
    "title": "Matrix algebra basics",
    "section": "Cofactor Expansion",
    "text": "Cofactor Expansion\nFind the signed determinant of each 2x2 submatrix.\n\\[\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} = \\left[\n\\begin{array}{rrr}\n10&-10&0\\\\\n-12&0&6\\\\\n-2&5&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#find-the-determinant",
    "href": "matrix24.html#find-the-determinant",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nUsing the cofactor matrix, \\(\\mathbf{\\Theta_{A}}\\) we perform cofactor expansion in order to find the determinant, \\(|\\mathbf{A}|\\). Cofactor expansion is given by:\n\\[|\\mathbf{A}|=\\sum\\limits_{i,j=1}^{n}a_{ij}\\Theta_{ij}\\]\nwhich means that we take two corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) and sum the products of their elements - this gives us the determinant of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24.html#find-the-determinant-1",
    "href": "matrix24.html#find-the-determinant-1",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nSo taking the first row from \\(\\mathbf{A}\\) (1,4,2) and from \\(\\mathbf{\\Theta_{A}}\\) (10,-10,0) above, we compute\n\\[a_{11} \\cdot \\Theta_{A11}+a_{12} \\cdot \\Theta_{A12}+a_{13} \\cdot \\Theta_{A13}\\]\n\\[1 \\cdot 10+ 4 \\cdot -10 + 2 \\cdot 0 = -30\\] This is the determinant of \\(\\mathbf{A}\\) , \\(|\\mathbf{A}|\\). Note that any corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) will produce the same result."
  },
  {
    "objectID": "matrix24.html#adjoint-matrix",
    "href": "matrix24.html#adjoint-matrix",
    "title": "Matrix algebra basics",
    "section": "Adjoint Matrix",
    "text": "Adjoint Matrix\nIf we transpose the cofactor matrix, (cof \\(\\mathbf{A'}\\)), the new matrix is called the adjoint matrix, denoted adj\\(\\mathbf{A}\\)=(cof \\(\\mathbf{A'}\\)).\n\\[adj\\mathbf{A} = \\mathbf{\\Theta_{A}'} = \\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#inverting-the-matrix",
    "href": "matrix24.html#inverting-the-matrix",
    "title": "Matrix algebra basics",
    "section": "Inverting the matrix",
    "text": "Inverting the matrix\nBelieve it or not, this all leads us to inverting the matrix, provided it is nonsingular.\n\\[\\mathbf{A^{-1}}=\\frac{1}{|\\mathbf{A}|}(adj {\\mathbf A})\\] The inverse of A is equal to one over the determinant of A times the adjoint matrix of A. So we’re pre-multiplying \\(adj {\\mathbf A}\\) by the (scalar) determinant, -30."
  },
  {
    "objectID": "matrix24.html#inverse-of-matrix-a",
    "href": "matrix24.html#inverse-of-matrix-a",
    "title": "Matrix algebra basics",
    "section": "Inverse of Matrix A",
    "text": "Inverse of Matrix A\n\\[{\\mathbf A^{-1}}=-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n=\\left[\n\\begin{array}{rrr}\n-\\frac{1}{3}&\\frac{2}{5}&\\frac{1}{15}\\\\\n\\frac{1}{3}&0&-\\frac{1}{6}\\\\\n0&-\\frac{1}{5}&\\frac{3}{10}\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#checking-our-work",
    "href": "matrix24.html#checking-our-work",
    "title": "Matrix algebra basics",
    "section": "Checking our work",
    "text": "Checking our work\nWe can check our work to be sure we’ve inverted correctly by making sure that \\(\\mathbf{A^{-1}A}=\\mathbf{I_{3}}\\); so,\n\\[-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n\\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n=\n-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n-30&0&0\\\\\n0&-30&0\\\\\n0&0&-30\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols",
    "href": "matrix24.html#connecting-data-matrices-to-ols",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\nLet’s examine these matrices a little to get a feel for what the computation of \\(\\widehat{\\beta}\\) involves: \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\n\\[\n\\mathbf{X}= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \n\\end{matrix}  \\right]\n\\] The dimensions of \\(\\mathbf{X}\\) are \\(n x k\\); the sample size by the number of regressors (including the constant)."
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols-1",
    "href": "matrix24.html#connecting-data-matrices-to-ols-1",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \n\\end{matrix}  \\right] \\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols-2",
    "href": "matrix24.html#connecting-data-matrices-to-ols-2",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i}\n\\end{matrix}  \\right] \\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationships among \\(X_k\\)."
  },
  {
    "objectID": "overview24.html",
    "href": "overview24.html",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise\n\n\n\n\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding.\n\n\n\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling.\n\n\n\n\nIs not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal.\n\n\n\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here.\n\n\n\n\n\nis a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together.\n\n\n\n\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra\n\n\n\n\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\).\n\n\n\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?\n\n\n\n\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term.\n\n\n\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later.\n\n\n\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24.html#all-models-are-wrong-ldots",
    "href": "overview24.html#all-models-are-wrong-ldots",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise"
  },
  {
    "objectID": "overview24.html#the-regression-model",
    "href": "overview24.html#the-regression-model",
    "title": "Course overview",
    "section": "",
    "text": "This course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding."
  },
  {
    "objectID": "overview24.html#course-goals",
    "href": "overview24.html#course-goals",
    "title": "Course overview",
    "section": "",
    "text": "In May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling."
  },
  {
    "objectID": "overview24.html#this-class",
    "href": "overview24.html#this-class",
    "title": "Course overview",
    "section": "",
    "text": "Is not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal."
  },
  {
    "objectID": "overview24.html#this-class-1",
    "href": "overview24.html#this-class-1",
    "title": "Course overview",
    "section": "",
    "text": "is aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here."
  },
  {
    "objectID": "overview24.html#this-class-2",
    "href": "overview24.html#this-class-2",
    "title": "Course overview",
    "section": "",
    "text": "is a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together."
  },
  {
    "objectID": "overview24.html#what-do-you-need-to-know-for-501",
    "href": "overview24.html#what-do-you-need-to-know-for-501",
    "title": "Course overview",
    "section": "",
    "text": "What do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra"
  },
  {
    "objectID": "overview24.html#matrix-algebra",
    "href": "overview24.html#matrix-algebra",
    "title": "Course overview",
    "section": "",
    "text": "You should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\)."
  },
  {
    "objectID": "overview24.html#probability-theory",
    "href": "overview24.html#probability-theory",
    "title": "Course overview",
    "section": "",
    "text": "what are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?"
  },
  {
    "objectID": "overview24.html#understanding-regression-models",
    "href": "overview24.html#understanding-regression-models",
    "title": "Course overview",
    "section": "",
    "text": "The class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term."
  },
  {
    "objectID": "overview24.html#regression-models",
    "href": "overview24.html#regression-models",
    "title": "Course overview",
    "section": "",
    "text": "Posit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later."
  },
  {
    "objectID": "overview24.html#regression-models-1",
    "href": "overview24.html#regression-models-1",
    "title": "Course overview",
    "section": "",
    "text": "Can be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24.html#models-to-understand-politics",
    "href": "overview24.html#models-to-understand-politics",
    "title": "Course overview",
    "section": "Models to understand politics",
    "text": "Models to understand politics\n\\(~\\)\nThis a methods class, but the methods are only meaningful to us insofar as they help us understand politics. So let’s motivate the class with a question about politics."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Course overview\nMatrix algebra basics\nProbability basics\nThinking about data\nBivariate model\nMultivariate model\nPrediction methods\nNormality\nInference\nModel specification\nDummy variables and interactions\nLimited dependent variables\nPanel data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression."
  },
  {
    "objectID": "overview24.html#careful-thinking",
    "href": "overview24.html#careful-thinking",
    "title": "Course overview",
    "section": "Careful thinking",
    "text": "Careful thinking\nCorrelation does not imply causation.\n\n\n\nRepression during the COVID-19 Pandemic\nAmong the products of the pandemic is an opportunity for governments, under the guise of protecting public health, to repress citizens.\nThere is some evidence of democratic backsliding prior to the pandemic, but the global public health crisis gave governments a specific and universal opportunity to take advantage of this softened ground and to become more authoritarian.\nWe’re going to examine the covariates of violence against civilians during the COVID period, focusing on how states of emergency create changes in government violence.\n\n\n\nData\nLet’s look at ACLED event data, and data on states of emergency during the COVID period from Lundgren et al (2020).\nThe next figure (which should look familiar), plots protests and repression since mid-2019, the onset of the pandemic marked at March 15, 2020.\nThe blue lines are local regressions showing trends; while protests return to pre-pandemic levels (similar to the years prior), repression increases since the onset of COVID-19.\n\n\n\nProtests\n\n\ncode\nacled&lt;-read.csv(\"/Users/dave/Documents/2013/PITF/analysis/protests2021/data/2022analysis/acled2022.csv\")\n\n##Event_date variable has a \"Chr\" format. \n#Need to convert it into date format to calculate 7 day averages for Protests and Violence against Civilians\n\nacled$event_date&lt;-as.Date(acled$event_date,format=\"%d %B %Y\")\n#class(acled$event_date)\n\n##Compute number of events per day\n\nacled$perday&lt;-1\n\nprotests&lt;-filter(acled,event_type==\"Protests\" )\nstateviol&lt;-filter(acled, event_type==\"Violence against civilians\")\nprotests&lt;-aggregate(perday ~ event_date, data = protests, sum)\nstateviol&lt;-aggregate(perday ~ event_date, data = stateviol, sum)\n\n\n##Calculating a 7 day moving average\nprotests$rollmean&lt;-rollmean(protests$perday,k=7, fill=NA)\nstateviol$rollmean&lt;-rollmean(stateviol$perday,k=7, fill=NA)\n\nprotests &lt;- protests %&gt;% filter(protests$event_date &lt; \"2022-06-25\")\nstateviol &lt;- stateviol %&gt;% filter(stateviol$event_date &lt; \"2022-06-25\")\n\nprotestplot &lt;- ggplot(data=protests, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Protests\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; 'ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Protests During the Pandemic\") \n   \n\nprotestplot\n\n\n\n\n\n\n\n\n\n\n\n\nRepression\n\n\ncode\nrepressionplot &lt;- ggplot(data=stateviol, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Violence against civilians\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Violence Against Civilians During the Pandemic\") \n  \nrepressionplot\n\n\n\n\n\n\n\n\n\n\n\n\nStates of Emergency during the Pandemic\n\n\ncode\n####################\n#analysis data\n####################\n\n#make analysis data frame\nanalysisdata &lt;- left_join(perday, soe, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"event_date\"))\nanalysisdata &lt;- left_join(analysisdata, covid, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"date\"))\n\n#zeros for NA in soe_s\nanalysisdata &lt;- analysisdata %&gt;% mutate(across(soe_s, ~replace_na(., 0)))\n\n#fill soe series after onset\nanalysisdata &lt;- analysisdata %&gt;% group_by(ccode) %&gt;% mutate(cumSOE = cumsum(soe_s)) %&gt;% ungroup\n\n#plot cumulative states of emergency over time\ntotalsoe &lt;- analysisdata %&gt;% group_by(event_date) %&gt;%\n  summarise(across(cumSOE, sum)) %&gt;%\n  ungroup() \n\ntotalsoe$event_date &lt;- as.Date(totalsoe$event_date)\n\ntotalsoe &lt;- totalsoe %&gt;% filter(event_date &gt;\"2020-01-01\"& event_date &lt;\"2020-07-01\")\n\nggplot() +\n  geom_line(data=totalsoe, aes(x=event_date, y=cumSOE))+\n  scale_x_date(date_breaks = \"1 months\", date_labels = '%b %Y') +\n  labs ( colour = NULL, x = \"Date\", y =  \"Active States of Emergency\" )  \n\n\n\n\n\n\n\n\n\n\n\n\nModeling repression\nLet’s build a simple model predicting violence against civilians. The data here are daily, covering 167 countries between January 2020 and July 2022. We’ll predict the 7 day moving average of repression depending on whether a state of emergency is in place.\n\\(~\\)\nMy expectation is states of emergency will make repression easier to motivate and therefore, more frequent.\n\n\n\nThe model\n\\(y\\) = violence against civilians\n\\(x\\) = states of emergency\nControlling for:\n\nnew daily deaths from COVID (logged)\npercentage of the population over 65 years old\nthe Human Development Index\nthe 7 day average number of protests.\n\n\n\n\nModeling Repression and SoEs\n\n\ncode\n##################\n#models\n#################\n\nanalysisdata$soeprotests &lt;- analysisdata$cumSOE*analysisdata$protest7day\nanalysisdata$lnd =  log(analysisdata$new_deaths+.01)\n#label variables for coef plot\nlibrary(\"labelled\")\nanalysisdata &lt;- analysisdata %&gt;%\n  set_variable_labels(\n    cumSOE = \"State of Emergency\",\n    lnd = \"ln(New COVID deaths)\",\n    aged_65_older = \"% age 65 +\",\n    human_development_index = \"Human Development Index\",\n    protest7day = \"Protests, 7 day avg\",\n    soeprotests = \"SoE * Protests, 7 day avg\", \n    `Violence against civilians` = \"Violence against Civilians\"\n  )\n\n#model predicting repression during covid \n\nm1 &lt;- lm(data=analysisdata, `Violence against civilians` ~ cumSOE  + lnd + aged_65_older+human_development_index + protest7day )\n#summary(m1)  \n\nlibrary(sjPlot)\ntab_model(m1, show.se=TRUE,  p.style=\"stars\", show.ci=FALSE)\n\n\n\n\n\n\n\n\n\n\n \nViolence against\nCivilians\n\n\nPredictors\nEstimates\nstd. Error\n\n\n(Intercept)\n0.93 ***\n0.03\n\n\nState of Emergency\n0.26 ***\n0.01\n\n\nln(New COVID deaths)\n0.02 ***\n0.00\n\n\n%age 65+\n-0.02 ***\n0.00\n\n\nHuman Development Index\n-0.68 ***\n0.04\n\n\nProtests,7 day avg\n0.04 ***\n0.00\n\n\nObservations\n165211\n\n\nR2 / R2 adjusted\n0.057 / 0.057\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\n\nAnother look\n\n\ncode\n#coefficient plot\nggcoef_model(\n  m1,show_p_values = FALSE,\n  signif_stars = FALSE, exponentiate =FALSE, intercept=TRUE,\n  colour=NULL, include = c(\"cumSOE\", \"lnd\", \"aged_65_older\", \"human_development_index\", \"protest7day\")\n)+\n  xlab(\"Average Marginal Effects\") +\n  ggtitle(\"Predictors of Violence Against Civilians\") \n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe estimates indicate:\n\nif all the \\(X\\) variables are set to zero, the expected violence against civilians is 1.16 (the intercept). Can we reasonably set all the \\(X\\) variables to zero?\nthe difference between a state with a SoE and without is different from zero, but quite small; 0.13 uses of violence.\nmore deaths from COVID are associated with more violence against civilians; \\(ln(\\beta) = .05\\). Exponentiate that, and the effect on violence is 1.05, so large compared to anything else in the model.\n\n\n\n\nQuantities of interest (simulation)\nLet’s generate predictions from the model by simulating the distribution of \\(\\widehat{\\beta}\\):\n\nsample 1000 times from a multivariate normal distribution with mean \\(\\widehat{\\beta}\\)s and variances of \\(var(\\widehat{\\beta})\\).\nplug those simulated \\(\\widehat{\\beta}\\)s into the equation using mean/median/mode values for the \\(X\\)s.\n\n\n\n\nSimulating quantities\nFirst, set \\(SoE = 0\\); vary \\(covid~deaths = ln(1 \\ldots 28000)\\), \\(\\%~age~ 65 = 6\\) (the median), \\(HDI = .74\\) (the median), and \\(protests = 2\\) (the median).\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*   0 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\nthen, repeat but with \\(SoE = 1\\):\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*1 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\n\n\n\nPredicted Violence against Civilians\n\n\ncode\n#simulated quantities\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),\n      ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= \n      numeric(0), deaths = numeric(0))\n\nfor (p in seq(0,28000,100)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                       B$b5*2, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                      B$b5*2, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\npredictions$deaths &lt;- log(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"ln(New COVID Deaths)\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5.5, y = .82, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 6.5, y = .55, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nSoE states have higher levels of violence against civilians, but the difference is very small, making me think we haven’t found anything particularly interesting here.\n\nCovid deaths is associated with more violence; though violence nearly doubles over the range of deaths (x-axis), the range is huge - from 1 to 28,000. If we transform the x-axis to deaths rather than \\(ln(deaths)\\), we can see this effect is pretty isolated.\n\n\n\n\nSimulated effects (again)\n\n\ncode\npredictions$deaths &lt;- exp(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"New COVID Deaths\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5000, y = 1, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 10000, y = .65, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe increase in violence occurs almost entirely between zero and 5000 COVID deaths. We can speculate on why this might be; doing so might lead us to wonder how useful this model is.\n\\(~\\) \\(~\\)\nThat’s the primary question we’ll ask this semester."
  },
  {
    "objectID": "overview24s.html#all-models-are-wrong-ldots",
    "href": "overview24s.html#all-models-are-wrong-ldots",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise"
  },
  {
    "objectID": "overview24s.html#the-regression-model",
    "href": "overview24s.html#the-regression-model",
    "title": "Course overview",
    "section": "",
    "text": "This course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding."
  },
  {
    "objectID": "overview24s.html#course-goals",
    "href": "overview24s.html#course-goals",
    "title": "Course overview",
    "section": "",
    "text": "In May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling."
  },
  {
    "objectID": "overview24s.html#this-class",
    "href": "overview24s.html#this-class",
    "title": "Course overview",
    "section": "",
    "text": "Is not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal."
  },
  {
    "objectID": "overview24s.html#this-class-1",
    "href": "overview24s.html#this-class-1",
    "title": "Course overview",
    "section": "",
    "text": "is aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here."
  },
  {
    "objectID": "overview24s.html#this-class-2",
    "href": "overview24s.html#this-class-2",
    "title": "Course overview",
    "section": "",
    "text": "is a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together."
  },
  {
    "objectID": "overview24s.html#what-do-you-need-to-know-for-501",
    "href": "overview24s.html#what-do-you-need-to-know-for-501",
    "title": "Course overview",
    "section": "",
    "text": "What do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra"
  },
  {
    "objectID": "overview24s.html#matrix-algebra",
    "href": "overview24s.html#matrix-algebra",
    "title": "Course overview",
    "section": "",
    "text": "You should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\)."
  },
  {
    "objectID": "overview24s.html#probability-theory",
    "href": "overview24s.html#probability-theory",
    "title": "Course overview",
    "section": "",
    "text": "what are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?"
  },
  {
    "objectID": "overview24s.html#understanding-regression-models",
    "href": "overview24s.html#understanding-regression-models",
    "title": "Course overview",
    "section": "",
    "text": "The class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term."
  },
  {
    "objectID": "overview24s.html#regression-models",
    "href": "overview24s.html#regression-models",
    "title": "Course overview",
    "section": "",
    "text": "Posit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later."
  },
  {
    "objectID": "overview24s.html#regression-models-1",
    "href": "overview24s.html#regression-models-1",
    "title": "Course overview",
    "section": "",
    "text": "Can be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24s.html#models-to-understand-politics",
    "href": "overview24s.html#models-to-understand-politics",
    "title": "Course overview",
    "section": "Models to understand politics",
    "text": "Models to understand politics\n\\(~\\)\nThis a methods class, but the methods are only meaningful to us insofar as they help us understand politics. So let’s motivate the class with a question about politics."
  },
  {
    "objectID": "overview24s.html#careful-thinking",
    "href": "overview24s.html#careful-thinking",
    "title": "Course overview",
    "section": "Careful thinking",
    "text": "Careful thinking\nCorrelation does not imply causation.\n\n\n\nRepression during the COVID-19 Pandemic\nAmong the products of the pandemic is an opportunity for governments, under the guise of protecting public health, to repress citizens.\nThere is some evidence of democratic backsliding prior to the pandemic, but the global public health crisis gave governments a specific and universal opportunity to take advantage of this softened ground and to become more authoritarian.\nWe’re going to examine the covariates of violence against civilians during the COVID period, focusing on how states of emergency create changes in government violence.\n\n\n\nData\nLet’s look at ACLED event data, and data on states of emergency during the COVID period from Lundgren et al (2020).\nThe next figure (which should look familiar), plots protests and repression since mid-2019, the onset of the pandemic marked at March 15, 2020.\nThe blue lines are local regressions showing trends; while protests return to pre-pandemic levels (similar to the years prior), repression increases since the onset of COVID-19.\n\n\n\nProtests\n\n\ncode\nacled&lt;-read.csv(\"/Users/dave/Documents/2013/PITF/analysis/protests2021/data/2022analysis/acled2022.csv\")\n\n##Event_date variable has a \"Chr\" format. \n#Need to convert it into date format to calculate 7 day averages for Protests and Violence against Civilians\n\nacled$event_date&lt;-as.Date(acled$event_date,format=\"%d %B %Y\")\n#class(acled$event_date)\n\n##Compute number of events per day\n\nacled$perday&lt;-1\n\nprotests&lt;-filter(acled,event_type==\"Protests\" )\nstateviol&lt;-filter(acled, event_type==\"Violence against civilians\")\nprotests&lt;-aggregate(perday ~ event_date, data = protests, sum)\nstateviol&lt;-aggregate(perday ~ event_date, data = stateviol, sum)\n\n\n##Calculating a 7 day moving average\nprotests$rollmean&lt;-rollmean(protests$perday,k=7, fill=NA)\nstateviol$rollmean&lt;-rollmean(stateviol$perday,k=7, fill=NA)\n\nprotests &lt;- protests %&gt;% filter(protests$event_date &lt; \"2022-06-25\")\nstateviol &lt;- stateviol %&gt;% filter(stateviol$event_date &lt; \"2022-06-25\")\n\nprotestplot &lt;- ggplot(data=protests, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Protests\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; 'ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Protests During the Pandemic\") \n   \n\nprotestplot\n\n\n\n\n\n\n\n\n\n\n\n\nRepression\n\n\ncode\nrepressionplot &lt;- ggplot(data=stateviol, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Violence against civilians\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Violence Against Civilians During the Pandemic\") \n  \nrepressionplot\n\n\n\n\n\n\n\n\n\n\n\n\nStates of Emergency during the Pandemic\n\n\ncode\n####################\n#analysis data\n####################\n\n#make analysis data frame\nanalysisdata &lt;- left_join(perday, soe, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"event_date\"))\nanalysisdata &lt;- left_join(analysisdata, covid, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"date\"))\n\n#zeros for NA in soe_s\nanalysisdata &lt;- analysisdata %&gt;% mutate(across(soe_s, ~replace_na(., 0)))\n\n#fill soe series after onset\nanalysisdata &lt;- analysisdata %&gt;% group_by(ccode) %&gt;% mutate(cumSOE = cumsum(soe_s)) %&gt;% ungroup\n\n#plot cumulative states of emergency over time\ntotalsoe &lt;- analysisdata %&gt;% group_by(event_date) %&gt;%\n  summarise(across(cumSOE, sum)) %&gt;%\n  ungroup() \n\ntotalsoe$event_date &lt;- as.Date(totalsoe$event_date)\n\ntotalsoe &lt;- totalsoe %&gt;% filter(event_date &gt;\"2020-01-01\"& event_date &lt;\"2020-07-01\")\n\nggplot() +\n  geom_line(data=totalsoe, aes(x=event_date, y=cumSOE))+\n  scale_x_date(date_breaks = \"1 months\", date_labels = '%b %Y') +\n  labs ( colour = NULL, x = \"Date\", y =  \"Active States of Emergency\" )  \n\n\n\n\n\n\n\n\n\n\n\n\nModeling repression\nLet’s build a simple model predicting violence against civilians. The data here are daily, covering 167 countries between January 2020 and July 2022. We’ll predict the 7 day moving average of repression depending on whether a state of emergency is in place.\n\\(~\\)\nMy expectation is states of emergency will make repression easier to motivate and therefore, more frequent.\n\n\n\nThe model\n\\(y\\) = violence against civilians\n\\(x\\) = states of emergency\nControlling for:\n\nnew daily deaths from COVID (logged)\npercentage of the population over 65 years old\nthe Human Development Index\nthe 7 day average number of protests.\n\n\n\n\nModeling Repression and SoEs\n\n\ncode\n##################\n#models\n#################\n\nanalysisdata$soeprotests &lt;- analysisdata$cumSOE*analysisdata$protest7day\nanalysisdata$lnd =  log(analysisdata$new_deaths+.01)\n#label variables for coef plot\nlibrary(\"labelled\")\nanalysisdata &lt;- analysisdata %&gt;%\n  set_variable_labels(\n    cumSOE = \"State of Emergency\",\n    lnd = \"ln(New COVID deaths)\",\n    aged_65_older = \"% age 65 +\",\n    human_development_index = \"Human Development Index\",\n    protest7day = \"Protests, 7 day avg\",\n    soeprotests = \"SoE * Protests, 7 day avg\", \n    `Violence against civilians` = \"Violence against Civilians\"\n  )\n\n#model predicting repression during covid \n\nm1 &lt;- lm(data=analysisdata, `Violence against civilians` ~ cumSOE  + lnd + aged_65_older+human_development_index + protest7day )\n#summary(m1)  \n\nlibrary(sjPlot)\ntab_model(m1, show.se=TRUE,  p.style=\"stars\", show.ci=FALSE)\n\n\n\n\n\n\n\n\n\n\n \nViolence against\nCivilians\n\n\nPredictors\nEstimates\nstd. Error\n\n\n(Intercept)\n0.93 ***\n0.03\n\n\nState of Emergency\n0.26 ***\n0.01\n\n\nln(New COVID deaths)\n0.02 ***\n0.00\n\n\n%age 65+\n-0.02 ***\n0.00\n\n\nHuman Development Index\n-0.68 ***\n0.04\n\n\nProtests,7 day avg\n0.04 ***\n0.00\n\n\nObservations\n165211\n\n\nR2 / R2 adjusted\n0.057 / 0.057\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\n\nAnother look\n\n\ncode\n#coefficient plot\nggcoef_model(\n  m1,show_p_values = FALSE,\n  signif_stars = FALSE, exponentiate =FALSE, intercept=TRUE,\n  colour=NULL, include = c(\"cumSOE\", \"lnd\", \"aged_65_older\", \"human_development_index\", \"protest7day\")\n)+\n  xlab(\"Average Marginal Effects\") +\n  ggtitle(\"Predictors of Violence Against Civilians\") \n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe estimates indicate:\n\nif all the \\(X\\) variables are set to zero, the expected violence against civilians is 1.16 (the intercept). Can we reasonably set all the \\(X\\) variables to zero?\nthe difference between a state with a SoE and without is different from zero, but quite small; 0.13 uses of violence.\nmore deaths from COVID are associated with more violence against civilians; \\(ln(\\beta) = .05\\). Exponentiate that, and the effect on violence is 1.05, so large compared to anything else in the model.\n\n\n\n\nQuantities of interest (simulation)\nLet’s generate predictions from the model by simulating the distribution of \\(\\widehat{\\beta}\\):\n\nsample 1000 times from a multivariate normal distribution with mean \\(\\widehat{\\beta}\\)s and variances of \\(var(\\widehat{\\beta})\\).\nplug those simulated \\(\\widehat{\\beta}\\)s into the equation using mean/median/mode values for the \\(X\\)s.\n\n\n\n\nSimulating quantities\nFirst, set \\(SoE = 0\\); vary \\(covid~deaths = ln(1 \\ldots 28000)\\), \\(\\%~age~ 65 = 6\\) (the median), \\(HDI = .74\\) (the median), and \\(protests = 2\\) (the median).\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*   0 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\nthen, repeat but with \\(SoE = 1\\):\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*1 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\n\n\n\nPredicted Violence against Civilians\n\n\ncode\n#simulated quantities\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),\n      ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= \n      numeric(0), deaths = numeric(0))\n\nfor (p in seq(0,28000,100)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                       B$b5*2, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                      B$b5*2, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\npredictions$deaths &lt;- log(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"ln(New COVID Deaths)\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5.5, y = .82, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 6.5, y = .55, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nSoE states have higher levels of violence against civilians, but the difference is very small, making me think we haven’t found anything particularly interesting here.\n\nCovid deaths is associated with more violence; though violence nearly doubles over the range of deaths (x-axis), the range is huge - from 1 to 28,000. If we transform the x-axis to deaths rather than \\(ln(deaths)\\), we can see this effect is pretty isolated.\n\n\n\n\nSimulated effects (again)\n\n\ncode\npredictions$deaths &lt;- exp(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"New COVID Deaths\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5000, y = 1, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 10000, y = .65, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe increase in violence occurs almost entirely between zero and 5000 COVID deaths. We can speculate on why this might be; doing so might lead us to wonder how useful this model is.\n\\(~\\) \\(~\\)\nThat’s the primary question we’ll ask this semester."
  },
  {
    "objectID": "probability24s.html#why-probability-distributions",
    "href": "probability24s.html#why-probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns."
  },
  {
    "objectID": "probability24s.html#probability-distributions",
    "href": "probability24s.html#probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Probability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?"
  },
  {
    "objectID": "probability24s.html#things-we-want-to-know-about-x",
    "href": "probability24s.html#things-we-want-to-know-about-x",
    "title": "Basic probability",
    "section": "",
    "text": "\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value)."
  },
  {
    "objectID": "probability24s.html#types-of-variables",
    "href": "probability24s.html#types-of-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Variables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "probability24s.html#discrete-variables",
    "href": "probability24s.html#discrete-variables",
    "title": "Basic probability",
    "section": "",
    "text": "May be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "probability24s.html#continuous-variables",
    "href": "probability24s.html#continuous-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Can be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "probability24s.html#levels-of-measurement",
    "href": "probability24s.html#levels-of-measurement",
    "title": "Basic probability",
    "section": "",
    "text": "These four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "probability24s.html#levels-of-measurement-and-models",
    "href": "probability24s.html#levels-of-measurement-and-models",
    "title": "Basic probability",
    "section": "",
    "text": "In general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24s.html#pdf-and-cdf",
    "href": "probability24s.html#pdf-and-cdf",
    "title": "Basic probability",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nProbability distributions can be described by\n\nProbability Density Function (PDF) which maps \\(X\\) onto the probability space describing the probabilities of every value of \\(X\\), \\(x\\).\nCumulative Distribution Function (CDF) which maps \\(X\\) onto the probability space describing the probability \\(X\\) is less than some value, \\(x\\)."
  },
  {
    "objectID": "probability24s.html#pdf-density",
    "href": "probability24s.html#pdf-density",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nDefinition\n\n\n\nThe Probability Density Function or PDF describes the probability a random variable takes on a particular value, and it does so for all values of that random variable."
  },
  {
    "objectID": "probability24s.html#pdf-density-1",
    "href": "probability24s.html#pdf-density-1",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nExample\n\n\n\nSuppose we toss a fair coin 1 time and want to describe the probability distribution of the outcome, \\(Y\\) which is a random variable. The possible outcomes are heads and tails, and the probability of each is .5. This is the PDF because it describes the probabilities associated with all possible outcomes of \\(Y\\)."
  },
  {
    "objectID": "probability24s.html#pdf-density-2",
    "href": "probability24s.html#pdf-density-2",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\nWhile establishing the probability of a value of a discrete variable is possible, establishing the probability of a particular value of a continuous variable is not.\nInstead, the continuous PDF describes the instantaneous rate of change in \\(Pr(X=x)\\) for every value of \\(x\\)."
  },
  {
    "objectID": "probability24s.html#pdf-plots",
    "href": "probability24s.html#pdf-plots",
    "title": "Basic probability",
    "section": "PDF plots",
    "text": "PDF plots"
  },
  {
    "objectID": "probability24s.html#cdf",
    "href": "probability24s.html#cdf",
    "title": "Basic probability",
    "section": "CDF",
    "text": "CDF\n\n\n\n\n\n\nDefinition\n\n\n\nFor a random variable, \\(Y\\), the probability \\(Y\\) is less than or equal to some particular value, \\(y\\) in the range of \\(Y\\) defines the cumulative density function.\nFor a continuous variable:\n\\[P(Y \\leq j) =\\int\\limits_{-\\infty}^{j} f(X)dX\\]\nFor a discrete variable:\n\\[P(Y \\leq j) = \\sum\\limits_{y\\leq j} P(Y=y)\n= 1- \\sum\\limits_{y&gt; j} P(Y=y)\\]"
  },
  {
    "objectID": "probability24s.html#cdf-plots",
    "href": "probability24s.html#cdf-plots",
    "title": "Basic probability",
    "section": "CDF plots",
    "text": "CDF plots"
  },
  {
    "objectID": "probability24s.html#notation",
    "href": "probability24s.html#notation",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\\(f(x)\\) denotes the PDF of \\(x\\).\n\\(F(x)\\) denotes the CDF of \\(x\\).\nSubstitute either the appropriate name, symbol, or function for \\(F\\) or \\(f\\) to indicate the distribution.\nLower case Greek letters denote PDFs: e.g. \\(f(x)= \\phi(x)\\) denotes the normal PDF.\nUpper case Greek letters denote CDFs: e.g. \\(F(x)=\\Phi(x)\\) denotes the normal CDF."
  },
  {
    "objectID": "probability24s.html#notation-1",
    "href": "probability24s.html#notation-1",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nExample\n\n\n\n\\(x\\) is distributed Normal, \\(N(\\mu, \\sigma^{2})\\):\n\\[F(x) = \\Phi_{\\mu, \\sigma^{2}}(x)  =    \\int  \\phi_{\\mu, \\sigma^{2}} (x) f(x)d(x) \\]\n\\[ f(x) = \\phi_{\\mu, \\sigma^{2}}(x) =\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{(x-\\mu)^{2}}{2 \\sigma^{2}} \\right) \\]\nNote the first derivative of the CDF is the PDF; the integral of the PDF is the CDF."
  },
  {
    "objectID": "probability24s.html#bernoulli",
    "href": "probability24s.html#bernoulli",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nSuppose a binary variable \\(x\\) that takes on only the values of zero and one (\\(x \\in {0, 1}\\)):\n\\[\nPr(X=1)=\\pi\\nonumber \\\\\nPr(x=0)= 1-Pr(x=1) \\\\\n= 1-\\pi   \n\\]"
  },
  {
    "objectID": "probability24s.html#bernoulli-1",
    "href": "probability24s.html#bernoulli-1",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe PDF is:\n\\[ f(x) = \\\\\n          \\pi    ~~~~~~~~ ~ ~ ~~~~ \\text{if } x=1\\\\\n         1-\\pi   ~ ~ ~ ~~~~\\text{if } x=0\n     \\]\nor\n\\[f(x) = \\pi^{x}(1-\\pi)^{1-x} \\]"
  },
  {
    "objectID": "probability24s.html#bernoulli-2",
    "href": "probability24s.html#bernoulli-2",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe CDF is:\n\\[F(x) =\\sum_{x} f(x) \\]\nand the expected value is\n\\[\nE(x) =\\sum_{x}x f(x)    \\\\\n= (1)(\\pi)+(0)(1-\\pi)   \\\\\n= \\pi\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial",
    "href": "probability24s.html#binomial",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nBernoulli is important because it is the foundation for a lot of other distributions, including the binomial distribution. The binomial describes the success probability function (where \\(x=1\\) is a “success”) from a set of \\(n\\) independent Bernoulli trials. So, the binomial is the probability of successes (“ones”) in \\(n\\) independent Bernoulli trials with identical probabilities, \\(\\pi\\).\n\\(~\\)\nThere are two essential parts to the binomial PDF - the probability of success, and the number of ways a success can occur."
  },
  {
    "objectID": "probability24s.html#binomial-1",
    "href": "probability24s.html#binomial-1",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe probability of success is the Bernoulli probability:\n\\[\nf(x) = \\pi^{x}(1-\\pi)^{1-x}\n\\]\nand the number of ways success can occur (called “n-tuples”) is\n\\[  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} = \\frac{n!}{x!(n-x)!}\n\\]\nNotice the notation for the n-tuple."
  },
  {
    "objectID": "probability24s.html#binomial-2",
    "href": "probability24s.html#binomial-2",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe PDF combines these:\n\\[\nf(x)=\n  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} \\pi^{x}(1-\\pi)^{n-x}\n\\]\nThere are \\(n\\) trials, \\(x\\) is the number of successes, and \\(n-x\\) is the number of failures. Each event in the n-tuple arises with the Bernoulli probability."
  },
  {
    "objectID": "probability24s.html#binomial-3",
    "href": "probability24s.html#binomial-3",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are going to toss a fair coin 4 times and want to know how many ways we can have heads come up twice.\n\\[\n\\frac{4!}{2!(4-2)!} = 6\n\\] Now, suppose we want to know the probability exactly 2 of those 4 tosses is heads.\n\\[\nPr(x=2) =\\frac{4!}{2!(4-2)!} (.5)^{2}(.5)^{2} \\\\\n= 6(0.0625) \\\\\n= 0.375\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial-4",
    "href": "probability24s.html#binomial-4",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nHere’s another example: suppose we flip 5 fair coins once each; what is the PDF of the number of heads we expect? In other words, what is the probability associated with each possible number of successes (heads), from 0 to 5?\n\\[\nP(x=0)  =  \\begin{pmatrix}\n    5  \\\\\n    0\n  \\end{pmatrix} \\cdot  (.5)^{0} \\cdot (.5) ^{5} = 1 \\cdot 1 \\cdot 0.03125=0.03125 \\\\\nP(x=1) =  \\begin{pmatrix}\n    5  \\\\\n    1\n  \\end{pmatrix} \\cdot (.5)^{1}\\cdot (.5)^{4} = 5 \\cdot .5 \\cdot 0.0625=0.15625 \\\\\nP(x=2) =  \\begin{pmatrix}\n    5  \\\\\n    2\n  \\end{pmatrix} \\cdot (.5)^{2}\\cdot (.5)^{3} = 10 \\cdot .25 \\cdot 0.125=0.3125 \\\\\n\\vdots \\vdots \\vdots\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial-family-distributions",
    "href": "probability24s.html#binomial-family-distributions",
    "title": "Basic probability",
    "section": "Binomial family distributions",
    "text": "Binomial family distributions\n\nthe geometric distribution describes repeated Bernoulli trials with probability of success \\(\\pi\\), until the first success.\nthe negative binomial describes the number of Bernoulli failures prior to the first success - it can be thought of as a counting process up to the first success.\nthe poisson describes Bernoulli trials where \\(\\pi\\) for any particular trial is very small."
  },
  {
    "objectID": "probability24s.html#the-normal-distribution",
    "href": "probability24s.html#the-normal-distribution",
    "title": "Basic probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe Normal distribution is the most widely used distribution in the social sciences.\n\nThe normal seems to “fit” a lot of the variables social scientists measure and use in models.\nEstimation techniques we often employ assume the disturbance term is normally distributed; one result is the coefficients are either normally distributed or t-distributed."
  },
  {
    "objectID": "probability24s.html#normal-pdf",
    "href": "probability24s.html#normal-pdf",
    "title": "Basic probability",
    "section": "Normal PDF",
    "text": "Normal PDF\nThe normal PDF: \\[\nPr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]\n\\]\nwhere two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\) describe the location and shape of the distribution, the mean and variance respectively; we indicate a normally distributed variable and its parameters as\n\\[\nY \\sim \\text{Normal}(\\mu,\\sigma^{2}) \\nonumber\n\\]\n\n\nDescribe these dataNormal?\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "probability24s.html#standard-normal",
    "href": "probability24s.html#standard-normal",
    "title": "Basic probability",
    "section": "Standard Normal",
    "text": "Standard Normal\nA useful special case of the Normal is the standard normal, \\(z \\sim \\text{Normal}(0,1)\\). The PDF for the standard normal is given by\n\\[\n\\phi(z)=\\frac{1}{\\sqrt{2 \\pi }} exp \\left[\\frac{-z^{2}}{2}\\right] \\nonumber\n\\]\nwhere the parameters themselves drop out since we’d be subtracting a mean of zero and dividing/multiplying by a variance of 1. The standard normal CDF is denoted \\(\\Phi(z)\\); the standard normal PDF is denoted \\(\\phi(z)\\)."
  },
  {
    "objectID": "probability24s.html#normal-pdfs-different-moments",
    "href": "probability24s.html#normal-pdfs-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs different moments",
    "text": "Normal PDFs different moments"
  },
  {
    "objectID": "probability24s.html#models",
    "href": "probability24s.html#models",
    "title": "Basic probability",
    "section": "Models",
    "text": "Models\n\nProbability models include unobserved disturbances; we make assumptions about the distributions of those errors, \\(\\epsilon\\).\nWhat we assume about the unobservables is always informed by what we know about the observables, mainly \\(y\\).\nA useful way to describe or summarize \\(y\\) is to characterize its observed distribution.\nThe distribution of \\(y\\) informs our assumption about the distribution of \\(\\epsilon\\)."
  },
  {
    "objectID": "probability24s.html#why-this-matters-to-ols",
    "href": "probability24s.html#why-this-matters-to-ols",
    "title": "Basic probability",
    "section": "Why this matters to OLS",
    "text": "Why this matters to OLS\nLinear regression is such an inferential model; we have a number of sources of uncertainty. We represent that uncertainty in the model via the disturbance term, \\(\\epsilon\\).\n\\(~\\)\nIn order to know things about \\(\\epsilon\\) and other parts of the model, we assume that the disturbances are normally distributed; \\(y\\) should be normal too."
  },
  {
    "objectID": "probability24s.html#normality-and-centrality",
    "href": "probability24s.html#normality-and-centrality",
    "title": "Basic probability",
    "section": "Normality and Centrality",
    "text": "Normality and Centrality\nWe rely on stuff being normally distributed, and central tendency being meaningful. The Central Limit Theorem facilitates this."
  },
  {
    "objectID": "probability24s.html#central-limit-theorem",
    "href": "probability24s.html#central-limit-theorem",
    "title": "Basic probability",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ {\\sum\\limits_{n \\rightarrow \\infty} (\\bar{X_i})} / {n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "probability24s.html#clt---why-is-this-valuable",
    "href": "probability24s.html#clt---why-is-this-valuable",
    "title": "Basic probability",
    "section": "CLT - Why is this valuable?",
    "text": "CLT - Why is this valuable?\n\nIf we assume repeated sampling and/or infinitely large samples, we can assume normality.\nWe know the properties of the Normal intimately well. We can use this knowledge to evaluate where some value of \\(x\\) lies on the Normal CDF, what the probability less than that value is - we can figure out what the probability of observing that value is (on the PDF).\nAs we’ll see, \\(\\widehat{\\beta}\\) is normally distributed in the OLS model (it is in MLE as well). The CLT and what we know about normality facilitate inference."
  },
  {
    "objectID": "probability24s.html#inference",
    "href": "probability24s.html#inference",
    "title": "Basic probability",
    "section": "Inference",
    "text": "Inference\nInference is our effort to measure and characterize our uncertainty about the model and its parameters.\n\nuncertainty is the most important thing we estimate in inferential models.\ncharacterizing uncertainty relies on probability theory."
  },
  {
    "objectID": "matrix24s.html#matrix-notation",
    "href": "matrix24s.html#matrix-notation",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model."
  },
  {
    "objectID": "matrix24s.html#matrices",
    "href": "matrix24s.html#matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A."
  },
  {
    "objectID": "matrix24s.html#dimensions",
    "href": "matrix24s.html#dimensions",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "The dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar."
  },
  {
    "objectID": "matrix24s.html#symmetric-matrices",
    "href": "matrix24s.html#symmetric-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#rectangular-matrices",
    "href": "matrix24s.html#rectangular-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3)."
  },
  {
    "objectID": "matrix24s.html#vectors",
    "href": "matrix24s.html#vectors",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24s.html#transposition",
    "href": "matrix24s.html#transposition",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nThe transpose of a matrix A is the matrix flipped on its side such that its rows become columns and columns become rows; the transpose of A is denoted \\({\\mathbf A'}\\) and is referred to as “\\({\\mathbf A}\\) transpose.”\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right]\n~~~~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5 & 2\\\\\n\\end{array} \\right] \\]\nso \\({\\mathbf X}\\), a (3,2) matrix, becomes \\({\\mathbf X'}\\), a (2,3) matrix."
  },
  {
    "objectID": "matrix24s.html#transposition-1",
    "href": "matrix24s.html#transposition-1",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nIf \\({\\mathbf A}\\) is symmetric, then \\({\\mathbf A}={\\mathbf A'}\\) – take a look at the following symmetric matrix and you’ll see why:\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#transposition-2",
    "href": "matrix24s.html#transposition-2",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nAlso note that the transpose of a transposed matrix results in the original matrix: \\((\\mathbf{X}')'=\\mathbf{X}\\).\nTransposing a row vector results in a column vector and vice versa:\n\\[\\mathbf{x} =  \\left[\n\\begin{array}{c}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n\\end{array} \\right]\n~~~~~~~~~\n\\mathbf{x'} =  \\left[\n\\begin{array}{cccc}\n-1 & -9 &4 &16\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#trace-of-a-matrix",
    "href": "matrix24s.html#trace-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nThe trace of a matrix is the sum of the diagonal elements of a square matrix, and is denoted \\(tr(\\mathbf{A})=a_{11}+a_{22}+a_{33}\\ldots+a_{nn} = \\sum\\limits_{i=1}^{n}a_{ii}\\)\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nsuch that\n\\(tr(\\mathbf{A})=2+5+2=9\\)"
  },
  {
    "objectID": "matrix24s.html#addition-subtraction",
    "href": "matrix24s.html#addition-subtraction",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\n\nAddition and subtraction of matrices is analogous to the same scalar operations, but depend on the conformatibility of the matrices to be added or subtracted.\nTwo matrices are conformable for addition or subtraction iff they are of the same order.\nNote that conformability means something different for addition/subtraction than for multiplication."
  },
  {
    "objectID": "matrix24s.html#addition-subtraction-1",
    "href": "matrix24s.html#addition-subtraction-1",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider the following (2,2) matrices and the problem \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{C}\\):\n\\[ \\left[\n\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21}& a_{22} \\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{cc}\nb_{11} & b_{12}\\\\\nb_{21}& b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21}& a_{22}+b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\nc_{11} & c_{12}\\\\\nc_{21}& c_{22} \\\\\n\\end{array} \\right] \\]\nAddition and subtraction are only possible among matrices that share the same order; otherwise, they are nonconformable."
  },
  {
    "objectID": "matrix24s.html#addition-subtraction-2",
    "href": "matrix24s.html#addition-subtraction-2",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider a second example of addition, and note that subtraction would follow directly:\n\\[ \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{ccc}\n1 &5 &7\\\\\n3& 7& 1\\\\\n2& 4& 9\\\\\n\\end{array} \\right]\n=\n  \\left[\n\\begin{array}{ccc}\n3 &9 &15\\\\\n7& 12& 4\\\\\n10& 7& 11\\\\\n\\end{array} \\right] \\]\nMatrix addition adheres to the commutative and associative properties such that:\n\\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) (Commutative), and \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}= \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) (Associative)."
  },
  {
    "objectID": "matrix24s.html#multiplication",
    "href": "matrix24s.html#multiplication",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\nLet’s begin by multiplying a scalar and a matrix - the product is a matrix whose elements are the scalar multiplied by each element of the original matrix, so:\n\\[ \\beta\n\\left[\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\n\\beta x_{11} & \\beta x_{12}\\\\\n\\beta x_{21}&  \\beta x_{22} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#multiplication-1",
    "href": "matrix24s.html#multiplication-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\n\nIn order to multiply two matrices (or vectors), they must be conformable for multiplication.\nTwo matrices are conformable for multiplication iff the number of columns in the first matrix is equal to the number of rows in the second matrix. That is, \\(\\mathbf{A_{i,j}}\\) and \\(\\mathbf{B_{j,k}}\\).\nAn easy way to think of this is to write the dimensions of the two matrices, (i,j),(j,k) - the inner dimensions are the same, so the matrix is conformable for multiplication - moreover, the outer dimensions (i,k) give the dimension of the product matrix."
  },
  {
    "objectID": "matrix24s.html#multiplication---inner-product",
    "href": "matrix24s.html#multiplication---inner-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\n\nTo illustrate multiplication, let’s start with a column vector, \\(\\mathbf{e}\\) whose order is (N,1) and take its inner product.\nThe inner product of a column vector is the transpose of the column vector (thus, a row vector) post-multiplied by the column vector, so \\({\\mathbf e'\\mathbf e}\\). The inner product is a scalar."
  },
  {
    "objectID": "matrix24s.html#multiplication---inner-product-1",
    "href": "matrix24s.html#multiplication---inner-product-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\nWhen we transpose the column vector \\({\\mathbf e}\\) of (N,1) order, we get a row vector \\({\\mathbf e'}\\) of (1,N) order. Inner dimensions match, so they are conformable.\n\\[\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n~~~~~\n\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right] \\] \\[=\n(-1 \\cdot -1)+(-9 \\cdot -9)+(4 \\cdot 4)+ (16 \\cdot 16)+ (-10 \\cdot -10) = 454\\]"
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing",
    "href": "matrix24s.html#least-squares-foreshadowing",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nNote the inner product of a column vector is a scalar; the inner product of \\(\\mathbf{e}\\), is the sum of the squares of \\(\\mathbf{e}\\).\n\\[\\mathbf{e'e}= e_{1}e_{1}+e_{2}e_{2}+\\ldots e_{N}e_{N} = \\sum\\limits_{i=1}^{N}e_{i}^{2}\\]"
  },
  {
    "objectID": "matrix24s.html#multiplication---outer-product",
    "href": "matrix24s.html#multiplication---outer-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - outer product",
    "text": "Multiplication - outer product\nThe outer product of a column vector is the transpose of the column vector (thus, a row vector) pre-multiplied by the column vector, so \\({\\mathbf e\\mathbf e'}\\). The outer product is an (N,N) matrix.\n\\[\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right]\n\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n\\]\nLet’s multiply \\({\\mathbf e\\mathbf e'}\\), a column vector of (5,1) by a row vector of (1,5); we’ll obtain a square matrix of (5,5)."
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing-1",
    "href": "matrix24s.html#least-squares-foreshadowing-1",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nLet \\(\\mathbf{e}\\) represent the residuals or errors in our regression. The outer product\n\\[{\\mathbf e\\mathbf e'} =  \\left[\n\\begin{array}{rrrrr}\n1 & 9 &-4 &-16 &10\\\\\n9 & 81 &-36 &-144 &90\\\\\n-4 & -36 &16 &64 &-40\\\\\n-16 & -144 &64 &256 &-160\\\\\n10 & 90 &-40 &-160 &100\\\\\n\\end{array} \\right] \\]\nis the variance-covariance matrix of \\(\\mathbf{e}\\). The squares on the main diagonal (which sums to the sum of squares) and the symmetry of the off-diagonal elements."
  },
  {
    "objectID": "matrix24s.html#inverting-matrices-1",
    "href": "matrix24s.html#inverting-matrices-1",
    "title": "Matrix algebra basics",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\n\nYou’ll have noticed that division has been conspicuously absent so far. In scalar algebra, we sometimes represent division in fractions, \\(\\frac{1}{2}\\).\nAnother way to represent the same quantity is \\(2^{-1}\\), or the inverse of 2. Of course, in scalar algebra, a number multiplied by its inverse equals 1, e.g., \\(2 \\cdot 2^{-1}=1\\) or \\(2 \\cdot \\frac{1}{2}=1\\). So, if we are given \\(4x=1\\) and want to solve for \\(x\\) what we really want to know is “what is the inverse of 4 such that \\(4 \\cdot 4^{-1}=1\\)?” We consider matrices in a similar manner."
  },
  {
    "objectID": "matrix24s.html#inverting-square-matrices",
    "href": "matrix24s.html#inverting-square-matrices",
    "title": "Matrix algebra basics",
    "section": "Inverting Square Matrices",
    "text": "Inverting Square Matrices\nImagine a square matrix, \\(\\mathbf{X}\\) and its inverse, denoted \\({\\mathbf{X^{-1}}}\\) (read as “X inverse”). Analogous to scalar algebra, a matrix multiplied by its inverse is equal to the identity matrix, \\(\\mathbf{I}\\).\nSo in order to find \\({\\mathbf{X^{-1}}}\\), we must find the matrix that, when multiplied by \\(\\mathbf{X}\\), produces \\(\\mathbf{I}\\). Thus, for a square matrix, its inverse (if it exists) is such that\n\\[\\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X^{-1}} \\mathbf{X}=\\mathbf{I}\\]\nNotice the commutative property at work here - this is because \\(\\mathbf{X}\\) is square, such that \\(n=m\\), so \\(\\mathbf{X}\\) and \\({\\mathbf{X^{-1}}}\\) have the same dimensions."
  },
  {
    "objectID": "matrix24s.html#determinant",
    "href": "matrix24s.html#determinant",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nAn important characteristic of square matrices is the determinant. The determinant, denoted \\(|X|\\), is a scalar; every square matrix has one. We evaluate the determinant by examining the cross-products of the matrice’s elements. This is simple in the 2x2 matrix, less so in larger matrices.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right| = x_{11}x_{22}-x_{12}x_{21}\\]"
  },
  {
    "objectID": "matrix24s.html#determinant-1",
    "href": "matrix24s.html#determinant-1",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nIn the 2x2 matrix, the determinant is the cross-product of the main diagonals minus the cross-product of the off-diagonals.\n\\[ |\\mathbf{X_{1}}|=\\left|\n\\begin{array}{cc}\n2 & 4\\\\\n6& 3 \\\\\n\\end{array} \\right| = 2 \\cdot 3-4 \\cdot 6 = -18\n\\]\nThe determinant is -18; \\(|\\mathbf{X_{1}}|=-18\\)."
  },
  {
    "objectID": "matrix24s.html#determinant-2",
    "href": "matrix24s.html#determinant-2",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nThis matrix has a determinant of zero - this is an important case.\n\\[ |\\mathbf{X_{2}}|=\\left|\n\\begin{array}{cc}\n3 & 6\\\\\n2& 4 \\\\\n\\end{array} \\right| = 3 \\cdot 4-6 \\cdot 2 =0\n\\] A matrix with determinant zero is singular. A singluar matrix has no inverse."
  },
  {
    "objectID": "matrix24s.html#singular-matrices",
    "href": "matrix24s.html#singular-matrices",
    "title": "Matrix algebra basics",
    "section": "Singular matrices",
    "text": "Singular matrices\nThere are some important conditions that will produce a determinant of zero and thus a singular matrix:\n\nIf all the elements of any row or column of a matrix are equal to zero, then the determinant is zero.\nIf two rows or columns of a matrix are identical, the determinant is zero.\nIf a row or column of a matrix is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing-2",
    "href": "matrix24s.html#least-squares-foreshadowing-2",
    "title": "Matrix algebra basics",
    "section": "Least Squares Foreshadowing",
    "text": "Least Squares Foreshadowing\nFor the linear model in least squares, this is important because computing estimates of \\(\\beta\\) requires inverting a matrix. Let’s derive \\(\\beta\\) in matrix notation to see how:\nThe estimated model is:\n\\[\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}}\\] Minimizing \\(\\widehat{\\mathbf{\\epsilon}}'\\widehat{\\mathbf{\\epsilon}}\\) (skipping the math for now), we get\n\\[(\\mathbf{X'X}) \\widehat{\\beta}=\\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares",
    "href": "matrix24s.html#foreshadowing-least-squares",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThe matrix \\((\\mathbf{X'X})\\) gives the sums of squares (on the main diagonal) and the sums of cross products (in the off-diagonals) of all the \\(\\mathbf{X}\\) variables; the matrix is symmetric. Since \\(\\beta\\) is the unknown vector in this equation, solve for \\(\\beta\\) by dividing both sides by \\((\\mathbf{X'X})\\) - in matrix terms, we premultiply each side by \\((\\mathbf{X'X)^{-1}}\\):\n\\[(\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) \\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares-1",
    "href": "matrix24s.html#foreshadowing-least-squares-1",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nAs we know, \\((\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) = \\mathbf{I}\\). So,\n\\[\\mathbf{I}\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\] or \\[\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]\nEstimating \\(\\widehat{\\beta}\\) requires inverting \\(\\mathbf{(X'X)}\\) If the matrix \\(\\mathbf{(X'X)}\\) is singular, then \\(\\mathbf{(X'X)^{-1}}\\) does not exist, and we cannot estimate \\(\\widehat{\\beta}\\). Least Squares fails."
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares-2",
    "href": "matrix24s.html#foreshadowing-least-squares-2",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThinking specifically of the \\(\\mathbf{X}\\) variables in the model, any of these conditions produce perfect collinearity - estimation fails because the matrix can’t invert.\n\nIf all the elements of any row or column of \\(\\mathbf{X}\\) are equal to zero, then the determinant is zero.\nIf two rows or columns of \\(\\mathbf{X}\\) are identical, the determinant is zero.\nIf a row or column of \\(\\mathbf{X}\\) is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24s.html#examples",
    "href": "matrix24s.html#examples",
    "title": "Matrix algebra basics",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2Example 3\n\n\n\\({\\mathbf X}\\) below has a row of zeros; compute the determinant using the difference of cross-products.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n0 & 0\\\\\n19& 12 \\\\\n\\end{array} \\right| = 0 \\cdot 12-0 \\cdot 19 = 0 \\]\nYou can see in \\({\\mathbf X}\\) that because all the cross-products involve multiplying by zero, the determinant will always be zero; this applies to larger matrices as well.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n5 & 7\\\\\n5& 7 \\\\\n\\end{array} \\right| = 5 \\cdot 7-5 \\cdot 7 =0 \\]\nIn \\({\\mathbf X}\\) it’s easy to see that the cross-products will always be equal to one another if the rows or columns are identical, and the difference between identical values is zero.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n64 & 48\\\\\n16& 12 \\\\\n\\end{array} \\right| = 64 \\cdot 12-48 \\cdot 16 = 768-768=0\\]\nFinally \\({\\mathbf X}\\) shows that if one row or column is a linear combination of other rows or columns, the determinant will be zero; in that matrix, the top row is 4 times the bottom row."
  },
  {
    "objectID": "matrix24s.html#inversion",
    "href": "matrix24s.html#inversion",
    "title": "Matrix algebra basics",
    "section": "Inversion",
    "text": "Inversion\nThe matrix we need to invert, \\(\\mathbf{X'X}\\) is rectangular, so inversion is more complex. I’m going to illustrate the method of cofactor expansion - the purpose here is to give you an idea what software does every time you estimate an OLS model."
  },
  {
    "objectID": "matrix24s.html#minor-of-a-matrix",
    "href": "matrix24s.html#minor-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Minor of a matrix",
    "text": "Minor of a matrix\nThe minor of a matrix is the determinant of a submatrix created by deleting the \\(i\\)th row and the \\(j\\)th column of the full matrix, where the minor is denoted by the element where the deleted rows intersect.\n\\[ \\mathbf{A} =  \\left[\n\\begin{array}{ccc}\na_{11} & a_{12} &a_{13}\\\\\na_{21}& a_{22} &a_{23}\\\\\na_{31} &a_{32} & a_{33}\\\\\n\\end{array} \\right] \\] \\[\\text{so the minor of } a_{11} \\text{ is }\n|\\mathbf{M_{11}}| =  \\left|\n\\begin{array}{cc}\na_{22} &a_{23}\\\\\na_{32} & a_{33}\\\\\n\\end{array} \\right|  =  a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\]"
  },
  {
    "objectID": "matrix24s.html#cofactor-matrix",
    "href": "matrix24s.html#cofactor-matrix",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nThe cofactor of an element (\\(c_{ij}\\)) is the minor with a positive or negative sign depending on whether \\(i+j\\) is odd (negative) or even (positive). This is given by:\n\\[\\theta_{ij}=(-1)^{i+j}|\\mathbf{M_{ij}}|\\]\nso, from the example finding the minor of \\(a_{11}\\), \\(i\\)=1 and \\(j=1\\); their sum is 2 which is even, so the cofactor is \\(+a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\). If we find the cofactor for every element, \\(a_{ij}\\) in a matrix and replace each element with its cofactor, the new matrix is called the cofactor matrix of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24s.html#cofactor-matrix-1",
    "href": "matrix24s.html#cofactor-matrix-1",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nWhat we have so far is the signed matrix of minors:\n\\[\\mathbf{A} = \\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#cofactor-expansion",
    "href": "matrix24s.html#cofactor-expansion",
    "title": "Matrix algebra basics",
    "section": "Cofactor Expansion",
    "text": "Cofactor Expansion\nFind the signed determinant of each 2x2 submatrix.\n\\[\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} = \\left[\n\\begin{array}{rrr}\n10&-10&0\\\\\n-12&0&6\\\\\n-2&5&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#find-the-determinant",
    "href": "matrix24s.html#find-the-determinant",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nUsing the cofactor matrix, \\(\\mathbf{\\Theta_{A}}\\) we perform cofactor expansion in order to find the determinant, \\(|\\mathbf{A}|\\). Cofactor expansion is given by:\n\\[|\\mathbf{A}|=\\sum\\limits_{i,j=1}^{n}a_{ij}\\Theta_{ij}\\]\nwhich means that we take two corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) and sum the products of their elements - this gives us the determinant of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24s.html#find-the-determinant-1",
    "href": "matrix24s.html#find-the-determinant-1",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nSo taking the first row from \\(\\mathbf{A}\\) (1,4,2) and from \\(\\mathbf{\\Theta_{A}}\\) (10,-10,0) above, we compute\n\\[a_{11} \\cdot \\Theta_{A11}+a_{12} \\cdot \\Theta_{A12}+a_{13} \\cdot \\Theta_{A13}\\]\n\\[1 \\cdot 10+ 4 \\cdot -10 + 2 \\cdot 0 = -30\\] This is the determinant of \\(\\mathbf{A}\\) , \\(|\\mathbf{A}|\\). Note that any corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) will produce the same result."
  },
  {
    "objectID": "matrix24s.html#adjoint-matrix",
    "href": "matrix24s.html#adjoint-matrix",
    "title": "Matrix algebra basics",
    "section": "Adjoint Matrix",
    "text": "Adjoint Matrix\nIf we transpose the cofactor matrix, (cof \\(\\mathbf{A'}\\)), the new matrix is called the adjoint matrix, denoted adj\\(\\mathbf{A}\\)=(cof \\(\\mathbf{A'}\\)).\n\\[adj\\mathbf{A} = \\mathbf{\\Theta_{A}'} = \\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#inverting-the-matrix",
    "href": "matrix24s.html#inverting-the-matrix",
    "title": "Matrix algebra basics",
    "section": "Inverting the matrix",
    "text": "Inverting the matrix\nBelieve it or not, this all leads us to inverting the matrix, provided it is nonsingular.\n\\[\\mathbf{A^{-1}}=\\frac{1}{|\\mathbf{A}|}(adj {\\mathbf A})\\] The inverse of A is equal to one over the determinant of A times the adjoint matrix of A. So we’re pre-multiplying \\(adj {\\mathbf A}\\) by the (scalar) determinant, -30."
  },
  {
    "objectID": "matrix24s.html#inverse-of-matrix-a",
    "href": "matrix24s.html#inverse-of-matrix-a",
    "title": "Matrix algebra basics",
    "section": "Inverse of Matrix A",
    "text": "Inverse of Matrix A\n\\[{\\mathbf A^{-1}}=-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n=\\left[\n\\begin{array}{rrr}\n-\\frac{1}{3}&\\frac{2}{5}&\\frac{1}{15}\\\\\n\\frac{1}{3}&0&-\\frac{1}{6}\\\\\n0&-\\frac{1}{5}&\\frac{3}{10}\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#checking-our-work",
    "href": "matrix24s.html#checking-our-work",
    "title": "Matrix algebra basics",
    "section": "Checking our work",
    "text": "Checking our work\nWe can check our work to be sure we’ve inverted correctly by making sure that \\(\\mathbf{A^{-1}A}=\\mathbf{I_{3}}\\); so,\n\\[-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n\\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n=\n-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n-30&0&0\\\\\n0&-30&0\\\\\n0&0&-30\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols",
    "href": "matrix24s.html#connecting-data-matrices-to-ols",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\nLet’s examine these matrices a little to get a feel for what the computation of \\(\\widehat{\\beta}\\) involves: \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\n\\[\n\\mathbf{X}= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \n\\end{matrix}  \\right]\n\\] The dimensions of \\(\\mathbf{X}\\) are \\(n x k\\); the sample size by the number of regressors (including the constant)."
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols-1",
    "href": "matrix24s.html#connecting-data-matrices-to-ols-1",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \n\\end{matrix}  \\right] \\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols-2",
    "href": "matrix24s.html#connecting-data-matrices-to-ols-2",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i}\n\\end{matrix}  \\right] \\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationships among \\(X_k\\)."
  },
  {
    "objectID": "overview24s.html",
    "href": "overview24s.html",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise\n\n\n\n\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding.\n\n\n\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling.\n\n\n\n\nIs not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal.\n\n\n\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here.\n\n\n\n\n\nis a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together.\n\n\n\n\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra\n\n\n\n\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\).\n\n\n\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?\n\n\n\n\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term.\n\n\n\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later.\n\n\n\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "syllabus24.html",
    "href": "syllabus24.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dave Clark\n   LNG 57; LN2430\n   dclark@binghamton.edu\n   clavedark\noffice hours: M 1:30-3:30pm\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL"
  },
  {
    "objectID": "syllabus24.html#seminar-description",
    "href": "syllabus24.html#seminar-description",
    "title": "Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret an informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "syllabus24.html#course-purpose",
    "href": "syllabus24.html#course-purpose",
    "title": "Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "syllabus24.html#learning-objectives",
    "href": "syllabus24.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "syllabus24.html#reading",
    "href": "syllabus24.html#reading",
    "title": "Syllabus",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways.\nIn addition, I’ll assign a small number of articles over the course of the term."
  },
  {
    "objectID": "syllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "syllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it.\n\n\nHow to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#on-reading-or-how-to-read",
    "href": "syllabus24.html#on-reading-or-how-to-read",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "On reading, or How to Read",
    "text": "On reading, or How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever. \\\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing. \\\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#course-requirements",
    "href": "syllabus24.html#course-requirements",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Requirements",
    "text": "Course Requirements\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in \\(\\LaTeX\\) or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n“D”, “60-69%”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;70%"
  },
  {
    "objectID": "syllabus24.html#how-to-read",
    "href": "syllabus24.html#how-to-read",
    "title": "Syllabus",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#course-requirements-and-grades",
    "href": "syllabus24.html#course-requirements-and-grades",
    "title": "Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "syllabus24.html#course-schedule",
    "href": "syllabus24.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013\nWeek 1. 17 Jan Introduction, Regression Discussion\n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "pdf.html",
    "href": "pdf.html",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "",
    "text": "Prof. Dave Clark\n   LNG 57\n   dclark@binghamton.edu\n   clavedark\n\n\n\n\n\n   Kathleen Bannon\n   kbannon1@binghamton.edu\n\n\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL\n\n\n\n\n\n   Thursday\n   1:15-2:15\n   CW 309"
  },
  {
    "objectID": "pdf.html#seminar-description",
    "href": "pdf.html#seminar-description",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret and informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "pdf.html#course-purpose",
    "href": "pdf.html#course-purpose",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "pdf.html#learning-objectives",
    "href": "pdf.html#learning-objectives",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "pdf.html#reading",
    "href": "pdf.html#reading",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways."
  },
  {
    "objectID": "pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it."
  },
  {
    "objectID": "pdf.html#how-to-read",
    "href": "pdf.html#how-to-read",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "pdf.html#course-requirements-and-grades",
    "href": "pdf.html#course-requirements-and-grades",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in \\(\\LaTeX\\) or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "pdf.html#course-schedule",
    "href": "pdf.html#course-schedule",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013}\nWeek 1. 17 Jan Introduction, Regression Discussion \n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "syllabus24.html#attendance",
    "href": "syllabus24.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class."
  },
  {
    "objectID": "syllabus24.html#academic-integrity",
    "href": "syllabus24.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "syllabus24.html#course-policies",
    "href": "syllabus24.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Font Awesome Quarto Extension",
    "section": "",
    "text": "This extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; fa &lt;icon-name&gt; &gt;}}\nOptional &lt;group&gt;, &lt;size=...&gt;, and &lt;title=...&gt;:\n{{&lt; fa &lt;group&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;title=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n{{&lt; fa brands twitter size=2xl &gt;}} (HTML only)\n\n\n\n{{&lt; fa brands github size=5x &gt;}} (HTML only)\n\n\n\n{{&lt; fa battery-half size=Huge &gt;}}\n\n\n\n{{&lt; fa envelope title=\"An envelope\" &gt;}}\n\n\n\n\nNote that the icon sets are currently not perfectly interchangeable across formats:\n\nhtml uses FontAwesome 6.4.2\npdf uses the fontawesome5 package, based on FontAwesome 5.\nOther formats are currently not supported, but PRs are always welcome!"
  },
  {
    "objectID": "syllabus24pdf.html",
    "href": "syllabus24pdf.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dave Clark\n   LNG 57; LN2430\n   dclark@binghamton.edu\n   clavedark\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL"
  },
  {
    "objectID": "syllabus24pdf.html#seminar-description",
    "href": "syllabus24pdf.html#seminar-description",
    "title": "Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret and informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "syllabus24pdf.html#course-purpose",
    "href": "syllabus24pdf.html#course-purpose",
    "title": "Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "syllabus24pdf.html#learning-objectives",
    "href": "syllabus24pdf.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "syllabus24pdf.html#reading",
    "href": "syllabus24pdf.html#reading",
    "title": "Syllabus",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways.\nIn addition, I’ll assign a small number of articles over the course of the term."
  },
  {
    "objectID": "syllabus24pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "syllabus24pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it.\n\n\nHow to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24pdf.html#course-requirements-and-grades",
    "href": "syllabus24pdf.html#course-requirements-and-grades",
    "title": "Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "syllabus24pdf.html#course-policies",
    "href": "syllabus24pdf.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "syllabus24pdf.html#course-schedule",
    "href": "syllabus24pdf.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013\nWeek 1. 17 Jan Introduction, Regression Discussion\n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "data.html#get-to-know-your-data-explore-etc",
    "href": "data.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "data.html#data-generating-process",
    "href": "data.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\nA Terrible Map\n\nWar outcomes\n\nwar outcome/duration data\n\nObservability\n\nAsking the right question"
  },
  {
    "objectID": "data.html#data-in-the-regression-context",
    "href": "data.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "data.html#data",
    "href": "data.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "data.html#types-of-variables",
    "href": "data.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "data.html#discrete-variables",
    "href": "data.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "data.html#continuous-variables",
    "href": "data.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "data.html#levels-of-measurement",
    "href": "data.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "data.html#levels-of-measurement-and-models",
    "href": "data.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "data.html#describe-these-data",
    "href": "data.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "data.html#describe-these-data-1",
    "href": "data.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "data.html#overstaying-terms",
    "href": "data.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata.html",
    "href": "thinkingdata.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata.html#data-generating-process",
    "href": "thinkingdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\n\nA Terrible Map\n\n\n\nWar outcomes\n\nwar outcome/duration data\n\n\n\nObservability\n\n\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata.html#data-in-the-regression-context",
    "href": "thinkingdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata.html#data",
    "href": "thinkingdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata.html#types-of-variables",
    "href": "thinkingdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata.html#discrete-variables",
    "href": "thinkingdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata.html#continuous-variables",
    "href": "thinkingdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata.html#levels-of-measurement",
    "href": "thinkingdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata.html#levels-of-measurement-and-models",
    "href": "thinkingdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata.html#describe-these-data",
    "href": "thinkingdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata.html#describe-these-data-1",
    "href": "thinkingdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata.html#overstaying-terms",
    "href": "thinkingdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html",
    "href": "thinkingdata/thinking-data24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata/thinking-data24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data-generating-process",
    "href": "thinkingdata/thinking-data24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\n\nA Terrible Map\n\n\n\nWar outcomes\n\nwar outcome/duration data\n\n\n\nObservability\n\n\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data-in-the-regression-context",
    "href": "thinkingdata/thinking-data24.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data",
    "href": "thinkingdata/thinking-data24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#types-of-variables",
    "href": "thinkingdata/thinking-data24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#discrete-variables",
    "href": "thinkingdata/thinking-data24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#continuous-variables",
    "href": "thinkingdata/thinking-data24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#levels-of-measurement",
    "href": "thinkingdata/thinking-data24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#levels-of-measurement-and-models",
    "href": "thinkingdata/thinking-data24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#describe-these-data",
    "href": "thinkingdata/thinking-data24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#describe-these-data-1",
    "href": "thinkingdata/thinking-data24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#overstaying-terms",
    "href": "thinkingdata/thinking-data24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata/tdata.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata/tdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#data-generating-process",
    "href": "thinkingdata/tdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\nA Terrible Map\n\nWar outcomes\n\nwar outcome/duration data\n\nObservability\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata/tdata.html#data-in-the-regression-context",
    "href": "thinkingdata/tdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata/tdata.html#data",
    "href": "thinkingdata/tdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata/tdata.html#types-of-variables",
    "href": "thinkingdata/tdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata/tdata.html#discrete-variables",
    "href": "thinkingdata/tdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata/tdata.html#continuous-variables",
    "href": "thinkingdata/tdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata/tdata.html#levels-of-measurement",
    "href": "thinkingdata/tdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata/tdata.html#levels-of-measurement-and-models",
    "href": "thinkingdata/tdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata/tdata.html#describe-these-data",
    "href": "thinkingdata/tdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#describe-these-data-1",
    "href": "thinkingdata/tdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#overstaying-terms",
    "href": "thinkingdata/tdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "tdata.html#get-to-know-your-data-explore-etc",
    "href": "tdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "tdata.html#data-generating-process",
    "href": "tdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "tdata.html#data-in-the-regression-context",
    "href": "tdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "tdata.html#data",
    "href": "tdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "tdata.html#types-of-variables",
    "href": "tdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "tdata.html#discrete-variables",
    "href": "tdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "tdata.html#continuous-variables",
    "href": "tdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "tdata.html#levels-of-measurement",
    "href": "tdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "tdata.html#levels-of-measurement-and-models",
    "href": "tdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "tdata.html#describe-these-data",
    "href": "tdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "tdata.html#describe-these-data-1",
    "href": "tdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "tdata.html#overstaying-terms",
    "href": "tdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinking-data24.html",
    "href": "thinking-data24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinking-data24.html#get-to-know-your-data-explore-etc",
    "href": "thinking-data24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinking-data24.html#data-generating-process",
    "href": "thinking-data24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "thinking-data24.html#data-in-the-regression-context",
    "href": "thinking-data24.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinking-data24.html#data",
    "href": "thinking-data24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinking-data24.html#types-of-variables",
    "href": "thinking-data24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinking-data24.html#discrete-variables",
    "href": "thinking-data24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinking-data24.html#continuous-variables",
    "href": "thinking-data24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinking-data24.html#levels-of-measurement",
    "href": "thinking-data24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinking-data24.html#levels-of-measurement-and-models",
    "href": "thinking-data24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinking-data24.html#describe-these-data",
    "href": "thinking-data24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinking-data24.html#describe-these-data-1",
    "href": "thinking-data24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinking-data24.html#overstaying-terms",
    "href": "thinking-data24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "tdata.html#a-terrible-map",
    "href": "tdata.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "tdata.html#war-outcomes",
    "href": "tdata.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "tdata.html#observability",
    "href": "tdata.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "tdata.html#asking-the-right-question",
    "href": "tdata.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinking-data24.html#a-terrible-map",
    "href": "thinking-data24.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "thinking-data24.html#war-outcomes",
    "href": "thinking-data24.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "thinking-data24.html#observability",
    "href": "thinking-data24.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "thinking-data24.html#asking-the-right-question",
    "href": "thinking-data24.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinking-data24.html#ooof",
    "href": "thinking-data24.html#ooof",
    "title": "Thinking About Data",
    "section": "ooof",
    "text": "ooof\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1"
  },
  {
    "objectID": "tdata.html#ooof",
    "href": "tdata.html#ooof",
    "title": "Thinking About Data",
    "section": "ooof",
    "text": "ooof\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1"
  },
  {
    "objectID": "thinking-data24.html#data-generating-process-1",
    "href": "thinking-data24.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinking-data24.html#war-outcomes-lake92-p.-31",
    "href": "thinking-data24.html#war-outcomes-lake92-p.-31",
    "title": "Thinking About Data",
    "section": "War outcomes (Lake (1992) p. 31)",
    "text": "War outcomes (Lake (1992) p. 31)\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74\n47\n12 1\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "bivariate24.html",
    "href": "bivariate24.html",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Regression in any form is based on the conditional expectation of \\(Y\\) - the expected value of \\(Y\\) is conditional on some \\(X\\)s, but we don’t know the actual conditions or effects of those \\(X\\)s. So we can write the regression like this:\n\\[E[y|X_{1}, \\ldots,X_{k}]= \\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\]\n\n\nLet \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances.\n\n\n\nThe predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\).\n\n\n\nThe differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\).\n\n\n\nRestating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#regression",
    "href": "bivariate24.html#regression",
    "title": "The Bivariate Model",
    "section": "Regression",
    "text": "Regression\nRegression in any form is based on the conditional expectation of \\(Y\\) - the expected value of \\(Y\\) is conditional on some \\(X\\)s, but we don’t know the actual conditions or effects of those \\(X\\)s. So we can write the regression like this:\n\\[E[y|X_{1}, \\ldots,X_{k}]= \\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\]"
  },
  {
    "objectID": "bivariate24.html#regression-1",
    "href": "bivariate24.html#regression-1",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Let \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "bivariate24.html#linear-predictions-residuals",
    "href": "bivariate24.html#linear-predictions-residuals",
    "title": "The Bivariate Model",
    "section": "Linear predictions, Residuals",
    "text": "Linear predictions, Residuals\nThe predicted points that form the line are \\(\\widehat{Y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\).\nThe differences between those predicted points, \\(\\widehat{Y_{i}}\\) and the observed values \\(Y_i\\) are:\n\\[Y_{i}-\\widehat{Y_{i}} = e  \\\\\n= Y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i} \\nonumber \\\\\n= Y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\]\nThese are the residuals, \\(e\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#in-matrix",
    "href": "bivariate24.html#in-matrix",
    "title": "The Bivariate Model",
    "section": "in matrix",
    "text": "in matrix\nRestating in matrix notation:\n\\[\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{e_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{e_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-i",
    "href": "bivariate24.html#getting-to-ols-i",
    "title": "The Bivariate Model",
    "section": "Getting to OLS I",
    "text": "Getting to OLS I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-1",
    "href": "bivariate24.html#ols-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nTo minimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]\n\\(\\ldots\\) illustrating explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24.html#ols-1-1",
    "href": "bivariate24.html#ols-1-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-ii",
    "href": "bivariate24.html#getting-to-ols-ii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS II",
    "text": "Getting to OLS II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero.\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling that the mean of a variable\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]\nSo, substituting these equations into\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\\]\nand solving for \\(\\hat{\\beta}_1\\) gives:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-iii",
    "href": "bivariate24.html#getting-to-ols-iii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS III",
    "text": "Getting to OLS III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24.html#matrices",
    "href": "bivariate24.html#matrices",
    "title": "The Bivariate Model",
    "section": "Matrices",
    "text": "Matrices\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#covariation-xy",
    "href": "bivariate24.html#covariation-xy",
    "title": "The Bivariate Model",
    "section": "Covariation X’y",
    "text": "Covariation X’y\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\) and \\(X_{2}\\)."
  },
  {
    "objectID": "bivariate24.html#matrices-and-regression",
    "href": "bivariate24.html#matrices-and-regression",
    "title": "The Bivariate Model",
    "section": "Matrices and Regression",
    "text": "Matrices and Regression\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \\nonumber \\\\\n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#check-this-out",
    "href": "bivariate24.html#check-this-out",
    "title": "The Bivariate Model",
    "section": "check this out",
    "text": "check this out\nagain: \\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \\nonumber \\\\\n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#matrices-and-regression-1",
    "href": "bivariate24.html#matrices-and-regression-1",
    "title": "The Bivariate Model",
    "section": "Matrices and Regression",
    "text": "Matrices and Regression\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \\nonumber \\\\\n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \\nonumber \\\\\n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \\nonumber \\\\\n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\n\\[\n\\begin{align}\nE(\\mathbf{\\epsilon\\epsilon'})= E \\left[\n\\begin{array}{c}\n\\epsilon_{1} \\\\\n\\vdots \\\\\n\\epsilon_{n}\\\\\n\\end{array} \\right]\nE \\left[\n\\begin{array}{cccc}\n\\epsilon_{1} & \\cdots &\\epsilon_{n}\\\\\n\\end{array} \\right] \\\\ ~\\\\\n= E\n\\left[\n\\begin{array}{cccc}\n\\epsilon_{1}^{2} & \\epsilon_{1}\\epsilon_{2} &\\cdots &\\epsilon_{1}\\epsilon_{n}\\\\\n\\epsilon_{2}\\epsilon_{1}& \\epsilon_{2}^{2} &\\cdots &\\epsilon_{2}\\epsilon_{n}\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n\\epsilon_{n}\\epsilon_{1} &\\epsilon_{n}\\epsilon_{2} &\\cdots & \\epsilon_{n}^{2}\\\\\n\\end{array} \\right]\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon-1",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[\nE(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0\\\\\n0&\\sigma^{2} &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & \\sigma^{2}\\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0\\\\\n0&1 &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & 1\\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I}\n\\]\nThis is the variance-covariance matrix of the disturbances (var-cov(e)); it is symmetric, the main diagonal containing the variances, the off-diagonal containing the covariances."
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\nThe variance-covariance of the estimate, \\(\\widehat{\\beta}\\) derives as follows:\n\\[\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'y}\\]\nsubstituting \\(\\mathbf{X\\beta}+\\epsilon\\) for \\(\\mathbf{y}\\), we get\n\\[\n\\begin{align}\n\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'(X\\beta + \\epsilon)}  \\nonumber \\\\  \n= \\mathbf{(X'X)^{-1}}\\mathbf{X'X\\beta}+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\  \n=\\beta+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\\n\\widehat{\\mathbf{\\beta}}-\\beta=\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta-1",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta-2",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] = \\] \\(~\\)\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k)\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "bivariate24.html#standard-errors-of-beta_k",
    "href": "bivariate24.html#standard-errors-of-beta_k",
    "title": "The Bivariate Model",
    "section": "Standard Errors of \\(\\beta_k\\)",
    "text": "Standard Errors of \\(\\beta_k\\)\n\\[\n\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_k,\\beta_2) &\\cdots & \\sqrt{var(\\beta_k)}\\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#property-1",
    "href": "bivariate24.html#property-1",
    "title": "The Bivariate Model",
    "section": "Property 1",
    "text": "Property 1\nIf \\(X' \\widehat{\\epsilon} = 0\\) holds, then the following properties exist:\n\n\n\n\n\n\nProposition\n\n\n\nEach \\(x\\) variable (each column vector of \\(\\mathbf{X}\\)) is uncorrelated with \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#property-2",
    "href": "bivariate24.html#property-2",
    "title": "The Bivariate Model",
    "section": "Property 2",
    "text": "Property 2\nAssuming a constant in the matrix \\(\\mathbf{X}\\),\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\sum\\limits_{i-1}^{n} \\epsilon_i = 0\\)\nbecause each element of the matrix \\(X' \\widehat{\\epsilon}\\) would be nonzero due to the constant; only by multiplying by \\(\\epsilon_i=0\\) would each element equal zero, and then the sum must be zero."
  },
  {
    "objectID": "bivariate24.html#property-3",
    "href": "bivariate24.html#property-3",
    "title": "The Bivariate Model",
    "section": "Property 3",
    "text": "Property 3\n\n\n\n\n\n\nProposition\n\n\n\nThe mean of the residuals is zero.\nIf the sum of the residuals is zero, that sum divided by \\(N\\) must also be zero."
  },
  {
    "objectID": "bivariate24.html#property-4",
    "href": "bivariate24.html#property-4",
    "title": "The Bivariate Model",
    "section": "Property 4",
    "text": "Property 4\n\n\n\n\n\n\nProposition\n\n\n\nThe regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, \\(\\mathbf{X}\\) and \\(y\\).\n\\[\n\\begin{align}\n\\epsilon = y - X\\widehat{\\beta} \\nonumber \\\\ \\\\\n\\text{multiplying by} ~  N^{-1} \\nonumber \\\\ \\\\\n\\bar{\\epsilon} = \\bar{y} - \\bar{X} \\widehat{\\beta} = 0  \\nonumber\n\\end{align}\n\\]\n\\(\\bar{y} - \\bar{X} \\widehat{\\beta}=0\\) implies \\(\\bar{y} = \\bar{X} \\widehat{\\beta}\\), and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane \\(\\bar{y} - \\bar{X} \\widehat{\\beta}\\), the mean of the residuals equals zero, implying the regression line or hyperplane passes through it."
  },
  {
    "objectID": "bivariate24.html#linear-predictions",
    "href": "bivariate24.html#linear-predictions",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\)."
  },
  {
    "objectID": "bivariate24.html#residuals",
    "href": "bivariate24.html#residuals",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#in-matrix-notation",
    "href": "bivariate24.html#in-matrix-notation",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Restating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#the-equation",
    "href": "bivariate24.html#the-equation",
    "title": "The Bivariate Model",
    "section": "The equation",
    "text": "The equation\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#mathbfxx",
    "href": "bivariate24.html#mathbfxx",
    "title": "The Bivariate Model",
    "section": "\\(\\mathbf{X'X}\\)",
    "text": "\\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#the-components",
    "href": "bivariate24.html#the-components",
    "title": "The Bivariate Model",
    "section": "The components",
    "text": "The components\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#components---covariation-mathbfxx",
    "href": "bivariate24.html#components---covariation-mathbfxx",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'X}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#components---covariation-mathbfxy",
    "href": "bivariate24.html#components---covariation-mathbfxy",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'y}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'y}\\)\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation",
    "href": "bivariate24.html#ols-matrix-derivation",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation-1",
    "href": "bivariate24.html#ols-matrix-derivation-1",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\   \n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "tdata.html#data-generating-process-1",
    "href": "tdata.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinkingdata24.html",
    "href": "thinkingdata24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata24.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata24.html#data-generating-process",
    "href": "thinkingdata24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "thinkingdata24.html#data-generating-process-1",
    "href": "thinkingdata24.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinkingdata24.html#a-terrible-map",
    "href": "thinkingdata24.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "thinkingdata24.html#war-outcomes",
    "href": "thinkingdata24.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "thinkingdata24.html#observability",
    "href": "thinkingdata24.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "thinkingdata24.html#asking-the-right-question",
    "href": "thinkingdata24.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinkingdata24.html#data",
    "href": "thinkingdata24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata24.html#types-of-variables",
    "href": "thinkingdata24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata24.html#discrete-variables",
    "href": "thinkingdata24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata24.html#continuous-variables",
    "href": "thinkingdata24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata24.html#levels-of-measurement",
    "href": "thinkingdata24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata24.html#levels-of-measurement-and-models",
    "href": "thinkingdata24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata24.html#describe-these-data",
    "href": "thinkingdata24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata24.html#describe-these-data-1",
    "href": "thinkingdata24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata24.html#overstaying-terms",
    "href": "thinkingdata24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "probability24s.html",
    "href": "probability24s.html",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns.\n\n\n\n\nProbability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value).\n\n\n\n\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita.\n\n\n\n\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely.\n\n\n\n\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war.\n\n\n\n\nThese four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information.\n\n\n\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "multivariate24.html",
    "href": "multivariate24.html",
    "title": "The Multivariate Model",
    "section": "",
    "text": "The Multivariate Model\nThe key issue in the multivariate regression is statistical control - our effort to mimic experimental conditions.\n\n\nExperimental Control\nIn an experiment, the research randomizes on all variables except the variable of interest (the treatment). This means the sample and all conditions the subjects in the sample face are random, or are explicitly fixed by the researcher. For example, in a diet pill study, the sample will be randomized with respect to demographics, health, weight, etc. The subjects’ diets during the study will be fixed or controlled by the researcher so they all eat the same. The lone exception will the administration of the diet pill - the treatment group will get the pill, the control group will get a placebo.\n\n\nIn the regression setting\nOne of the reasons we use models like the linear regression is to mimic the conditions of an experiment. The model allows us to isolate the effect of \\(x_1\\) while accounting for the independent effect of \\(x_2\\) on \\(y\\). So we’re able to say ``the effect of \\(x_1\\), controlling for the influence of \\(x_2\\) is \\(\\beta_1\\).’’\n\n\nHolding constant at mean\nThis linear regression :\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2} x_2 + \\epsilon_i\\]\nis equivalent to this one ::\n\\[y_i = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\widehat{\\beta_2^*}( x_2-\\bar{x_2}) + \\epsilon_i\\]\nOnly the constant shifts (think back to linear transformations; \\(\\widehat{\\beta_0}\\) shifts by a factor of \\(-k\\widehat{\\beta}\\)).\n\n\nConstant at mean\nWhat this shows is that even without transforming \\(x_2\\), we are holding its effect constant at its mean while we estimate the effect of \\(x_1\\).\n\n\nConditional Means\nLet \\(D_1\\) be a binary variable:\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}D_1 + \\hat{\\beta_2} x_1 + \\epsilon_i\\]\nThe partial effect of \\(D_1\\) measures the difference between the means for \\(D_1=0,1\\), given the mean effect of \\(x_1\\).\nSuppose we estimate a model predicting anti-government protests, and we think the main predictor will be liberal political institutions, controlling for per capita wealth. We think as liberalism increases, so do protests; they decrease with authoritarianism. So the regression looks like this:\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nWhat is the effect of liberal political institutions controlling for GDP per capita?\nWhen we ``control’’ for a variable like GDP per capita, we are trying to evaluate two things:\n\nthe direct or immediate effect of institutions on protests -this is because liberalism might promote free assembly.\nthe indirect of effect of institutions on GDP per capita, and then on protests - this is because liberalism might promote economic growth and development, and thereby influence protests.\n\nIn our regression\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nthis means \\(\\hat{\\beta_1}\\) is the effect of liberalism that does not go through GDP (or any other control variable).\nFrom our regression,\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nPolitical institutions influence GDP per capita:\n\\[\\text{GDPpc}_i = \\hat{\\gamma_0} + \\hat{\\gamma_1}\\text{Politics}_i  + \\upsilon_i\\]\nSubstituting,\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i +\\hat{\\beta_2}(\\hat{\\gamma_0} + \\hat{\\gamma_1}\\text{Politics}_i  + \\upsilon_i )+ \\epsilon_i\\]\nso, the partial effect of liberal institutions is\n\\[\\frac{\\partial(protests)}{\\partial(politics)} = \\hat{\\beta_1} \\]\nThe effect is partial because it excludes the effect of politics that runs through GDP per capita. Here’s the total effect:\n\\[\\frac{\\partial(protests)}{\\partial(politics)} = \\hat{\\beta_1} + \\hat{\\beta_2}\\hat{\\gamma_1}\\]\nNotice that if liberalism has {} on GDP, so \\(\\hat{\\gamma_1}=0\\), then there is no indirect effect and the partial and total effects are the same.\n\n\nCounterfactuals\nAnother way to think about statistical control is to think about counterfactuals we’d want to evaluate:\n\ndemocracy promotes protests; but what if the state is rich?\ndemocracy promotes protests; but what if the state is poor?\n\n\n\nRandomization\nThe intuition is the same as in the experimental ideal. In the experiment the control and treatment groups are randomized with respect to all things but the treatment itself. In the regression, we want observations randomized over dimensions like wealth (i.e., we have rich and poor, etc) and all other things except the treatment itself - liberal institutions. If it were possible, we might collect data on protests and GDP but for states that are otherwise exactly the same.\n\n\nOther ways to exert control\nCollecting data on protests and GDP for states that are otherwise exactly the same - this the foundation of what are called methods. Matching is aimed at mimicking experimental design, exerting control via sampling. is a related technique that estimates weights for the control so it more closely resembles the treatment group.\n\n\nFrisch-Waugh-Lovell Theorem\nIn the linear least squares regression \\(y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\epsilon\\) produces an estimate for \\(\\hat{\\beta_1}\\) that is the same as the estimate of \\(\\hat{\\beta_1^*}\\) produced by estimating the regression \\(y\\) on \\(x_1\\), saving the residuals \\(\\hat{r_1}\\), then regressing \\(x_1\\) on \\(x_2\\), computing the residuals, \\(\\hat{r_2}\\), and then regressing \\(\\hat{r_1}\\) on \\(\\hat{r_2}\\).\n\n\\(\\hat{r_1}\\) measures the part of \\(y\\) that is unrelated to \\(x_1\\).\nThe regression \\(x_1=\\beta_0+ \\beta_2 x_2\\) measures the overlapping (correlated) parts of \\(x_1\\) and \\(x_2\\).\n\\(\\hat{r_2}\\) measures the unrelated parts of \\(x_1\\) and \\(x_2\\), the part of \\(x_1\\) unrelated to \\(x_2\\).\n\\(\\hat{r_1}\\) measures the part of \\(x_2\\) that is totally independent of \\(x_1\\)\nthe regression \\(\\hat{r_1}=\\beta_0+ \\beta_1^* \\hat{r_2}\\) estimates \\(\\beta_1^*\\) which measures the effect of the part of \\(x_1\\) that is unrelated to \\(x_2\\).\n\\(\\beta_1^*\\) is the partial effect of \\(x_1\\); we measured that part of \\(x_1\\) using \\(\\hat{r_1}\\).\n\n\n\nRegression Anatomy\nOne final way to think about this is a variant on FWL called ``regression anatomy’’ by Angrist and Pischke in their terrific book Mostly Harmless Econometrics.\n\n\nAnatomy\nIn the bivariate case, the slope on \\(\\beta_1\\) is:\n\\[\\beta_1 = \\frac{cov(y_i, x_i)}{V(x_i)}\\]\nIn the multivariate case, the slope on \\(\\beta_1\\) is:\n\\[\\beta_1 = \\frac{cov(y_i, \\tilde{x_{k,i}})}{V(\\tilde{x_{k,i}}) }\\]\nwhere \\(\\tilde{x_{k,i}}\\) refers to the residuals from the regression of \\(x_{k,i}\\) on \\(\\mathbf{X}\\), where \\(X\\) is all other right-side variables.\n\nThe residual from that regression measures the part of \\(x\\) that is unrelated to \\(X\\).\nThe estimate of \\(\\beta_1\\) then is the correlation of \\(y\\) and the part of \\(x\\) that is unrelated to \\(X\\).\nSince \\(\\tilde{x_{k,i}}\\) is unrelated to \\(X\\), its correlation to \\(y\\) is purged of any part through \\(X\\), and is therefore partial.\n\n\n\nOmitted Variable Bias\nIn the population, the true regression is:\n\\[y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\widehat{\\beta_2}x_2 + \\varepsilon \\]\nIn our sample, the regression we estimate is:\n\\[y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\varepsilon \\]\nSo the model omits a variable. What’s the effect?\n\n\nOmitted Variable Bias\n\nIn the bivariate regression, the estimate is \\(\\widehat{\\beta_1}\\)\nIn the multivariate regression, the estimate is \\(\\widetilde{\\beta_1} = \\widehat{\\beta_1} + \\widehat{\\beta_2}\\widetilde{\\delta_1}\\). That is, the estimate for \\(\\widetilde{\\beta_1}\\) now accounts for the relationship of \\(x_1,x_2\\), measured in \\(\\widetilde{\\delta_1}\\). \\(\\widetilde{\\beta_1}\\) now measures the partial effect of \\(x_1\\) on \\(y\\).\nIf we exclude \\(x_2\\), we misestimate \\(\\widehat{\\beta_1}\\) by \\(\\pm \\widehat{\\beta_2}\\widetilde{\\delta_1}\\)\n\n\n\n\nEvaluating an Estimator\nWhat criteria make for a “good” estimator?\n\nUnbiasedness:\n\n\nIs the expected value of \\(\\hat{\\beta}\\) equal to \\(\\beta\\)?\nHow far away do we expect \\(\\hat{\\beta}\\) to be from \\(\\beta\\)?\n\\(E[\\hat{\\beta} - \\beta]\\)\nThe estimator for which this quantity is smallest is the least biased.\n\nBias will always exist, but we want it to be as small as possible, and random (not systematic).\n\nEfficiency:\n\n\nEfficiency measures how close to the true \\(\\beta\\) we are {}.\nEfficiency describes the average size of bias; the average distance of \\(E[\\hat{\\beta} - \\beta]\\).\nThis is about the size of the variance; large variance estimates will, on average, miss the true \\(\\beta\\) by a lot. Small variance estimators will miss the true \\(\\beta\\) by a little, on average.\n\n\nConsistency:\n\n\nConsistency can be thought of as large-sample-unbiasedness.\n\\(E[\\hat{\\beta} - \\beta] \\rightarrow\\) as \\(N \\rightarrow \\infty\\)\nThis is considerably less important to us than bias and efficiency. In part, this is due to the reality that our (non experimental) data are small samples in many cases. It’s also true OLS has good small sample properties.\nMLE does not have good small sample properties, and relies strongly on consistency.\n\n\n\nAssumptions\nUnder the following four assumptions, the OLS estimator \\(\\hat{\\beta}\\) is an unbiased estimator of \\(\\beta\\):\n\nLinear in parameters.\nRandom Sampling.\nZero Conditional Mean of Disturbances.\nNo Perfect Collinearity.\n\n\n\n\n\n\n\nTheorem :Unbiasedness of OLS:\n\n\n\nUnder these four assumptions, OLS estimators are unbiased estimators of the population parameters:\n\\[\nE[{\\widehat{\\beta_{j}}]= \\beta_{j} ~\\forall ~ j} \\nonumber\n\\]\n\n\n\n\nOLS\nIf the model also meets these two assumptions, then the OLS estimator has the smallest variance of all estimators:\n\nHomoskedastic disturbances; \\(Var(u|x_1,x_2,\\ldots,x_k)=\\sigma^2\\).\nUncorrelated disturbances; \\(cov(u_i,u_j|x_1,x_2,\\ldots,x_k)=0\\).\n\nIf Assumptions 1-6 are all met, then the model is BLUE and thus satisfies the Gauss-Markov Theorem. Additionally, these assumptions provide us the first two moments of the sampling distribution for the \\(\\hat{\\beta}\\)s.\n\n\n\n\n\n\nTheorem: Gauss Markov\n\n\n\nUnder these assumptions, the OLS estimator is unbiased and has the smallest variance among all linear unbiased estimators.\n\n\n\n\nOLS\nHowever, in order to talk precisely about the uncertainty surrounding the point estimates, we need to make one more assumption about the error term:\n\nThe disturbances, \\(u_i\\) are independent of the \\(X\\)s and are normally distributed: \\(u \\sim N(0,\\sigma^{2})\\).\n\n\n\nIf assumptions fail \\(\\ldots\\)\n\nsample is not random - bias.\n\\(E[u | X] \\neq 0\\) - endogeneity; bias and standard errors are too small.\nperfect collinearity - matrix is singular, regression fails.\ncorrelated \\(X\\) variables (imperfect collinearity) - unbiased estimates, inefficient standard errors.\nerrors are not identically distributed (heteroskedastic) - inefficiency.\nerrors are not independently distributed (correlated errors) - inefficiency.\n\n\n\nSimulating OLS assumptions"
  },
  {
    "objectID": "bivariate24s.html#regression-1",
    "href": "bivariate24s.html#regression-1",
    "title": "The Bivariate Model",
    "section": "Regression",
    "text": "Regression\nLet \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "bivariate24s.html#linear-predictions",
    "href": "bivariate24s.html#linear-predictions",
    "title": "The Bivariate Model",
    "section": "Linear predictions",
    "text": "Linear predictions\nThe predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\)."
  },
  {
    "objectID": "bivariate24s.html#residuals",
    "href": "bivariate24s.html#residuals",
    "title": "The Bivariate Model",
    "section": "Residuals",
    "text": "Residuals\nThe differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24s.html#in-matrix-notation",
    "href": "bivariate24s.html#in-matrix-notation",
    "title": "The Bivariate Model",
    "section": "In matrix notation",
    "text": "In matrix notation\nRestating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-i",
    "href": "bivariate24s.html#getting-to-ols-i",
    "title": "The Bivariate Model",
    "section": "Getting to OLS I",
    "text": "Getting to OLS I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-1",
    "href": "bivariate24s.html#ols-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nTo minimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]\n\\(\\ldots\\) illustrating explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-1-1",
    "href": "bivariate24s.html#ols-1-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-ii",
    "href": "bivariate24s.html#getting-to-ols-ii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS II",
    "text": "Getting to OLS II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero.\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling that the mean of a variable\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]\nSo, substituting these equations into\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\\]\nand solving for \\(\\hat{\\beta}_1\\) gives:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-iii",
    "href": "bivariate24s.html#getting-to-ols-iii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS III",
    "text": "Getting to OLS III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24s.html#the-components",
    "href": "bivariate24s.html#the-components",
    "title": "The Bivariate Model",
    "section": "The components",
    "text": "The components\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24s.html#components---covariation-mathbfxx",
    "href": "bivariate24s.html#components---covariation-mathbfxx",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'X}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24s.html#components---covariation-mathbfxy",
    "href": "bivariate24s.html#components---covariation-mathbfxy",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'y}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'y}\\)\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation",
    "href": "bivariate24s.html#ols-matrix-derivation",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation-1",
    "href": "bivariate24s.html#ols-matrix-derivation-1",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\   \n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\n\\[\n\\begin{align}\nE(\\mathbf{\\epsilon\\epsilon'})= E \\left[\n\\begin{array}{c}\n\\epsilon_{1} \\\\\n\\vdots \\\\\n\\epsilon_{n}\\\\\n\\end{array} \\right]\nE \\left[\n\\begin{array}{cccc}\n\\epsilon_{1} & \\cdots &\\epsilon_{n}\\\\\n\\end{array} \\right] \\\\ ~\\\\\n= E\n\\left[\n\\begin{array}{cccc}\n\\epsilon_{1}^{2} & \\epsilon_{1}\\epsilon_{2} &\\cdots &\\epsilon_{1}\\epsilon_{n}\\\\\n\\epsilon_{2}\\epsilon_{1}& \\epsilon_{2}^{2} &\\cdots &\\epsilon_{2}\\epsilon_{n}\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n\\epsilon_{n}\\epsilon_{1} &\\epsilon_{n}\\epsilon_{2} &\\cdots & \\epsilon_{n}^{2}\\\\\n\\end{array} \\right]\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon-1",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[\nE(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0\\\\\n0&\\sigma^{2} &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & \\sigma^{2}\\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0\\\\\n0&1 &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & 1\\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I}\n\\]\nThis is the variance-covariance matrix of the disturbances (var-cov(e)); it is symmetric, the main diagonal containing the variances, the off-diagonal containing the covariances."
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\nThe variance-covariance of the estimate, \\(\\widehat{\\beta}\\) derives as follows:\n\\[\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'y}\\]\nsubstituting \\(\\mathbf{X\\beta}+\\epsilon\\) for \\(\\mathbf{y}\\), we get\n\\[\n\\begin{align}\n\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'(X\\beta + \\epsilon)}  \\nonumber \\\\  \n= \\mathbf{(X'X)^{-1}}\\mathbf{X'X\\beta}+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\  \n=\\beta+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\\n\\widehat{\\mathbf{\\beta}}-\\beta=\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta-1",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta-2",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] = \\] \\(~\\)\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k)\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "bivariate24s.html#standard-errors-of-beta_k",
    "href": "bivariate24s.html#standard-errors-of-beta_k",
    "title": "The Bivariate Model",
    "section": "Standard Errors of \\(\\beta_k\\)",
    "text": "Standard Errors of \\(\\beta_k\\)\n\\[\n\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & \\sqrt{var(\\beta_k)}\\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "bivariate24s.html#property-1",
    "href": "bivariate24s.html#property-1",
    "title": "The Bivariate Model",
    "section": "Property 1",
    "text": "Property 1\nIf \\(X' \\widehat{\\epsilon} = 0\\) holds, then the following properties exist:\n\n\n\nProposition\n\n\nEach \\(x\\) variable (each column vector of \\(\\mathbf{X}\\)) is uncorrelated with \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24s.html#property-2",
    "href": "bivariate24s.html#property-2",
    "title": "The Bivariate Model",
    "section": "Property 2",
    "text": "Property 2\nAssuming a constant in the matrix \\(\\mathbf{X}\\),\n\n\n\nProposition\n\n\n\\(\\sum\\limits_{i-1}^{n} \\epsilon_i = 0\\)\nbecause each element of the matrix \\(X' \\widehat{\\epsilon}\\) would be nonzero due to the constant; only by multiplying by \\(\\epsilon_i=0\\) would each element equal zero, and then the sum must be zero."
  },
  {
    "objectID": "bivariate24s.html#property-3",
    "href": "bivariate24s.html#property-3",
    "title": "The Bivariate Model",
    "section": "Property 3",
    "text": "Property 3\n\n\n\nProposition\n\n\nThe mean of the residuals is zero.\nIf the sum of the residuals is zero, that sum divided by \\(N\\) must also be zero."
  },
  {
    "objectID": "bivariate24s.html#property-4",
    "href": "bivariate24s.html#property-4",
    "title": "The Bivariate Model",
    "section": "Property 4",
    "text": "Property 4\n\n\n\nProposition\n\n\nThe regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, \\(\\mathbf{X}\\) and \\(y\\).\n\\[\n\\begin{align}\n\\epsilon = y - X\\widehat{\\beta} \\nonumber \\\\ \\\\\n\\text{multiplying by} ~  N^{-1} \\nonumber \\\\ \\\\\n\\bar{\\epsilon} = \\bar{y} - \\bar{X} \\widehat{\\beta} = 0  \\nonumber\n\\end{align}\n\\]\n\\(\\bar{y} - \\bar{X} \\widehat{\\beta}=0\\) implies \\(\\bar{y} = \\bar{X} \\widehat{\\beta}\\), and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane \\(\\bar{y} - \\bar{X} \\widehat{\\beta}\\), the mean of the residuals equals zero, implying the regression line or hyperplane passes through it."
  },
  {
    "objectID": "bivariate24.html#derivation-i",
    "href": "bivariate24.html#derivation-i",
    "title": "The Bivariate Model",
    "section": "Derivation I",
    "text": "Derivation I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i",
    "href": "bivariate24.html#ols-i",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nMinimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-1",
    "href": "bivariate24.html#ols-i-1",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-2",
    "href": "bivariate24.html#ols-i-2",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nThis shows explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24.html#derivation-ii",
    "href": "bivariate24.html#derivation-ii",
    "title": "The Bivariate Model",
    "section": "Derivation II",
    "text": "Derivation II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii",
    "href": "bivariate24.html#ols-ii",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-1",
    "href": "bivariate24.html#ols-ii-1",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-2",
    "href": "bivariate24.html#ols-ii-2",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero."
  },
  {
    "objectID": "bivariate24.html#ols-ii-3",
    "href": "bivariate24.html#ols-ii-3",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-4",
    "href": "bivariate24.html#ols-ii-4",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling the mean of a variable is\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-5",
    "href": "bivariate24.html#ols-ii-5",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-6",
    "href": "bivariate24.html#ols-ii-6",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#properties",
    "href": "bivariate24.html#properties",
    "title": "The Bivariate Model",
    "section": "Properties",
    "text": "Properties\nNote that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about \\(\\widehat{\\beta}\\), and inferences about those estimates."
  },
  {
    "objectID": "bivariate24s.html#derivation-i",
    "href": "bivariate24s.html#derivation-i",
    "title": "The Bivariate Model",
    "section": "Derivation I",
    "text": "Derivation I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i",
    "href": "bivariate24s.html#ols-i",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nMinimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-1",
    "href": "bivariate24s.html#ols-i-1",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-2",
    "href": "bivariate24s.html#ols-i-2",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nThis shows explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24s.html#derivation-ii",
    "href": "bivariate24s.html#derivation-ii",
    "title": "The Bivariate Model",
    "section": "Derivation II",
    "text": "Derivation II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii",
    "href": "bivariate24s.html#ols-ii",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-1",
    "href": "bivariate24s.html#ols-ii-1",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-2",
    "href": "bivariate24s.html#ols-ii-2",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero."
  },
  {
    "objectID": "bivariate24s.html#ols-ii-3",
    "href": "bivariate24s.html#ols-ii-3",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-4",
    "href": "bivariate24s.html#ols-ii-4",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling the mean of a variable is\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-5",
    "href": "bivariate24s.html#ols-ii-5",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-6",
    "href": "bivariate24s.html#ols-ii-6",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#properties",
    "href": "bivariate24s.html#properties",
    "title": "The Bivariate Model",
    "section": "Properties",
    "text": "Properties\nNote that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about \\(\\widehat{\\beta}\\), and inferences about those estimates."
  },
  {
    "objectID": "bivariate24.html#ols-i-3",
    "href": "bivariate24.html#ols-i-3",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-3",
    "href": "bivariate24s.html#ols-i-3",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-4",
    "href": "bivariate24.html#ols-i-4",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-4",
    "href": "bivariate24s.html#ols-i-4",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-5",
    "href": "bivariate24.html#ols-i-5",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-5",
    "href": "bivariate24s.html#ols-i-5",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-6",
    "href": "bivariate24.html#ols-i-6",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24.html#ols-ii-7",
    "href": "bivariate24.html#ols-ii-7",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-8",
    "href": "bivariate24.html#ols-ii-8",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-9",
    "href": "bivariate24.html#ols-ii-9",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSo, substituting and solving for \\(\\hat{\\beta}_1\\) gives:\n\n\n\n\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-10",
    "href": "bivariate24.html#ols-ii-10",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24.html#derivation-iii",
    "href": "bivariate24.html#derivation-iii",
    "title": "The Bivariate Model",
    "section": "Derivation III",
    "text": "Derivation III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation-2",
    "href": "bivariate24.html#ols-matrix-derivation-2",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon-2",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nThis matrix is important because it gives us our estimate of \\(\\sigma^2\\). This is important enough to merit its own slide."
  },
  {
    "objectID": "bivariate24s.html#ols-i-6",
    "href": "bivariate24s.html#ols-i-6",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24s.html#ols-ii-7",
    "href": "bivariate24s.html#ols-ii-7",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-8",
    "href": "bivariate24s.html#ols-ii-8",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-9",
    "href": "bivariate24s.html#ols-ii-9",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSo, substituting and solving for \\(\\hat{\\beta}_1\\) gives:\n\n\n\n\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-10",
    "href": "bivariate24s.html#ols-ii-10",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24s.html#derivation-iii",
    "href": "bivariate24s.html#derivation-iii",
    "title": "The Bivariate Model",
    "section": "Derivation III",
    "text": "Derivation III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation-2",
    "href": "bivariate24s.html#ols-matrix-derivation-2",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon-2",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nThis matrix is important because it gives us our estimate of \\(\\sigma^2\\). This is important enough to merit its own slide."
  },
  {
    "objectID": "bivariate24.html#aside-on-sigma2",
    "href": "bivariate24.html#aside-on-sigma2",
    "title": "The Bivariate Model",
    "section": "Aside on \\(\\sigma^2\\)",
    "text": "Aside on \\(\\sigma^2\\)\n\\(\\widehat{\\sigma^2}\\) is our estimate of the error variance.\n\\[\\widehat{\\sigma^2} = (u'u) \\frac{1}{n-k-1}\\] This is the sum squared error (inner product of the residuals) distributed across our degrees of freedom. You should see that small samples will have smaller denominators, so potentially larger error variance."
  },
  {
    "objectID": "bivariate24.html#average-error",
    "href": "bivariate24.html#average-error",
    "title": "The Bivariate Model",
    "section": "Average error",
    "text": "Average error\n\\[\\widehat{\\sigma} = \\sqrt{\\widehat{\\sigma^2}}\\] The square root of \\(\\widehat{\\sigma^2}\\) gives us the the residual standard error also called the root mean squared error (RMSE) - it is in the same metric as \\(y\\), so is useful as an indicator of the average error in the regression."
  },
  {
    "objectID": "bivariate24s.html#aside-on-sigma2",
    "href": "bivariate24s.html#aside-on-sigma2",
    "title": "The Bivariate Model",
    "section": "Aside on \\(\\sigma^2\\)",
    "text": "Aside on \\(\\sigma^2\\)\n\\(\\widehat{\\sigma^2}\\) is our estimate of the error variance.\n\\[\\widehat{\\sigma^2} = (u'u) \\frac{1}{n-k-1}\\] This is the sum squared error (inner product of the residuals) distributed across our degrees of freedom. You should see that small samples will have smaller denominators, so potentially larger error variance."
  },
  {
    "objectID": "bivariate24s.html#average-error",
    "href": "bivariate24s.html#average-error",
    "title": "The Bivariate Model",
    "section": "Average error",
    "text": "Average error\n\\[\\widehat{\\sigma} = \\sqrt{\\widehat{\\sigma^2}}\\] The square root of \\(\\widehat{\\sigma^2}\\) gives us the the residual standard error also called the root mean squared error (RMSE) - it is in the same metric as \\(y\\), so is useful as an indicator of the average error in the regression."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "exercise #1 answers\nexercise #2 answers\nexercise #3 answers\nexercise #4 answers\nsimulation shiny app\nsimulation code\ncentral limit theorem shiny app"
  },
  {
    "objectID": "ex1answers2024.html",
    "href": "ex1answers2024.html",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\npdf-cdf"
  },
  {
    "objectID": "ex1answers2024.html#q1",
    "href": "ex1answers2024.html#q1",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\npdf-cdf"
  },
  {
    "objectID": "ex1answers2024.html#q2",
    "href": "ex1answers2024.html#q2",
    "title": "exercise #1 answers",
    "section": "Q2",
    "text": "Q2\n\n\ncode\nxb &lt;- runif(1000, min=-4, max=4)\npdf1 &lt;- dnorm(xb, mean=0, sd=1)\npdf2 &lt;- dnorm(xb, mean=0, sd=sqrt(.5))\npdf3 &lt;- dnorm(xb, mean=-1, sd=sqrt(1.5))\npdf4 &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, pdf1, pdf2, pdf3, pdf4)\n\n\n\nggplot(data=df, aes(x=xb, y=pdf1)) +\n  geom_line() +\n  geom_line(aes(y=pdf2), linetype=\"dotted\") +\n  geom_line(aes(y=pdf3), linetype=\"longdash\" ) +\n  annotate(\"text\", x = 2.5, y = .1, label = \"Normal (0,1)\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"Normal (0, .5)\") +\n  annotate(\"text\", x = -3.3, y = .2, label = \"Normal (-1, 1.5)\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal PDFs\")"
  },
  {
    "objectID": "ex1answers2024.html#dance-model",
    "href": "ex1answers2024.html#dance-model",
    "title": "exercise #1 answers",
    "section": "Dance model",
    "text": "Dance model\nSimple regression - is one artist’s music more danceable than the other - dummy variable for the Rolling Stones.\n\n\ncode\nd &lt;- summary(lm(data=bsum, danceability ~ as.factor(artist_name)))\nmodelsummary(d, stars=TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.593***\n\n\n\n(0.004)\n\n\nas.factor(artist_name)The Rolling Stones\n−0.124***\n\n\n\n(0.005)\n\n\nNum.Obs.\n2875\n\n\nR2\n0.186\n\n\nR2 Adj.\n0.185\n\n\nRMSE\n0.13\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nThe Stones are statistically less danceable than Taylor Swift is.\n\nDanceability over time\nDoes danceability change over time? Let’s use dummmies for years to relax linearity.\n\n\ncode\n# modeling danceability over time for each artist\n\n#taylor swift\ntsreg &lt;- summary(lm(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), danceability ~ as.factor(album_release_year)))\nmodelsummary(tsreg, stars=TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.571***\n\n\n\n(0.011)\n\n\nas.factor(album_release_year)2008\n0.016\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2010\n−0.015\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2012\n0.062***\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2014\n0.066***\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2015\n0.044\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2017\n0.064***\n\n\n\n(0.016)\n\n\nas.factor(album_release_year)2018\n0.031\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2019\n0.087**\n\n\n\n(0.027)\n\n\nas.factor(album_release_year)2020\n−0.026+\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2021\n−0.015\n\n\n\n(0.015)\n\n\nas.factor(album_release_year)2022\n0.062***\n\n\n\n(0.017)\n\n\nNum.Obs.\n1265\n\n\nR2\n0.115\n\n\nR2 Adj.\n0.108\n\n\nRMSE\n0.10\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nCompared to her first release, it appears TS has gotten more danceable since 2012, though her music varies considerably.\n\n\ncode\n#stones\nrsreg &lt;- summary(lm(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), danceability ~ as.factor(album_release_year)))\nmodelsummary(rsreg, stars=TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.599***\n\n\n\n(0.016)\n\n\nas.factor(album_release_year)1965\n−0.033+\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)1966\n−0.105***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)1967\n−0.085***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)1968\n−0.101**\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1969\n−0.105***\n\n\n\n(0.028)\n\n\nas.factor(album_release_year)1970\n−0.135***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)1971\n−0.150***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)1972\n−0.139***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)1973\n−0.148***\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1974\n−0.067*\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1976\n0.006\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)1977\n−0.168***\n\n\n\n(0.026)\n\n\nas.factor(album_release_year)1978\n−0.026\n\n\n\n(0.023)\n\n\nas.factor(album_release_year)1980\n0.006\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1981\n−0.050\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1982\n−0.240***\n\n\n\n(0.039)\n\n\nas.factor(album_release_year)1983\n−0.017\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1986\n−0.052+\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1989\n−0.069*\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1991\n−0.176***\n\n\n\n(0.026)\n\n\nas.factor(album_release_year)1994\n−0.113**\n\n\n\n(0.035)\n\n\nas.factor(album_release_year)1995\n−0.069*\n\n\n\n(0.028)\n\n\nas.factor(album_release_year)1997\n−0.080**\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2004\n−0.250***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)2005\n−0.044\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)2011\n−0.308***\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)2012\n−0.210***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)2016\n−0.224***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)2017\n−0.137***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2018\n−0.197***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2019\n−0.140***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)2020\n−0.213***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)2021\n−0.179***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2022\n−0.233***\n\n\n\n(0.024)\n\n\nNum.Obs.\n1610\n\n\nR2\n0.272\n\n\nR2 Adj.\n0.256\n\n\nRMSE\n0.12\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nThe Stones have generally been as danceable or less so since their first album.\n\n\ncode\n#plot avg dancability \n\nggplot() +\n  geom_density(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), aes(x=danceability))+\n  geom_density(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), aes(x=danceability), linetype=\"dotted\") +\n  annotate(\"text\", x=.18, y=2, label=\"Stones\")+\n  annotate(\"text\", x=.45, y=3.5, label=\"Taylor\") +\n  labs(x=\"Danceability\" , y=\"Density\")\n\n\n\n\n\n\n\n\n\n\n\nArtists over time\nBorrowing Mary L’s idea:\n\n\ncode\n#plot  dancability over time\n\nggplot() +\n  geom_smooth(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), aes(y=danceability, x=album_release_year)) +\n  geom_smooth(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), aes(y=danceability, x=album_release_year)) +\n  annotate(\"text\", x=1980, y=.6, label=\"Stones\")+\n  annotate(\"text\", x=2017, y=.55, label=\"Taylor\") +\n  labs(x=\"Years\" , y=\"Danceability\")"
  },
  {
    "objectID": "ex2answers2024.html",
    "href": "ex2answers2024.html",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\ndata &lt;- c(-1.21,    0,  -.31,\n-.216,  0   ,.92,\n-2.74,  1   ,-1.19, \n.429,   1   ,2.40 ,\n-.40,   1,  .62 ,\n-1.42,  0   ,-.54,\n-1.14,  0   ,-2.35,\n-.74,   0   ,-.137)\n\ndf &lt;- matrix(data, nrow=8, ncol=3, byrow=TRUE)\ncolnames(df) &lt;- c('x2','x1','y')\ndf &lt;- as.data.frame(df)\n\ny &lt;- df$y \ncons &lt;- rep(1, length(y)) # add a constant to the X matrix\nX &lt;- cbind(cons, df$x1, df$x2)\ncolnames(X) &lt;- c('cons','x1', 'x2') \n\n\n\n\n\n\ncode\nb &lt;- solve(t(X)%*%X) %*% t(X)%*%y\nbmat &lt;- as.data.frame(t(b))\nm1 &lt;- lm(data=df, y ~ x1 + x2) \nblm &lt;- as.data.frame(t(coef(m1)))\nblm &lt;- blm %&gt;% mutate(cons=`(Intercept)`) %&gt;% subset(select=c(cons, x1, x2))\nB &lt;- rbind(blm, bmat)\nB &lt;- as.data.frame(t(B))\nB &lt;- rename(B, c(`LM ests` = V1, `Matrix ests` = V2))\n\ntt(B,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_c0qe5qxrp160y3pcxvvc\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                LM ests\n                Matrix ests\n              \n        \n        \n        \n                \n                  0.585513723356887\n                  0.585513723356887\n                \n                \n                  1.0464305226653\n                  1.0464305226653\n                \n                \n                  1.13088629216768\n                  1.13088629216768\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\ncode\n#residuals\ne &lt;- y-X%*%b\ne\n\n\n           [,1]\n[1,]  0.4728587\n[2,]  0.5787577\n[3,]  0.2766842\n[4,]  0.2829055\n[5,] -0.5595897\n[6,]  0.4803448\n[7,] -1.6463034\n[8,]  0.1143421\n\n\ncode\n#estimate of sigma squared \nsse&lt;-(t(e)%*%e)\nsigma2 &lt;- (t(e)%*%e) * 1/(8-3)  #dividing by N-k\nsigma2 \n\n\n          [,1]\n[1,] 0.7964812\n\n\ncode\n#variance-covariance matrix of B\nvcb &lt;- drop(sigma2) * solve(t(X)%*%X)  \nvcb\n\n\n           cons           x1           x2\ncons  0.2720816 -0.164252184  0.119324327\nx1   -0.1642522  0.425007762 -0.005243268\nx2    0.1193243 -0.005243268  0.126242412\n\n\ncode\n#compare to lm var-cov matrix\n\nlmvcov &lt;- as.data.frame(vcov(m1))\nmvcov &lt;- as.data.frame(vcb)\n\ntt(lmvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_6halg9n3qm45vugrs1eu\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                (Intercept)\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368847\n                \n                \n                  -0.164252183738144\n                  0.425007761536868\n                  -0.00524326815847027\n                \n                \n                  0.119324327368847\n                  -0.00524326815847027\n                  0.126242411520152\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\ncode\ntt(mvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_yf6xrlt5wt9cra19wyir\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                cons\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368848\n                \n                \n                  -0.164252183738144\n                  0.425007761536869\n                  -0.0052432681584703\n                \n                \n                  0.119324327368848\n                  -0.00524326815847029\n                  0.126242411520152\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\ncode\n# now, get the vc of b : var(p) = XVX'\n# X nxk; V kxk; X' kxn\n\nV &lt;- vcb \n\nvcp = X%*%V%*%t(X) #vcov of xb (yhats)\n\nvarp&lt;- diag(vcp, names = TRUE) #main diagonal is variances of xbs \n\n# compute predictions for x1=0, x1=1\nXp &lt;- as.data.frame(X) #data frame to change values of x1\nXp$x1 = 0\n  xb0 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\nXp$x1 = 1\n  xb1 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\npred &lt;- data.frame(cbind(X, xb0, xb1)) \n\nggplot() +\n  geom_line(data=pred, aes(x = x2, y = xb0)) +\n  geom_line(data=pred, aes(x=x2, y=xb1))+\n  annotate(\"text\", x = -.5, y = 1.5, label = \"x1=1\") +\n  annotate(\"text\", x = -1, y = -1, label = \"x1=0\") +\n  labs(x=\"x2\", y=\"predicted y\")"
  },
  {
    "objectID": "ex2answers2024.html#q2",
    "href": "ex2answers2024.html#q2",
    "title": "exercise #2 answers",
    "section": "Q2",
    "text": "Q2\n\n\ncode\ncell &lt;- read.csv(\"/users/dave/documents/teaching/501/2024/exercises/ex2/cellphones.csv\")\n\ncell$population&lt;-as.numeric(cell$population)\n\nsummary(cell)\n\n\n      year         state           state_numeric     population      \n Min.   :2012   Length:50          Min.   : 1.00   Min.   :  576412  \n 1st Qu.:2012   Class :character   1st Qu.:14.25   1st Qu.: 1855441  \n Median :2012   Mode  :character   Median :26.50   Median : 4491154  \n Mean   :2012                      Mean   :26.34   Mean   : 6265634  \n 3rd Qu.:2012                      3rd Qu.:38.75   3rd Qu.: 6834295  \n Max.   :2012                      Max.   :51.00   Max.   :38041430  \n numberofdeaths   urban_percent   cell_subscription    cell_ban  \n Min.   :  59.0   Min.   : 0.00   Min.   :  518     Min.   :0.0  \n 1st Qu.: 213.2   1st Qu.:20.25   1st Qu.: 1617     1st Qu.:0.0  \n Median : 488.5   Median :34.50   Median : 4150     Median :0.0  \n Mean   : 670.9   Mean   :34.82   Mean   : 6001     Mean   :0.2  \n 3rd Qu.: 853.8   3rd Qu.:45.75   3rd Qu.: 6648     3rd Qu.:0.0  \n Max.   :3398.0   Max.   :84.00   Max.   :35616     Max.   :1.0  \n    text_ban    total_miles_driven\n Min.   :0.00   Min.   :  4792    \n 1st Qu.:0.00   1st Qu.: 19238    \n Median :1.00   Median : 47116    \n Mean   :0.68   Mean   : 59305    \n 3rd Qu.:1.00   3rd Qu.: 73461    \n Max.   :1.00   Max.   :326272    \n\n\ncode\ndatasummary(population + numberofdeaths + urban_percent ~ mean + median + min\n            + max + var, data=cell)\n\n\n\n\n\n\nmean\nmedian\nmin\nmax\nvar\n\n\n\n\npopulation\n6265634.34\n4491154.00\n576412.00\n38041430.00\n4.900426e+13\n\n\nnumberofdeaths\n670.92\n488.50\n59.00\n3398.00\n456928.56\n\n\nurban_percent\n34.82\n34.50\n0.00\n84.00\n388.23"
  },
  {
    "objectID": "ex2answers2024.html#q1a",
    "href": "ex2answers2024.html#q1a",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\nb &lt;- solve(t(X)%*%X) %*% t(X)%*%y\nbmat &lt;- as.data.frame(t(b))\nm1 &lt;- lm(data=df, y ~ x1 + x2) \nblm &lt;- as.data.frame(t(coef(m1)))\nblm &lt;- blm %&gt;% mutate(cons=`(Intercept)`) %&gt;% subset(select=c(cons, x1, x2))\nB &lt;- rbind(blm, bmat)\nB &lt;- as.data.frame(t(B))\nB &lt;- rename(B, c(`LM ests` = V1, `Matrix ests` = V2))\n\ntt(B,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_c0qe5qxrp160y3pcxvvc\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                LM ests\n                Matrix ests\n              \n        \n        \n        \n                \n                  0.585513723356887\n                  0.585513723356887\n                \n                \n                  1.0464305226653\n                  1.0464305226653\n                \n                \n                  1.13088629216768\n                  1.13088629216768"
  },
  {
    "objectID": "ex2answers2024.html#q1b",
    "href": "ex2answers2024.html#q1b",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\n#residuals\ne &lt;- y-X%*%b\ne\n\n\n           [,1]\n[1,]  0.4728587\n[2,]  0.5787577\n[3,]  0.2766842\n[4,]  0.2829055\n[5,] -0.5595897\n[6,]  0.4803448\n[7,] -1.6463034\n[8,]  0.1143421\n\n\ncode\n#estimate of sigma squared \nsse&lt;-(t(e)%*%e)\nsigma2 &lt;- (t(e)%*%e) * 1/(8-3)  #dividing by N-k\nsigma2 \n\n\n          [,1]\n[1,] 0.7964812\n\n\ncode\n#variance-covariance matrix of B\nvcb &lt;- drop(sigma2) * solve(t(X)%*%X)  \nvcb\n\n\n           cons           x1           x2\ncons  0.2720816 -0.164252184  0.119324327\nx1   -0.1642522  0.425007762 -0.005243268\nx2    0.1193243 -0.005243268  0.126242412\n\n\ncode\n#compare to lm var-cov matrix\n\nlmvcov &lt;- as.data.frame(vcov(m1))\nmvcov &lt;- as.data.frame(vcb)\n\ntt(lmvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_6halg9n3qm45vugrs1eu\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                (Intercept)\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368847\n                \n                \n                  -0.164252183738144\n                  0.425007761536868\n                  -0.00524326815847027\n                \n                \n                  0.119324327368847\n                  -0.00524326815847027\n                  0.126242411520152\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\ncode\ntt(mvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_yf6xrlt5wt9cra19wyir\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                cons\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368848\n                \n                \n                  -0.164252183738144\n                  0.425007761536869\n                  -0.0052432681584703\n                \n                \n                  0.119324327368848\n                  -0.00524326815847029\n                  0.126242411520152"
  },
  {
    "objectID": "ex2answers2024.html#q1c",
    "href": "ex2answers2024.html#q1c",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\n# now, get the vc of b : var(p) = XVX'\n# X nxk; V kxk; X' kxn\n\nV &lt;- vcb \n\nvcp = X%*%V%*%t(X) #vcov of xb (yhats)\n\nvarp&lt;- diag(vcp, names = TRUE) #main diagonal is variances of xbs \n\n# compute predictions for x1=0, x1=1\nXp &lt;- as.data.frame(X) #data frame to change values of x1\nXp$x1 = 0\n  xb0 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\nXp$x1 = 1\n  xb1 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\npred &lt;- data.frame(cbind(X, xb0, xb1)) \n\nggplot() +\n  geom_line(data=pred, aes(x = x2, y = xb0)) +\n  geom_line(data=pred, aes(x=x2, y=xb1))+\n  annotate(\"text\", x = -.5, y = 1.5, label = \"x1=1\") +\n  annotate(\"text\", x = -1, y = -1, label = \"x1=0\") +\n  labs(x=\"x2\", y=\"predicted y\")"
  },
  {
    "objectID": "ex2answers2024.html#model",
    "href": "ex2answers2024.html#model",
    "title": "exercise #2 answers",
    "section": "model",
    "text": "model\n\n\ncode\nm2 &lt;- lm(data=cell, numberofdeaths ~ text_ban+ miles + pop)\nmodelsummary(m2, stars=TRUE, coef_rename = TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n145.859*\n\n\n\n(54.596)\n\n\nTexting Ban\n−186.190**\n\n\n\n(54.018)\n\n\nTotal Miles Driven in State (billions)\n16.903***\n\n\n\n(2.470)\n\n\nState population (millions)\n−55.986*\n\n\n\n(21.295)\n\n\nNum.Obs.\n50\n\n\nR2\n0.938\n\n\nR2 Adj.\n0.934\n\n\nAIC\n663.4\n\n\nBIC\n673.0\n\n\nLog.Lik.\n−326.695\n\n\nF\n232.354\n\n\nRMSE\n166.50\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\ncode\ndfavg &lt;- cell \n\npredictions&lt;- data.frame(upper=numeric(0),lower=numeric(0),xb=numeric(0), miles=numeric(0), ban=numeric(0))\nc&lt;-1\nfor (i in seq(0,1,1)) { \n  for (j in seq(1,320,1)) {\n  dfavg$text_ban = i\n  dfavg$miles = j+5\n  all &lt;- data.frame(predict(m2, se.fit=TRUE, newdata=dfavg))\n  upper = median(all$fit, na.rm=TRUE)+1.96*(median(all$se.fit, na.rm=TRUE))\n  lower = median(all$fit, na.rm=TRUE)-1.96*(median(all$se.fit, na.rm=TRUE))\n  xb = median(all$fit, na.rm=TRUE)\n  predictions[c,] &lt;-data.frame(upper,lower,xb,j, i)\n  c=c+1\n  }\n}\n  \n  ggplot()+\n  geom_line(data= predictions%&gt;%filter(ban==0), aes(x=miles, y=xb))+\n  geom_line(data= predictions%&gt;%filter(ban==1), aes(x=miles, y=xb)) +\n  labs ( colour = NULL, x = \"Miles Driven (Billions)\", y =  \"Expected Traffic Fatalities\" ) +\n  annotate(\"text\", x = 200, y = 2500, label = \"Text Ban\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 80, y = 2000, label = \"No Text Ban\", size=3.5, colour=\"gray30\")+\n    labs(x=\"Miles Driven (Billions)\", y=\"Expected Traffic Fatalities\")"
  },
  {
    "objectID": "ex2answers2024.html#model-south",
    "href": "ex2answers2024.html#model-south",
    "title": "exercise #2 answers",
    "section": "model (south)",
    "text": "model (south)\n\n\ncode\nm3 &lt;- lm(data=cell, numberofdeaths ~ text_ban+ miles + pop + south)\nmodelsummary(m3, stars=TRUE, coef_rename = TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n138.381**\n\n\n\n(47.751)\n\n\nTexting Ban\n−180.066***\n\n\n\n(47.233)\n\n\nTotal Miles Driven in State (billions)\n11.662***\n\n\n\n(2.542)\n\n\nState population (millions)\n−14.502\n\n\n\n(21.432)\n\n\nsouth\n246.421***\n\n\n\n(63.144)\n\n\nNum.Obs.\n50\n\n\nR2\n0.954\n\n\nR2 Adj.\n0.950\n\n\nAIC\n650.8\n\n\nBIC\n662.3\n\n\nLog.Lik.\n−319.408\n\n\nF\n231.981\n\n\nRMSE\n143.91\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\ncode\ndfavg &lt;- cell \n\npredictions&lt;- data.frame(upper=numeric(0),lower=numeric(0),xb=numeric(0), miles=numeric(0), south=numeric(0))\nc&lt;-1\nfor (i in seq(0,1,1)) { \n  for (j in seq(1,320,1)) {\n  dfavg$south = i\n  dfavg$miles = j+5\n  all &lt;- data.frame(predict(m3, se.fit=TRUE, newdata=dfavg))\n  upper = median(all$fit, na.rm=TRUE)+1.96*(median(all$se.fit, na.rm=TRUE))\n  lower = median(all$fit, na.rm=TRUE)-1.96*(median(all$se.fit, na.rm=TRUE))\n  xb = median(all$fit, na.rm=TRUE)\n  predictions[c,] &lt;-data.frame(upper,lower,xb,j, i)\n  c=c+1\n  }\n}\n  \n  ggplot()+\n  geom_line(data= predictions%&gt;%filter(south==0), aes(x=miles, y=xb))+\n  geom_line(data= predictions%&gt;%filter(south==1), aes(x=miles, y=xb)) +\n  labs ( colour = NULL, x = \"Miles Driven (Billions)\", y =  \"Expected Traffic Fatalities\" ) +\n  annotate(\"text\", x = 250, y = 2500, label = \"Non-south\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 80, y = 2000, label = \"South\", size=3.5, colour=\"gray30\")+\n    labs(x=\"Miles Driven (Billions)\", y=\"Expected Traffic Fatalities\")"
  },
  {
    "objectID": "ex3answers2024.html",
    "href": "ex3answers2024.html",
    "title": "Exercise #3",
    "section": "",
    "text": "code\nresults &lt;- data.frame ( var = character (0), coef = numeric(0), se = numeric (0), n = numeric (0))\n\nfor(i in seq(10,1000,1)) { \n  set.seed(12345)\n  data &lt;- tibble(\n    X &lt;- rnorm_multi(i, 3, \n                     mu=c(0, 0, 0), \n                     sd=1,\n                     r = c(0.0, 0.0, 0.0),\n                     varnames=c(\"x1\", \"x2\", \"e\"))\n  ) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e)\n  \n  mod &lt;- (lm(y ~ x1 + x2, data=data))\n  \n  results[((i-9)*3-2):((i-9)*3),1:4] &lt;- data.frame ( var = c(\"x0\",\"x1\",\"x2\"), coef(summary(mod))[,1:2], i)\n  \n}\n\nresults &lt;-data.frame(results, t=results$coef/results$se)\n\n\np1 &lt;- ggplot(results, aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs ( colour = NULL, x = \"Sample Size\", y =  \"Estimated Coefficient\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\np1\n\n\n\n\n\ncode\np2 &lt;- ggplot(results, aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs ( colour = NULL, x = \"Sample Size\", y =  \"Estimated Standard Error\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\np2"
  },
  {
    "objectID": "ex3answers2024.html#rhoxe--",
    "href": "ex3answers2024.html#rhoxe--",
    "title": "Exercise #3",
    "section": "rho(x,e) —-",
    "text": "rho(x,e) —-\n\n\ncode\nresults &lt;- data.frame()\ni=1\nfor(r in seq(0, .95, .01)) {\n  set.seed(8675309)\n  X &lt;- rnorm_multi(1000, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0, r, 0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) \n  y = .5 + X$x1 + 2*X$x2 + X$e\n  data &lt;- data.frame(X, y)\n  m &lt;- lm(y ~ x1 + x2, data=data)\n  \n  b0&lt;-coef(summary(m))[1,1]\n  b1&lt;-coef(summary(m))[2,1]\n  b2&lt;-coef(summary(m))[3,1]\n  se0&lt;-coef(summary(m))[1,2]\n  se1&lt;-coef(summary(m))[2,2]\n  se2&lt;-coef(summary(m))[3,2]\n  results[i,1:7] &lt;- data.frame(rho = r, b0, b1, b2, se0, se1, se2)\n  \n  i = i+1\n}\n\nb &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=b0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=b1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=b2), color=\"green\")+\n  labs ( colour = NULL, x = \"Correlation (x1, e)\", y =  \"Estimated Coefficients\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nb\n\n\n\n\n\ncode\nse &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=se0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=se1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=se2), color=\"green\") +\n  labs ( colour = NULL, x = \"Correlation (x1, e)\", y =  \"Estimated St. Errs\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nse"
  },
  {
    "objectID": "ex3answers2024.html#rhox1x2--",
    "href": "ex3answers2024.html#rhox1x2--",
    "title": "Exercise #3",
    "section": "rho(x1,x2) —-",
    "text": "rho(x1,x2) —-\n\n\ncode\nresults &lt;- data.frame()\ni=1\nfor(r in seq(0, .95, .01)) {\n  set.seed(8675309)\n  X &lt;- rnorm_multi(1000, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(r, 0, 0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) \n  y = .5 + X$x1 + 2*X$x2 + X$e\n  data &lt;- data.frame(X, y)\n  m &lt;- lm(y ~ x1 + x2, data=data)\n  i = i+1\n  b0&lt;-coef(summary(m))[1,1]\n  b1&lt;-coef(summary(m))[2,1]\n  b2&lt;-coef(summary(m))[3,1]\n  se0&lt;-coef(summary(m))[1,2]\n  se1&lt;-coef(summary(m))[2,2]\n  se2&lt;-coef(summary(m))[3,2]\n  results[i,1:7] &lt;- data.frame(rho = r, b0, b1, b2, se0, se1, se2)\n}\n\nb &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=b0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=b1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=b2), color=\"green\")+\n  labs ( colour = NULL, x = \"Correlation (x1, x2)\", y =  \"Estimated Coefficients\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nb\n\n\n\n\n\ncode\nse &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=se0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=se1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=se2), color=\"green\") +\n  labs ( colour = NULL, x = \"Correlation (x1, x2)\", y =  \"Estimated St. Errs\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nse"
  },
  {
    "objectID": "normality24.html",
    "href": "normality24.html",
    "title": "Normality",
    "section": "",
    "text": "Let’s put the pieces of the OLS model together.\n\nthe data matrix consists of a vector, \\(y\\), and a matrix \\(\\mathbf{X}\\) including a constant.\n\\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) :: the covariance of \\(\\mathbf{X}\\) and \\(y\\) weighted by the covariance of \\(\\mathbf{X}\\).\n\\(\\widehat{\\sigma^2} = (\\epsilon' \\epsilon) * 1/(N - k - 1)\\)\nthe variance-covariance matrix of \\(\\widehat{\\beta}\\) is \\(\\widehat{\\sigma^2} \\mathbf{(X'X)^{-1}}\\)\nthe square root of the main diagonal gives the standard errors of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "normality24.html#implementing-the-model",
    "href": "normality24.html#implementing-the-model",
    "title": "Normality",
    "section": "",
    "text": "Let’s put the pieces of the OLS model together.\n\nthe data matrix consists of a vector, \\(y\\), and a matrix \\(\\mathbf{X}\\) including a constant.\n\\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) :: the covariance of \\(\\mathbf{X}\\) and \\(y\\) weighted by the covariance of \\(\\mathbf{X}\\).\n\\(\\widehat{\\sigma^2} = (\\epsilon' \\epsilon) * 1/(N - k - 1)\\)\nthe variance-covariance matrix of \\(\\widehat{\\beta}\\) is \\(\\widehat{\\sigma^2} \\mathbf{(X'X)^{-1}}\\)\nthe square root of the main diagonal gives the standard errors of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "normality24.html#implementing-the-model-1",
    "href": "normality24.html#implementing-the-model-1",
    "title": "Normality",
    "section": "Implementing the model",
    "text": "Implementing the model\nIn a little more detail, note we’re making use of every part of the regression here:\n\nEstimate \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\nCompute \\(u = y - \\mathbf{X \\widehat{\\beta}}\\)\nCompute the sum of squared residuals, \\((u' u)\\).\nUse that to compute the error variance, \\(\\widehat{\\sigma^2} = (u'u) * 1/(N - k - 1)\\)\nTo measure uncertainty around \\(\\mathbf{\\widehat{\\beta}}\\), divide the error variance by the covariance of \\(\\mathbf{X}\\).\nThat’s the variance-covariance matrix of \\(\\widehat{\\beta} = \\widehat{\\sigma^2} \\mathbf{(X'X)^{-1}}\\) the square root of the main diagonal gives the standard errors of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "normality24.html#quantities",
    "href": "normality24.html#quantities",
    "title": "Normality",
    "section": "Quantities",
    "text": "Quantities\nWe end up with a number of interesting and useful quantities:\n\n\\(\\widehat{y} = \\mathbf{X} \\widehat{\\beta}\\) :: linear predictions\n\\(u = y- \\widehat{y}\\) :: these are the residuals.\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\).\n\\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\)\n\\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\). $ TSS = SSE + MSS$"
  },
  {
    "objectID": "normality24.html#uncertainty-about-mathbfwidehatbeta",
    "href": "normality24.html#uncertainty-about-mathbfwidehatbeta",
    "title": "Normality",
    "section": "Uncertainty about \\(\\mathbf{\\widehat{\\beta}}\\)",
    "text": "Uncertainty about \\(\\mathbf{\\widehat{\\beta}}\\)\nWhy are we uncertain about the estimates?\n\n\\(\\widehat{\\beta}\\) represents our sample-based estimates of the population parameters, assuming the sample is random, representative, etc.\nIf we imagine our one sample to be one of many possible samples, then \\(\\widehat{\\beta}\\) is one estimate of many possible estimates from those hypothetical samples.\nIf we have many samples, each with a mean, we know those means have a normal distribution. So \\(\\widehat{\\beta}\\) is drawn from a normal distribution."
  },
  {
    "objectID": "normality24.html#distribution-of-hatbeta",
    "href": "normality24.html#distribution-of-hatbeta",
    "title": "Normality",
    "section": "Distribution of \\(\\hat{\\beta}\\)",
    "text": "Distribution of \\(\\hat{\\beta}\\)\nThe \\(\\widehat{\\beta}\\)s are normally distributed:\n\\[ \\widehat{\\beta} \\sim \\mathcal{N}(\\beta, var({\\beta})) \\]\nThe estimates are normally distributed, and the \\(\\widehat{\\beta}\\)s we estimate are drawn from that distribution, conditional on the sample of data, \\(N\\). The sample itself is one of a (theoretically) infinite number of samples.\nThink of it this way - the forces in \\(\\epsilon\\) shaped which of these samples of \\(\\widehat{\\beta}\\) we happened to draw. If we’d gotten a different sample, our estimates of \\(\\widehat{\\beta}\\) would have been different. So which distribution we’re drawing from changes as we change model specification even in the same data sample.\nIf we estimated regressions in all those possible samples, the estimates would comprise the entire distribution of \\(\\widehat{\\beta}\\). The mean would be \\(\\beta\\); the variances would be \\(var({\\beta})\\).\nIf the \\(\\widehat{\\beta}\\)s are normally distributed with mean \\(\\beta\\) and we have an estimate of the variance, we can evaluate in probability terms how ``close’’ \\(\\widehat{\\beta}\\) is to \\(\\beta\\); after all, we know the properties of the normal distribution. That’s inference.\nHow do we have such confidence \\(\\widehat{\\beta}\\) is normally distributed?\nWe can mainly rely on the normality of \\(\\widehat{\\beta}\\) because of the central limit theorem.\nIt’s also true that the variance in \\(\\widehat{\\beta}\\) arises from our particular sample (note the variance of the assumed distribution of \\(\\widehat{\\beta}\\) is estimated). So large samples will be associated with smaller variances. Another reason we can rely on normality is that, in large samples, \\(\\mathbf{X' \\widehat{\\beta}}\\) is normal, and in the OLS model, \\(y\\) is normal - therefore \\(\\widehat{y}\\) is normal, and so is \\(y -\\widehat{y} = \\epsilon\\).}"
  },
  {
    "objectID": "normality24.html#central-limit-theorem",
    "href": "normality24.html#central-limit-theorem",
    "title": "Normality",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ \\sum\\limits_{n \\rightarrow \\infty} \\frac{(\\bar{X_i})}{n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "normality24.html#question",
    "href": "normality24.html#question",
    "title": "Normality",
    "section": "Question",
    "text": "Question\nSuppose we estimated a regression, but then wanted to simulate the distribution of \\(\\hat{\\beta}\\)s. How would we do it? \\ ~\\\nOnce we generated that distribution, how would we characterize \\(\\hat{\\beta}, \\hat{\\sigma^2}\\)?"
  },
  {
    "objectID": "normality24.html#simulating-the-distribution-of-widehatbeta",
    "href": "normality24.html#simulating-the-distribution-of-widehatbeta",
    "title": "Normality",
    "section": "Simulating the distribution of \\(\\widehat{\\beta}\\)",
    "text": "Simulating the distribution of \\(\\widehat{\\beta}\\)\n\nsuppose, from our regression, we have one estimate of each \\(\\widehat{\\beta}\\), one estimate of the variance of each.\nwe know these are draws from a normal distribution.\nsuppose we assume they are drawn from a normal distribution with mean \\(\\widehat{\\beta}\\), and variance \\(\\widehat{\\sigma^2}\\).\nwe could make a large number of draws from that distribution, say 10,000. What would this look like? Why?\nif we did this, we’d have the simulated distribution of \\(\\widehat{\\beta}\\). How would we summarize that distribution?"
  },
  {
    "objectID": "normality24.html#measuring-uncertainty",
    "href": "normality24.html#measuring-uncertainty",
    "title": "Normality",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\n-We can think of \\(\\widehat{\\sigma^2}\\) as the average (squared) error per observation.\n\nThe var-cov \\(\\widehat{\\beta}\\) matrix weights the average squared error by the covariance of \\(\\mathbf{X}\\).\nThe main diagonal of the var-cov \\(\\widehat{\\beta}\\) is the variance of \\(\\widehat{\\beta}\\); the square root is the standard deviation of \\(\\widehat{\\beta}\\).\nIf \\(\\widehat{\\beta}\\) is the estimate of the mean of all normally distributed \\(\\beta\\)s, its standard deviation is the average distance of estimates like this one from the population parameter.\nIf it’s large, the average \\(\\widehat{\\beta}\\) is far from the population parameter.\nIf it’s small, the average \\(\\widehat{\\beta}\\) is close to the population parameter."
  },
  {
    "objectID": "normality24.html#simulating-uncertainty",
    "href": "normality24.html#simulating-uncertainty",
    "title": "Normality",
    "section": "Simulating Uncertainty",
    "text": "Simulating Uncertainty\nIf we simulate the distribution of \\(\\widehat{\\beta}\\) based on the estimated coefficients and var-cov matrix, we can rely on the moments of that distribution:\n\nthe median would be our point estimate of \\(\\widehat{\\beta}\\).\nthe percentiles (2.5, 97.5) would be our confidence boundaries.\n\nNotice the generality here - we could simulate \\(\\widehat{\\beta}\\), multiply by interesting values of \\(\\mathbf{X}\\) to generate predictions, and rely on those moments of the simulated distribution."
  },
  {
    "objectID": "matrix24s.html",
    "href": "matrix24s.html",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model.\n\n\n\n\nA single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A.\n\n\n\nThe dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar.\n\n\n\n\nA is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]\n\n\n\n\n\n\nA rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3).\n\n\n\nA vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "predictionmethods24.html",
    "href": "predictionmethods24.html",
    "title": "Prediction Methods",
    "section": "",
    "text": "We’ll use a dataset of Major League Baseball attendance to illustrate different methods of generating predictions from multivariate models.\n\n\ncode\nmlb &lt;- read.csv(\"/users/dave/documents/teaching/501/2023/slides/L3_multivariate/code/mlbattendance/MLBattend.csv\")  \n\n# rescale home attendance\nmlb$Home_attend &lt;- mlb$Home_attend/1000\n\ndatasummary(All(mlb) ~ mean+ median+ sd+min+max,  data=mlb, style=\"grid\", title = \"MLB attendance data\")\n\n\n\nMLB attendance data\n\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\nSeason\n1985.06\n1985.00\n9.27\n1969.00\n2000.00\n\n\nHome_attend\n1777.99\n1681.90\n755.87\n306.76\n4483.35\n\n\nRuns_scored\n694.94\n691.50\n105.17\n329.00\n1009.00\n\n\nRuns_allowed\n694.89\n693.00\n105.52\n331.00\n1103.00\n\n\nWins\n78.85\n79.00\n12.67\n37.00\n114.00\n\n\nLosses\n78.88\n79.00\n12.65\n40.00\n110.00\n\n\nGames_behind\n14.39\n13.00\n11.75\n0.00\n52.00\n\n\n\n\n\n\n\n\n\nHere’s a simple model predicting season attendance at MLB games, regressing home attendance on runs scored, then generating “in-sample” predictions by computing:\n\\[\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\]\nwhere \\(x_i\\) is the number of runs scored for each row/observation in the estimation data.\n\n\ncode\nm1 &lt;- lm(data=mlb, Home_attend ~  Runs_scored)\nmodelsummary(m1)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−691.488\n\n\n\n(151.853)\n\n\nRuns_scored\n3.554\n\n\n\n(0.216)\n\n\nNum.Obs.\n838\n\n\nR2\n0.244\n\n\nR2 Adj.\n0.244\n\n\nAIC\n13256.5\n\n\nBIC\n13270.7\n\n\nLog.Lik.\n−6625.265\n\n\nF\n270.514\n\n\nRMSE\n656.62\n\n\n\n\n\n\n\ncode\np1 &lt;- data.frame(predict(m1), mlb$Runs_scored)\n\nggplot(p1, aes(x=mlb.Runs_scored, y=predict.m1.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\nHere’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nmodelsummary(m3)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−70849.854\n\n\n\n(4473.400)\n\n\nRuns_scored\n3.123\n\n\n\n(0.338)\n\n\nRuns_allowed\n−1.887\n\n\n\n(0.357)\n\n\nSeason\n36.215\n\n\n\n(2.284)\n\n\nGames_behind\n−8.273\n\n\n\n(2.755)\n\n\nNum.Obs.\n838\n\n\nR2\n0.471\n\n\nR2 Adj.\n0.469\n\n\nAIC\n12963.1\n\n\nBIC\n12991.5\n\n\nLog.Lik.\n−6475.561\n\n\nF\n185.758\n\n\nRMSE\n549.20\n\n\n\n\n\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from. What’s missing here?\n\n\n\n\n\n\nControl\n\n\n\nHold all variables constant at means, medians, or modes, except the one of interest, so we can focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "predictionmethods24.html#bivariate-model-predictions",
    "href": "predictionmethods24.html#bivariate-model-predictions",
    "title": "Prediction Methods",
    "section": "",
    "text": "Here’s a simple model predicting season attendance at MLB games, regressing home attendance on runs scored, then generating “in-sample” predictions by computing:\n\\[\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\]\nwhere \\(x_i\\) is the number of runs scored for each row/observation in the estimation data.\n\n\ncode\nm1 &lt;- lm(data=mlb, Home_attend ~  Runs_scored)\nmodelsummary(m1)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−691.488\n\n\n\n(151.853)\n\n\nRuns_scored\n3.554\n\n\n\n(0.216)\n\n\nNum.Obs.\n838\n\n\nR2\n0.244\n\n\nR2 Adj.\n0.244\n\n\nAIC\n13256.5\n\n\nBIC\n13270.7\n\n\nLog.Lik.\n−6625.265\n\n\nF\n270.514\n\n\nRMSE\n656.62\n\n\n\n\n\n\n\ncode\np1 &lt;- data.frame(predict(m1), mlb$Runs_scored)\n\nggplot(p1, aes(x=mlb.Runs_scored, y=predict.m1.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")"
  },
  {
    "objectID": "predictionmethods24.html#mltivariate-model-predictions",
    "href": "predictionmethods24.html#mltivariate-model-predictions",
    "title": "Prediction methods",
    "section": "",
    "text": "Here’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nsummary(m3)\n\n\n\nCall:\nlm(formula = Home_attend ~ Runs_scored + Runs_allowed + Season + \n    Games_behind, data = mlb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1273.3  -402.7   -51.4   342.7  2921.6 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -7.085e+04  4.473e+03 -15.838  &lt; 2e-16 ***\nRuns_scored   3.123e+00  3.383e-01   9.230  &lt; 2e-16 ***\nRuns_allowed -1.887e+00  3.571e-01  -5.286  1.6e-07 ***\nSeason        3.621e+01  2.284e+00  15.858  &lt; 2e-16 ***\nGames_behind -8.273e+00  2.755e+00  -3.003  0.00276 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 550.8 on 833 degrees of freedom\nMultiple R-squared:  0.4715,    Adjusted R-squared:  0.4689 \nF-statistic: 185.8 on 4 and 833 DF,  p-value: &lt; 2.2e-16\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from.\nWhat’s missing here? Ideally, we’d hold all variables, except the one of interest, constant so we can just focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "predictionmethods24.html#at-means-predictions-adjusted-effects",
    "href": "predictionmethods24.html#at-means-predictions-adjusted-effects",
    "title": "Prediction Methods",
    "section": "At-means predictions (adjusted effects)",
    "text": "At-means predictions (adjusted effects)\nAt-means predictions (also called “adjusted effects”) set all variables except the \\(x\\) of interest at a sensible value - usually the mean, median, or mode depending on the level of measurment. Holding those constant, but varying just the \\(x\\) of interest produces predictions of \\(y\\) that are neater and easier to discuss than the in-sample ones above.\n\nSummarize the estimation data\nLet’s find the means etc. of the variables in the model. It’s important only to consider the cases that are in the estimation data, that is, actually used in the model. Write code to identify cases in the model, then use the means, etc. of these for the predictions:\n\n\ncode\nmlb$used &lt;- TRUE\nmlb$used[na.action(m3)] &lt;- FALSE\nestdata &lt;- mlb %&gt;%  filter(used==\"TRUE\")\n\ndatasummary(Home_attend+Runs_scored +Runs_allowed + Season + Games_behind ~ mean+ median+ sd+min+max,  data=mlb, style=\"grid\", title = \"MLB estimation data\")\n\n\n\nMLB estimation data\n\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\nHome_attend\n1777.99\n1681.90\n755.87\n306.76\n4483.35\n\n\nRuns_scored\n694.94\n691.50\n105.17\n329.00\n1009.00\n\n\nRuns_allowed\n694.89\n693.00\n105.52\n331.00\n1103.00\n\n\nSeason\n1985.06\n1985.00\n9.27\n1969.00\n2000.00\n\n\nGames_behind\n14.39\n13.00\n11.75\n0.00\n52.00\n\n\n\n\n\n\n\nIn this case, it turns out we use all the cases in the data - it’s important to check this any time you’re making model predictions.\n\n\nGenerate at-mean predictions\nCreate a new data frame with as many observations as there are interesting values of the \\(x\\) variable of interest. Then, set all variables except the \\(x\\) of interest at their means, medians, or modes. Using the standard errors of the predictions, generate the boundaries of the confidence intervals by end point transformation.\n\\[ \\widehat{y} \\pm 1.96 \\times se(\\widehat{y}) \\]\n\n\ncode\noosdata &lt;- data.frame(Intercept=1, Runs_allowed=694 , Season=1985 , Games_behind=14 , Runs_scored= c(seq(330,1000,10)))\n\nmlb.predict &lt;- data.frame(oosdata, predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=oosdata))\n\n# confidence bounds by end point transformation\nmlb.predict &lt;- mlb.predict %&gt;% mutate(ub=fit.fit+1.96*se.fit) %&gt;% mutate(lb=fit.fit-1.96*se.fit)\n\natmean &lt;- ggplot(mlb.predict, aes(x=Runs_scored, y=fit.fit)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") + \n  theme_minimal() +\n  ggtitle(\"At-mean effects\")\n\natmean\n\n\n\n\n\n\n\nAside on standard errors of \\(\\widehat{y}\\)\nThe standard errors of \\(\\widehat{y}\\) are calculated using the \\(x\\) values, and the variance-covariance matrix of the coefficients. These are usually calculated as follows:\n\\[ X ~VX' \\]\nmultiplying the \\(X\\) matrix (remembering this could be the out of sample \\(X\\) matrix), the variance-covariance matrix of \\(\\widehat{\\beta}\\), and the transpose of the \\(X\\) matrix. The square root of the main diagonal gives the standard errors of \\(\\widehat{y}\\).\n\n\n\n\n\n\nUncertainty about \\(\\widehat{y}\\)\n\n\n\nThe intuition is that we’re using our uncertainty about the coefficients to generate measures of uncertainty about the predictions."
  },
  {
    "objectID": "predictionmethods24.html#average-effects-end-point-boundaries",
    "href": "predictionmethods24.html#average-effects-end-point-boundaries",
    "title": "Prediction Methods",
    "section": "Average effects, end point boundaries",
    "text": "Average effects, end point boundaries\nAverage effects are where we’re computing \\(N\\) predictions for every interesting value of \\(x\\), then taking the average of those predictions for each value of \\(x\\). This is a counterfactual approach - we’re using the actual data, changing only the variable of interest, as if all observations took on the same value of that variable.\nFor instance, in our MLB attendance model, we change the value of “Runs Scored” to the minimum value, 329, for every observation, keeping all the other variables as they are in the real estimation data. Compute the predictions for all the observations at \\(x = 329\\), then take the average of those predictions - now iterate to 330, 331, etc.\n\n\n\n\n\n\nCounterfactuals\n\n\n\nWe are treating the estimation data as if every team scored 329 runs - this is the counterfactual - then computing the average attendance for that counterfactual, and doing this for all counterfactuals (values of \\(x\\)).\n\n\n\n\ncode\n# preserve the original values of Runs Scored\nmlb &lt;- mlb%&gt;%\nmutate(original_runs_scored = Runs_scored)\n\nxb = 0\nse = 0\nrs = 0\n\nfor(i in seq(330,1000,1)){  #iterating by 1 to max number of obs we're taking medians for\n  mlb$Runs_scored &lt;- i\n  mlb.predict &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=mlb))\n  xb[i-329] &lt;- median(mlb.predict$fit.fit) #index using i but minus constant to start at row 1\n  se[i-329] &lt;- median(mlb.predict$se.fit)\n  rs[i-329] &lt;- i\n}\n\navg.pred &lt;- data.frame(xb, se, rs)\n\n# upper and lower bounds by end point transformation\n\navg.pred &lt;- avg.pred %&gt;% mutate(ub=xb+1.96*se) %&gt;% mutate(lb=xb-1.96*se)\n\n# reset the estimation data to the actual values of Runs Scored\nmlb &lt;- mlb%&gt;%\n  mutate(Runs_scored=original_runs_scored )\n\n#plot\naverage &lt;- ggplot(avg.pred, aes(x=rs, y=xb)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Average Effects\") +\n  theme_minimal()\n\naverage\n\n\n\n\n\n\nComparing at-mean and average effects\n\n\ncode\natmean + average"
  },
  {
    "objectID": "predictionmethods24.html#average-effects-percentile-boundaries",
    "href": "predictionmethods24.html#average-effects-percentile-boundaries",
    "title": "Prediction methods",
    "section": "Average effects, percentile boundaries",
    "text": "Average effects, percentile boundaries\nIn the last example, we computed confidence interval boundaries by end point transformation. Here, we’ll compute the same average effects as above, but using percentiles of the distribution of predictions as upper and lower bounds.\n\n\ncode\n# preserve the original values of Runs Scored\nmlb &lt;- mlb%&gt;%\n  mutate(original_runs_scored = Runs_scored)\n\nxb = 0\nrs = 0\nqtilepreds &lt;-  data.frame ( lb0 = numeric(0),med0= numeric(0),\n              ub0= numeric(0), rs = numeric(0))\n\nfor(i in seq(330,1100,1)){  #iterating by 1 to max number of obs we're taking medians for\n  mlb$Runs_scored &lt;- i\n  mlb.predict &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=mlb))\n  xb &lt;-  quantile(mlb.predict$fit.fit, probs=c(.025,.5,.975))\n  XB &lt;- data.frame(t(xb))\n  qtilepreds[(i-329):(i-329),1:4] &lt;- data.frame(XB, i)  \n}\n\n# reset the estimation data to the actual values of Runs Scored\nmlb &lt;- mlb%&gt;%\n  mutate(Runs_scored=original_runs_scored )\n\n#plot\n\navgqtile &lt;- ggplot(qtilepreds, aes(x=rs, y=med0)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb0, ymax=ub0), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Average Effects, Percentile Bounds\")\n\navgqtile\n\n\n\n\n\n\n\n\n\n\nComparing average effects with different CI methods\n\n#compare\navgqtile + average"
  },
  {
    "objectID": "predictionmethods24.html#simulated-effects",
    "href": "predictionmethods24.html#simulated-effects",
    "title": "Prediction Methods",
    "section": "Simulated effects",
    "text": "Simulated effects\nThe last method we’ll consider is simulated effects. This is a way to generate predictions that are based on the distribution of the coefficients. The \\(\\widehat{\\beta}\\)s are estimates of the mean of a normal distribution, and the var-cov matrix of the coefficients is an estimate of the variance of that distribution. We can assume the distribution of \\(\\beta\\) is Normal because of the Central Limit Theorem.\nHere’s the process. Treat our \\(\\widehat{\\beta}\\)s as the mean of a normal distribution, and the var-cov matrix of the coefficients as the variance. Generate a large number of draws from that distribution, say 10,000. This will give us the simulated distribution of \\(\\widehat{\\beta}\\). We’ll have 10,000 estimates of \\(\\widehat{\\beta}\\).\nUsing these, we can now produce 10,000 predictions for each interesting value of \\(x\\). We can then summarize the distribution of those predictions, using the median as our point estimate, and the percentiles (2.5, 97.5) as our confidence boundaries.\n\n\ncode\nB &lt;- data.frame(rmvnorm(n=10000, mean=coef(m3), vcov(m3))) #simulated distribution of the estimates using the var-cov matrix of B as the variance, and the estimates of B as the mean.\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4')\n\nsim.preds &lt;- data.frame ( lb = numeric(0),med= numeric(0),\n                            ub= numeric(0), r = numeric(0))\n\nfor(i in seq(330,1000,1)){\n  xbR  &lt;- quantile(B$b0 + B$b1*i + B$b2*694 + B$b3*1985 +B$b4*14, probs=c(.025,.5,.975))\n  xbR&lt;- data.frame(t(xbR))\n  sim.preds[(i-329):(i-329),1:4] &lt;- data.frame(xbR, i)\n}\n\n#plot\nsim &lt;- ggplot(sim.preds, aes(x=r, y=med)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Simulated Effects\") +\n  theme_minimal()\n\nsim\n\n\n\n\n\n\nComparing methods\n\n\ncode\n(atmean + average + sim)"
  },
  {
    "objectID": "predictionmethods24.html#multivariate-model-predictions",
    "href": "predictionmethods24.html#multivariate-model-predictions",
    "title": "Prediction Methods",
    "section": "",
    "text": "Here’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nmodelsummary(m3)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−70849.854\n\n\n\n(4473.400)\n\n\nRuns_scored\n3.123\n\n\n\n(0.338)\n\n\nRuns_allowed\n−1.887\n\n\n\n(0.357)\n\n\nSeason\n36.215\n\n\n\n(2.284)\n\n\nGames_behind\n−8.273\n\n\n\n(2.755)\n\n\nNum.Obs.\n838\n\n\nR2\n0.471\n\n\nR2 Adj.\n0.469\n\n\nAIC\n12963.1\n\n\nBIC\n12991.5\n\n\nLog.Lik.\n−6475.561\n\n\nF\n185.758\n\n\nRMSE\n549.20\n\n\n\n\n\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from. What’s missing here?\n\n\n\n\n\n\nControl\n\n\n\nHold all variables constant at means, medians, or modes, except the one of interest, so we can focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "slides.html#web-format",
    "href": "slides.html#web-format",
    "title": "Slides",
    "section": "",
    "text": "Course overview\nMatrix algebra basics\nProbability basics\nThinking about data\nBivariate model\nMultivariate model\nPrediction methods\nNormality\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "ex4answers2024.html",
    "href": "ex4answers2024.html",
    "title": "exercise #4 answers",
    "section": "",
    "text": "code\nnsim &lt;- function(i){\n  set.seed(12345)\n  X &lt;- rnorm_multi(i, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0.0, 0.0, 0.0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e) %&gt;%\n    mutate(pry=plogis(y)) %&gt;%\n    mutate(ybin=ifelse(pry&gt;.5,1,0)) %&gt;%\n    mutate(yb = rbinom(i,1,pry))\n  \n  mod &lt;- (glm(yb ~ x1 + x2, data=X , family=binomial(link=\"logit\")))\n  # print(summary(mod))\n  coef &lt;- numeric(0)\n  coef&lt;-coef(summary(mod))[1:3,1]\n  se &lt;- numeric(0)\n  se &lt;-coef(summary(mod))[1:3,2]\n  n &lt;- numeric(0)\n  n &lt;- i\n  var = c(\"x0\",\"x1\",\"x2\")\n  out &lt;- data.frame(var, coef, se, n)\n}\n\ni &lt;- seq(10,1000,1) #vector of i for \"nsim\" function\ndfout &lt;- map(i,nsim) |&gt; list_rbind()\nmleout &lt;- dfout %&gt;% filter(n&lt; 300)\n#plot\n\nb &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Coefficient\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\n\nse &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Standard Error\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\nb+se"
  },
  {
    "objectID": "ex4answers2024.html#q1---mle",
    "href": "ex4answers2024.html#q1---mle",
    "title": "exercise #4 answers",
    "section": "",
    "text": "code\nnsim &lt;- function(i){\n  set.seed(12345)\n  X &lt;- rnorm_multi(i, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0.0, 0.0, 0.0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e) %&gt;%\n    mutate(pry=plogis(y)) %&gt;%\n    mutate(ybin=ifelse(pry&gt;.5,1,0)) %&gt;%\n    mutate(yb = rbinom(i,1,pry))\n  \n  mod &lt;- (glm(yb ~ x1 + x2, data=X , family=binomial(link=\"logit\")))\n  # print(summary(mod))\n  coef &lt;- numeric(0)\n  coef&lt;-coef(summary(mod))[1:3,1]\n  se &lt;- numeric(0)\n  se &lt;-coef(summary(mod))[1:3,2]\n  n &lt;- numeric(0)\n  n &lt;- i\n  var = c(\"x0\",\"x1\",\"x2\")\n  out &lt;- data.frame(var, coef, se, n)\n}\n\ni &lt;- seq(10,1000,1) #vector of i for \"nsim\" function\ndfout &lt;- map(i,nsim) |&gt; list_rbind()\nmleout &lt;- dfout %&gt;% filter(n&lt; 300)\n#plot\n\nb &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Coefficient\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\n\nse &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Standard Error\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\nb+se"
  },
  {
    "objectID": "ex4answers2024.html#q1-ols",
    "href": "ex4answers2024.html#q1-ols",
    "title": "exercise #4 answers",
    "section": "Q1 OLS",
    "text": "Q1 OLS\n\n\ncode\nnsim &lt;- function(i){\n  set.seed(12345)\n  X &lt;- rnorm_multi(i, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0.0, 0.0, 0.0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e)\n  \n  mod &lt;- (lm(y ~ x1 + x2, data=X))\n  # print(summary(mod))\n  coef &lt;- numeric(0)\n  coef&lt;-coef(summary(mod))[1:3,1]\n  se &lt;- numeric(0)\n  se &lt;-coef(summary(mod))[1:3,2]\n  n &lt;- numeric(0)\n  n &lt;- i\n  var = c(\"x0\",\"x1\",\"x2\")\n  out &lt;- data.frame(var, coef, se, n)\n}\n\ni &lt;- seq(10,1000,1) #vector of i for \"nsim\" function\ndfout &lt;- map(i,nsim) |&gt; list_rbind()\nolsout &lt;- dfout %&gt;% filter(n&lt; 300)\n#plot\n\nb &lt;- ggplot(dfout, aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Coefficient\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\n\nse &lt;- ggplot(dfout, aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Standard Error\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\nb+se\n\n\n\n\n\n\n\n\n\nMLE’s small sample properties are far more variable than OLS small sample properties. In samples smaller than 300, MLE estimates varied from -19 to 990; OLS estimates varied from .30 to 2.37."
  },
  {
    "objectID": "ex4answers2024.html#q2---ill-treatment-and-torture-data",
    "href": "ex4answers2024.html#q2---ill-treatment-and-torture-data",
    "title": "exercise #4 answers",
    "section": "Q2 - Ill-Treatment and Torture data",
    "text": "Q2 - Ill-Treatment and Torture data\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nitt &lt;- itt %&gt;%\n  set_variable_labels(\n    lagRA = \"Restricted Access, t-1\",\n    civilwar = \"Civil War\",\n    lagprotest = \"Protests, t-1\",\n    p1 = \"Polity\",\n    p2 = \"Polity^2\",\n    p3 = \"Polity^3\", \n    wdi_gdpc = \"GDP per Capita (WDI)\",\n    wdi_pop = \"Population (WDI)\"\n  )\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +p1 +I(wdi_gdpc/1000) + I(wdi_pop/100000), data=itt)\nm2 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1 +I(wdi_gdpc/1000) + I(wdi_pop/100000), data=itt)\n\n# modelsummary(m1, coef_rename = TRUE, stars = TRUE, gof_map = c(\"adj.r.squared\", \"nobs\", \"F\", \"rmse\" ), gof_digits = 2)\n\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),  column.labels=c(\"Model 1\", \"Model 2\"),  dep.var.caption=\"Dependent Variable: Scarring Torture\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Restricted Access, t-1\", \"Civil War\", \"Protests, t-1\", \"Polity\", \"&lt;p&gt;Polity&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;\", \"&lt;p&gt;Polity&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;\", \"GDP per capita\", \"Population\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Scarring Torture\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nRestricted Access, t-1\n\n\n9.476*** (1.075)\n\n\n9.549*** (1.073)\n\n\n\n\nCivil War\n\n\n6.926*** (1.627)\n\n\n6.805*** (1.626)\n\n\n\n\nProtests, t-1\n\n\n0.203** (0.078)\n\n\n0.217** (0.077)\n\n\n\n\nPolity\n\n\n-1.243 (0.785)\n\n\n0.021 (0.057)\n\n\n\n\n\nPolity2\n\n\n\n0.121 (0.074)\n\n\n\n\n\n\n\nPolity3\n\n\n\n-0.003 (0.002)\n\n\n\n\n\n\nGDP per capita\n\n\n0.022 (0.045)\n\n\n0.020 (0.035)\n\n\n\n\nPopulation\n\n\n0.001*** (0.0002)\n\n\n0.001*** (0.0002)\n\n\n\n\nConstant\n\n\n8.317*** (2.452)\n\n\n4.963*** (0.820)\n\n\n\n\n\n\n\n\nObservations\n\n\n899\n\n\n899\n\n\n\n\nR2\n\n\n0.175\n\n\n0.172\n\n\n\n\nAdjusted R2\n\n\n0.167\n\n\n0.167\n\n\n\n\nF Statistic\n\n\n23.575*** (df = 8; 890)\n\n\n30.961*** (df = 6; 892)\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05"
  },
  {
    "objectID": "ex4answers2024.html#q2---simulated-effects",
    "href": "ex4answers2024.html#q2---simulated-effects",
    "title": "exercise #4 answers",
    "section": "Q2 - simulated effects",
    "text": "Q2 - simulated effects\nSimulate the distribution of \\(\\hat{\\beta}\\)’s and plot the expected scarring torture reports for different levels of protests against the government.\n\n\ncode\n#simulate b's \nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc + wdi_pop, data=itt)\n\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n                \ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= numeric(0), protest = numeric(0))\n\nfor (p in seq(1,40,1)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*0 + B$b3*p +B$b4*13+ B$b5*169+B$b6*2197+B$b7*8333+B$b8*.00005, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*0 + B$b3*p +B$b4*13+ B$b5*169+B$b6*2197+B$b7*8333+B$b8*.00005, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\n\nggplot()+\n  geom_ribbon(data=predictions, aes(x=protest, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=protest, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=protest, y=med0))+\n  geom_line(data= predictions, aes(x=protest, y=med1))+\n  labs ( colour = NULL, x = \"Protests Against Government\", y =  \"Expected Scarring Torture Reports\" ) +\n  annotate(\"text\", x = 8, y = 22, label = \"Restricted Access\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 8, y = 11, label = \"Unrestricted Access\", size=3.5, colour=\"gray30\")"
  },
  {
    "objectID": "inference24.html",
    "href": "inference24.html",
    "title": "Inference",
    "section": "",
    "text": "We’ve dug pretty extensively into how we produce OLS estimates of \\(\\widehat{\\beta}\\). Now we turn to the question of uncertainty about those estimates. After all, the data are a sample; the variables are perhaps imperfect measures; the variables may contain errors; we have ideas about the data generating process, but we don’t know the true model, so the model is certainly misspecified.\nWe need ways to express our uncertainty about the estimates of \\(\\beta\\), and about our confidence in the claims we make from the model.\nIf we simply assume \\(\\widehat{\\beta} = \\beta\\), then we are assuming:\n\nall the regression assumptions are met;\nthe sample is exactly equivalent to or representative of the population;\nthe model is specified correctly, and there are no sources of measurement error.\n\nThe probability these are all true is slim, but we are uncertain about the extent to which we do or do not meet these requirements. We need some statistical tools for quantifying our uncertainty about \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "inference24.html#framework-for-inference",
    "href": "inference24.html#framework-for-inference",
    "title": "Inference",
    "section": "Framework for Inference",
    "text": "Framework for Inference\n\nestablish a null hypothesis, e.g. \\(\\beta_1=0\\).\nestimate \\(\\widehat{\\beta_1}\\)\nestimate the error variance \\(\\widehat{\\sigma^2}\\).\ndetermine the distribution of \\(\\widehat{\\beta_k}\\).\ncompute a test statistic for \\(\\widehat{\\beta_1}\\).\ncompare that test statistic to critical values on the distribution of \\(\\widehat{\\beta_k}\\).\ndetermine the probability of observing the test statistic value, given the distribution of \\(\\widehat{\\beta_k}\\)"
  },
  {
    "objectID": "inference24.html#measuring-uncertainty",
    "href": "inference24.html#measuring-uncertainty",
    "title": "Inference",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\n\nStandard errors are basic measures of uncertainty.\nThe standard errors of the estimates of \\(\\widehat{\\beta_k}\\) are the standard deviations of the sampling distribution of the estimator.\nStandard errors are analogous to standard deviations surrounding the estimates."
  },
  {
    "objectID": "inference24.html#variance-of-the-estimates",
    "href": "inference24.html#variance-of-the-estimates",
    "title": "Inference",
    "section": "Variance of the estimates",
    "text": "Variance of the estimates\nIn the bivariate model, the variances of \\(\\widehat{\\beta_{0}}\\) and \\(\\widehat{\\beta_{1}}\\) are:\n\\[\\text{var}(\\widehat{\\beta_{0}}) = \\frac{\\sum\\limits_{i=1}^{n}x_{i}^{2}}{n \\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}} \\sigma^{2}\\] and\n\\[\\text{var}(\\widehat{\\beta_{1}}) = \\frac{\\sigma^{2}}{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}} \\]\nThe covariance of the the two estimates is\n\\[\\text{cov}(\\widehat{\\beta_{0}},\\widehat{\\beta_{1}}) = \\frac{-\\bar{x}}{ \\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}} \\sigma^{2}\\]\nCompute \\(\\widehat{\\sigma}^2\\) by\n\\[\\widehat{\\sigma}^2 = \\frac{\\sum\\widehat{u}^2}{n-k-1} \\]\nand the standard errors of \\(\\widehat{\\beta_{0}}\\) and \\(\\widehat{\\beta_{1}}\\) are the square roots of the first two expressions.\nRecalling that \\(\\mathbf{\\widehat{u'u}}\\) = SSE = \\(\\sum_{i=1}^{N}\\widehat{u^2}\\),\n\\[\\widehat{\\sigma}^2 = \\mathbf{\\widehat{u}'\\widehat{u}}  \\frac{1}{(n-k-1)}\\]\n\nRepetition: Variance-Covariance of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[ E(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0 \\\\\n0&\\sigma^{2} &\\cdots &0 \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\n0 &0 &\\cdots & \\sigma^{2} \\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0 \\\\\n0&1 &\\cdots &0 \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\n0 &0 &\\cdots & 1 \\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I} \\]\nThis is the variance-covariance matrix of the disturbances (\\(var-cov(e)\\)); it is symmetric, the main diagonal containing the variances of \\(\\epsilon_i\\). Assuming \\(Var(u|X)= \\sigma^2\\), the average of the main diagonal is \\(\\widehat{\\sigma^2}\\); the average of a constant is the constant."
  },
  {
    "objectID": "inference24.html#repetition-variance-covariance-of-widehatbeta",
    "href": "inference24.html#repetition-variance-covariance-of-widehatbeta",
    "title": "Inference",
    "section": "Repetition: Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Repetition: Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "inference24.html#variance-covariance-of-widehatbeta",
    "href": "inference24.html#variance-covariance-of-widehatbeta",
    "title": "Inference",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)}",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)}\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] =\\]\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k) \\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k) \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k) \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "inference24.html#standard-errors-of-beta_k",
    "href": "inference24.html#standard-errors-of-beta_k",
    "title": "Inference",
    "section": "Standard Errors of \\(\\beta_k\\) ",
    "text": "Standard Errors of \\(\\beta_k\\) \n\\[\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k) \\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k) \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & \\sqrt{var(\\beta_k)} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "inference24.html#assume",
    "href": "inference24.html#assume",
    "title": "Inference",
    "section": "Assume",
    "text": "Assume\n\\[ \\widehat{u} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}) \\]\nIf we meet the GM assumptions, then:\n\\[ \\widehat{\\beta} \\sim \\mathcal{N}(\\beta, \\widehat{var(\\beta)}) \\]\n\\[\\widehat{var(\\beta)} = \\widehat{\\sigma^{2}} \\mathbf{(X'X)^{-1}} \\]\nThe estimated error variance will be smaller relative to large variation in the \\(X\\) variables, so as variation in \\(X\\) grows, the uncertainty surrounding \\(\\mathbf{\\widehat{\\beta_k}}\\) will get smaller."
  },
  {
    "objectID": "inference24.html#think-about-the-ols-simulations",
    "href": "inference24.html#think-about-the-ols-simulations",
    "title": "Inference",
    "section": "Think about the OLS simulations",
    "text": "Think about the OLS simulations\nThe estimated error variance will be smaller relative to large variation in the \\(X\\) variables, so as variation in \\(X\\) grows, the uncertainty surrounding \\(\\mathbf{\\widehat{\\beta_k}}\\) will get smaller.\nwhat does this suggest about sample size? what does this suggest about correlation among \\(x\\) variables?"
  },
  {
    "objectID": "inference24.html#characteristics-of-textvarwidehatbeta_j",
    "href": "inference24.html#characteristics-of-textvarwidehatbeta_j",
    "title": "Inference",
    "section": "Characteristics of \\(\\text{var}(\\widehat{\\beta_{j}})\\)",
    "text": "Characteristics of \\(\\text{var}(\\widehat{\\beta_{j}})\\)\n\n\n\n\n\n\nStandard Errors vary with \\(\\sigma^{2}\\)\n\n\n\nThe variance of \\(\\widehat{\\beta_{j}}\\) varies directly with \\(\\sigma^{2}\\), so as the error variance increases, so does the variance surrounding \\(\\widehat{\\beta_{j}}\\) - this makes sense if we think about what the variance of the error term suggests. Large \\(\\sigma^{2}\\) suggests our model is not predicting \\(Y\\) very effectively - if the model itself is imprecise, then its component parts (the \\(\\widehat{\\beta_{j}}\\)s) will also be imprecise. So as \\(\\sigma^{2}\\) increases, so does \\(\\text{var}(\\widehat{\\beta_{j}})\\).\n\n\n\n\n\n\n\n\nStandard Errors vary with variation in \\(\\mathbf{X}\\)\n\n\n\nThe variance of \\(\\widehat{\\beta_{j}}\\) varies indirectly with \\(\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}\\), so as the sum of squares of \\(x\\) increases, \\(\\text{var}(\\widehat{\\beta_{j}})\\) decreases. This too makes sense because it means the more variability in \\(x\\) or the more information our \\(x\\) variables contain, the better our estimates of \\(\\widehat{\\beta_{j}}\\) are. Thus, more variability in the \\(x\\)s reduces \\(\\text{var}(\\widehat{\\beta_{j}})\\).\n\n\n\n\n\n\n\n\nStandard Errors vary with sample size\n\n\n\nSample size varies inversely with \\(\\text{var}(\\widehat{\\beta_{j}})\\) - larger samples (probably with greater variation in the \\(x\\)s and thus larger \\(\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}\\)) will produce more precise estimates of \\(\\widehat{\\beta_{j}}\\) and smaller \\(\\text{var}(\\widehat{\\beta_{j}})\\)."
  },
  {
    "objectID": "inference24.html#inferences-about-widehatbeta",
    "href": "inference24.html#inferences-about-widehatbeta",
    "title": "Inference",
    "section": "Inferences about \\(\\widehat{\\beta}\\)",
    "text": "Inferences about \\(\\widehat{\\beta}\\)\nThe standard error of \\(\\widehat{\\beta_k}\\) is the square root of the \\(k^{th}\\) diagonal element of the variance-covariance matrix of \\(\\widehat{\\beta}\\) ~\nIn scalar terms,\n\\[s.e.(\\widehat{\\beta_{1}}) =\\sqrt{\\frac{\\widehat{\\sigma^{2}}}{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}} \\nonumber   \\nonumber  \n= \\frac{\\widehat{\\sigma}}{\\sqrt{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}} \\]\nand the critical value is given by\n\\[z=\\frac{\\widehat{\\beta_{j}}-\\beta_{j}}{\\frac{\\widehat{\\sigma}}{\\sqrt{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}}}\\]\n\\[t= \\frac{\\widehat{\\beta_{j}}-\\beta_{j}\\sqrt{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}}{\\widehat{\\sigma}} \\]\nThis is not Normal, even though the numerator is - the denominator is composed of the squared residuals, each of which is a \\(\\chi^{2}\\) variable. A standard normal variable divided by a \\(\\chi^{2}\\) distributed variable is distributed \\(t\\) with \\(n-k-1\\) degrees of freedom."
  },
  {
    "objectID": "inference24.html#confidence-intervals",
    "href": "inference24.html#confidence-intervals",
    "title": "Inference",
    "section": "",
    "text": "The estimates of \\(\\widehat{\\beta_{j}}\\) are drawn from a normal distribution, and we know the distribution of the variance, and we know how those distributions are related. For the \\(t\\) distribution, we know that 95% of the probability mass falls within 2 standard deviations of the mean, so if we construct a 95% confidence interval, we can say that 95% of the time the true value of \\(\\beta\\) will fall within the interval. Put another way, the interval contains a range of probable values for the true value of \\(\\beta\\).\nBecause we know (from above) that\n\\[t= \\frac{\\widehat{\\beta_{j}}-\\beta_{j}}{s.e. \\widehat{\\beta_{j}}}\\]\nwe can easily compute a confidence interval surrounding \\(\\widehat{\\beta_{j}}\\) by\n\\[CI_{c} = \\widehat{\\beta_{j}} \\pm c \\cdot {s.e. \\widehat{\\beta_{j}}}\\]\nwhere \\(c\\) represents the size of the confidence interval, so if \\(c\\) is the 97.5th percentile in the \\(t\\) distribution with \\(6\\) degrees of freedom, then the value of \\(c\\) is 2.447 (see the t-table in the back of the text). Thus, we compute the confidence interval by:\n\\[CI_{97.5} = \\widehat{\\beta_{j}} +  2.447 \\cdot {s.e. \\widehat{\\beta_{j}}}, \\widehat{\\beta_{j}} -  2.447 \\cdot {s.e. \\widehat{\\beta_{j}}}\\]"
  },
  {
    "objectID": "inference24.html#point-estimates",
    "href": "inference24.html#point-estimates",
    "title": "Inference",
    "section": "",
    "text": "Using confidence intervals, we state the probability the true coefficient lies between the upper and lower bounds of the interval. Point estimate tests allow us to test specific hypotheses about the value of \\(\\widehat{\\beta_{j}}\\). Let’s go back to the computation of a \\(t\\) statistic:\n\\[t= \\frac{\\widehat{\\beta_{j}}-\\beta_{j}}{s.e. \\widehat{\\beta_{j}}}\\]\nYou’ll notice in the numerator we’re subtracting the true coefficient from our estimate. Of course, we don’t know the true values of \\(\\beta\\), so we choose a value to which we want to compare our estimate. Then we’re generating the probability of drawing a sample with \\(\\widehat{\\beta_{j}}\\) given the value we choose."
  },
  {
    "objectID": "inference24.html#hypotheses",
    "href": "inference24.html#hypotheses",
    "title": "Inference",
    "section": "",
    "text": "Normally, we set up our hypothesis tests such that the value of \\(\\beta\\) we select is zero. Thus, we state null and alternative hypotheses:\n\\[H_0:\\beta_j=0 \\nonumber  \\\\\nH_1:\\beta_j\\ne 0 \\]\nThen, we choose significance levels, find the critical value associated with that significance level, and compare \\(t_{\\widehat{\\beta_j}}=\\widehat{\\beta_j}/se(\\widehat{\\beta_j})\\) to the critical value. Finally, we either reject or fail to reject \\(H_0\\). Note that we could choose other values to which we could compare the probability of \\(\\widehat{\\beta_{j}}\\), though selecting those values can be tricky."
  },
  {
    "objectID": "inference24.html#hypotheses-1",
    "href": "inference24.html#hypotheses-1",
    "title": "Inference",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nThe relationship we expect between \\(x\\) and \\(y\\) is the alternative hypothesis.\n\\(H_a\\) is the alternative to the null - the null includes everything that’s not the alternative; mathematically, it always includes zero.\nNon-directional alternative hypotheses simply expect \\(\\widehat{\\beta}\\) will not be zero. These are not very specific, less desirable.\nDirectional hypotheses expect a direction, e.g., \\(\\widehat{\\beta}\\) will be positive; the null in this case is \\(\\widehat{\\beta} \\leq 0\\). This is more specific, more desirable.\n\n\n\n\n\n\n\nDirectional Hypotheses\n\n\n\nStatistical software doesn’t know your data or argument}\nR/Stata does not know your hypotheses, whether directional or not; it does not know if your data are panel, time series, or cross sectional.\n\nR/Stata report 2-tailed p-values.\nIf you have a directional hypothesis, you need the 1-tailed p-value.\nDivide the 2-tailed value by 2 - in a 2-tailed test, it’s divided between the two tails. You want just one tail, so dividing the 2-tailed probability by 2 gives you the 1-tailed probability.\n\n\n\n\nIf you expect \\(\\widehat{\\beta} &gt; 0\\), and your estimated \\(\\widehat{\\beta} = -1.2\\) with a p-value of .001, you cannot reject the null. While \\(\\widehat{\\beta}\\) is statistically different from zero, it’s not in the direction of your hypothesis, so the result is not meaningful, does not support the alternative."
  },
  {
    "objectID": "inference24.html#dont-expect-the-null",
    "href": "inference24.html#dont-expect-the-null",
    "title": "Inference",
    "section": "Don’t Expect the Null",
    "text": "Don’t Expect the Null\nThe classical hypothesis test is always constructed such that the null includes zero. It is not possible to expect as the alternative hypothesis, that \\(\\widehat{\\beta} = 0\\). We should never see the alternative hypothesis::\nX will have no relationship to y: \\(H_A: \\beta_1=0\\)\nIn the classical test, the null includes a specific point (usually zero), though it can contain that point and a range of other values, e.g.:\n\\[H_0: \\beta_1 \\leq 0\\]\nHere, the null that \\(\\beta_1\\) is less than or equal to zero includes zero, but also includes all negative values. The alternative is that \\(\\beta_1\\) is greater than zero, so the alternative is a range of values, not a specific point.\nIf we flip this around to say “I expect \\(\\beta_1\\) to be zero”, then we are expecting the null hypothesis. Note that the alternative hypothesis is now a specific point, and the null is a range of values without any specific point expectation.\nIt’s important to note that the null may be around a point other than zero. We might expect the coefficient to be different from 5, so the null is \\(H_0: \\beta_1=5\\). Or, we might expect the coefficient to be less than or equal to 5, so the null is \\(H_0: \\beta_1 \\leq 5\\).\nThe point is we cannot expect in the alternative that \\(\\beta_1\\) is zero where the null is that it is anything except zero unless we build a new type of test. See Gill (1999) for an excellent discussion of this."
  },
  {
    "objectID": "inference24.html#joint-hypothesis-tests",
    "href": "inference24.html#joint-hypothesis-tests",
    "title": "Inference",
    "section": "",
    "text": "Normally we test hypotheses about specific parameters, but there are cases where we are interested in a hypothesis like this:\n\\[H_0:\\beta_1=\\beta_2=0 \\nonumber  \\\\\nH_1:\\beta_1\\ne \\beta_2\\ne 0\\]\nTo test something like this (and really, tests like this are something we should be extremely interested in), we use the F-statistic.\nThe joint hypothesis above has two restrictions - we expect two parameters to be zero in the null. We can have as many as \\(k-1\\) exclusions or restrictions in the model, and we typically call the exclusions or restrictions \\(q\\)."
  },
  {
    "objectID": "inference24.html#intuition",
    "href": "inference24.html#intuition",
    "title": "Inference",
    "section": "Intuition",
    "text": "Intuition\nDecompose the variation in the data and model this way:\n\\[TSS= MSS+RSS\\]\nSuppose a regression like this:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\nOur usual, single coefficient hypothesis test, say on \\(\\beta_3\\) is\n\\[H_0: \\beta_3=0\\]\nwhich is equivalent to\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + {\\mathbf{0}} x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\] call this the unrestricted model:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\n\\[RSS_U = \\sum_{i=1}^N \\widehat{u_U}^2\\]\nand refer to this as the restricted model:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + {\\mathbf{0}} x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\n\\[RSS_R = \\sum_{i=1}^N \\widehat{u_R}^2\\]\nAre these quantities different from one another?\n\\[RSS_U \\neq RSS_R\\]\nIf\n\\[RSS_U = RSS_R = 0\\]\nthen the two models are indistinguishable, and \\(\\beta_3\\) is not different from zero. Alternatively, if\n\\[RSS_U \\neq RSS_R \\neq 0\\]\nwe can reject the null that the models are the same, and the specific null that \\(\\beta_3=0\\). By the way, this single coefficient F-test is inferentially equivalent to the t-test; in fact, \\(F_{\\beta_{k}}= t^2_{\\beta_{k}}\\)."
  },
  {
    "objectID": "inference24.html#extension",
    "href": "inference24.html#extension",
    "title": "Inference",
    "section": "Extension",
    "text": "Extension\nExtend this to the following hypothesis test:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\n\\[H_0: \\beta_3=\\beta_4=\\beta_5=0\\]\nor that the effects of \\(x_3, x_4, x_5\\) are individually and jointly zero.\nUnrestricted:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\nRestricted:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + {\\mathbf{0}} x_3 + {\\mathbf{0}} x_4 +{\\mathbf{0}} x_5 + u_i \\]"
  },
  {
    "objectID": "inference24.html#f-test",
    "href": "inference24.html#f-test",
    "title": "Inference",
    "section": "F-test",
    "text": "F-test\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)} \\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or “normed” by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\)."
  },
  {
    "objectID": "inference24.html#alternatively",
    "href": "inference24.html#alternatively",
    "title": "Inference",
    "section": "Alternatively",
    "text": "Alternatively\nWe could equivalently state this as:\n\\[F=\\frac{(\\sum\\widehat{u}^{2}_{R}-\\sum\\widehat{u}^{2}_{U})/q}{\\sum\\widehat{u}^{2}_{U}/(n-k-1)} \\]\nNotice that the denominator is also \\(\\widehat{\\sigma^2}\\). The resulting statistic is distributed \\(F\\sim F_{q,n-k-1}\\).\nor we can write the same thing in terms of the \\(R^2\\) estimates of the two models:\n\\[F=\\frac{(R^2_{U}-R^2_{R})/q}{(1-R^2_{U})/(n-k-1)} \\]"
  },
  {
    "objectID": "inference24.html#in-practice",
    "href": "inference24.html#in-practice",
    "title": "Inference",
    "section": "In practice ",
    "text": "In practice \nFor a joint hypothesis test, we estimate two nested models which we label the restricted and unrestricted models.\n\n\n\n\n\n\nNote\n\n\n\nTwo models are nested iff they contain precisely the same observations, and the variables in one model are a strict subset of the variables in the other.\n\n\nThe unrestricted model has all \\(k\\) variables; the restricted model has \\(k-q\\) variables, excluding the variables whose joint effect we want to test.\nEstimate the models, and examine how much the RSS increases when we exclude the \\(q\\) variables from the model - of course, the RSS will always increase when we drop variables, but the question is how much will it increase. The F-test measures the increase in RSS in the restricted model relative to that of the unrestricted model."
  },
  {
    "objectID": "inference24.html#a-common-example",
    "href": "inference24.html#a-common-example",
    "title": "Inference",
    "section": "A common example",
    "text": "A common example\nEvery OLS model reports a “model F-test” - this compares the model you estimated with all your \\(x\\) variables (this is the unrestricted model) to the null model - no variables, just a constant. The null hypothesis is\n\\[H_0: \\beta_1=\\beta_2 \\ldots = \\beta_k = 0\\]\nIt’s a test of your model against the model with only a constant - it’s comparing the value of your model against the model where our best guess at what explains \\(y\\) is \\(\\beta_0 = \\bar{y}\\)."
  },
  {
    "objectID": "inference24.html#tests-like-this-are-important-and-powerful",
    "href": "inference24.html#tests-like-this-are-important-and-powerful",
    "title": "Inference",
    "section": "Tests like this are important and powerful:",
    "text": "Tests like this are important and powerful:\nOur enterprise is really about comparative model testing. Hypothesis tests on individual coefficients do not allow us to compare models. Our theories imply different explanations for phenomena, and thus different empirical models. To treat the theories comparatively, we must test our models comparatively as well. So tests such as the joint hypothesis test are critically important to that endeavor.\nThis test is a distributional test; that is, its product is a point or range on a known probability distribution. Thus, we can be quite exactly about the probability with which we either do or do not reject the joint null hypothesis.\nThis is not the case of another popular measure of model fit, the \\(R^2\\), which has no sampling distribution. The \\(R^2\\) cannot tell us anything probabilistic about the model or more specifically, about the hypothesis that a variable or variables do or do not significantly affect the fit of the model."
  },
  {
    "objectID": "inference24.html#dont-expect-the-null-hypothesis",
    "href": "inference24.html#dont-expect-the-null-hypothesis",
    "title": "Inference",
    "section": "Don’t Expect the Null Hypothesis",
    "text": "Don’t Expect the Null Hypothesis"
  },
  {
    "objectID": "inference24.html#dont-expect-the-null-hypothesis-1",
    "href": "inference24.html#dont-expect-the-null-hypothesis-1",
    "title": "Inference",
    "section": "Don’t Expect the Null Hypothesis",
    "text": "Don’t Expect the Null Hypothesis"
  },
  {
    "objectID": "inference24.html#measuring-uncertainty-1",
    "href": "inference24.html#measuring-uncertainty-1",
    "title": "Inference",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\n\nStandard errors are basic measures of uncertainty.\nThe standard errors of the estimates of \\(\\widehat{\\beta_k}\\) are the standard deviations of the sampling distribution of the estimator.\nStandard errors are analogous to standard deviations surrounding the estimates."
  },
  {
    "objectID": "specification24.html",
    "href": "specification24.html",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "We’re trying to explain variation. What does this look like?\n\nbest guess at variation in \\(y\\) would be its mean (constant-only model). with variables, variation is composed of two parts: \\(y = \\mu + \\epsilon\\). The first is \\(x\\beta\\), our systematic component; the second is the random component. \\(x\\) shapes the systematic part of \\(y\\).\ntotal variation in \\(y\\) is the sum of the variance of these components: \\(\\text{var}(y) = \\text{var}(x)\\beta + \\text{var}(e)\\)\nthe squared variation in \\(y\\) due to \\(x\\) is the MSS.\nthe squared variation in \\(e\\) is the RSS.\ntheir sum is the TSS.\n\n\n\nRecall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\).\n\\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\).\n\\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\).\n\n\n\n\\[TSS= MSS+RSS\\]\nand\n\\[R^2 = \\frac{MSS}{TSS}\\] or\n\\[R^2 = 1- \\frac{RSS}{TSS}\\]\n\n\n\n\\(R^2\\) increases as \\(k\\) increases. The adjusted \\(R^2\\) penalizes for additional regressors (though still increases with \\(k\\)):\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(N-1)}{(N-k-1)} \\]\nin the numerator, adjusting for \\(N\\) minus one for the constant, and in the denominator for \\(N-k-1\\).\n\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight\n\n\n\n\n\n\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\nbecause \\(R^2\\) does not have a sampling distribution, we cannot generally say an \\(R^2\\) value is large or small - relative to what? If, say, it were distributed normally, we could make claims about the probability of observing a given \\(R^2\\) value.\nit is possible to construct such a sampling distribution, but it seems noone ever does, so the value of the \\(R^2\\) is not that great.\n\n\n\n\nRoot Mean Squared Error (standard error of the estimate):\n\\[RMSE = \\sqrt{\\frac{RSS}{(N-k)}}\\]\nIs the average (squared) error. It’s useful because it’s in the same units as \\(y\\), so interpretable as the average (squared) distance of an observation from the regression line.\n\n\n\nIn the linear model, the F-test is probably the most useful tool for comparing (nested) models. Reviewing, models are nested iff:\ntheir samples are identical. the variables in one are a strict subset of those in the other.\nThe nested models are the Restricted and Unrestricted.\n\n\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or “normed” by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\).\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#summary(m1)\n# estimation sample to ensure restricted model2 is nested in unrestricted model 1\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittm1sample &lt;- itt %&gt;%  filter(used==\"TRUE\")\nm2 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+wdi_gdpc, data=itt)\n#summary(m2)\n\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),  column.labels=c(\"Model 1\", \"Model 2\"),  dep.var.caption=\"Dependent Variable: Scarring Torture\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Restricted Access, t-1\", \"Civil War\", \"Protests, t-1\", \"Polity\", \"&lt;p&gt;Polity&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;\", \"&lt;p&gt;Polity&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;\", \"GDP per capita\", \"Population\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Scarring Torture\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nRestricted Access, t-1\n\n\n11.012*** (0.993)\n\n\n11.092*** (0.992)\n\n\n\n\nCivil War\n\n\n7.841*** (1.618)\n\n\n7.741*** (1.617)\n\n\n\n\nProtests, t-1\n\n\n0.237** (0.077)\n\n\n0.249** (0.077)\n\n\n\n\nPolity\n\n\n-1.327 (0.790)\n\n\n0.021 (0.058)\n\n\n\n\n\nPolity2\n\n\n\n0.117 (0.075)\n\n\n\n\n\n\n\nPolity3\n\n\n\n-0.003 (0.002)\n\n\n\n\n\n\nGDP per capita\n\n\n0.00000 (0.00005)\n\n\n0.00002 (0.00004)\n\n\n\n\nPopulation\n\n\n9.072*** (2.459)\n\n\n5.039*** (0.825)\n\n\n\n\n\n\n\n\nObservations\n\n\n899\n\n\n899\n\n\n\n\nR2\n\n\n0.163\n\n\n0.160\n\n\n\n\nAdjusted R2\n\n\n0.156\n\n\n0.155\n\n\n\n\nF Statistic\n\n\n24.785*** (df = 7; 891)\n\n\n34.048*** (df = 5; 893)\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\n\ncode\nftest &lt;- anova(m1,m2)\n\nkable(ftest, digits=3)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n891\n93364.23\nNA\nNA\nNA\nNA\n\n\n893\n93684.63\n-2\n-320.405\n1.529\n0.217\n\n\n\n\n\ncode\n#stargazer(ftest$Res.Df, ftest$RSS, ftest$F, ftest$`Pr(&gt;F)`, type=\"html\")\n#,  single.row=TRUE, header=FALSE, digits=3, align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\nHow much do individual observations shape the regression estimates? This discussion draws an excellent monograph by Fox (1991).\n\nleverage - how much an observation affects the slope of the regression line - it might have great leverage, but be on the line, so not discrepant.\ndiscrepancy - how unusual an observation is.\ninfluence - how much an unusual observation affects the slope of the regression line. How much a discrepant observation exerts leverage.\n\n\n\n\n\\(h\\) is an element of the \\(hat\\) matrix:\n\\[ y = X \\beta \\] \\[= X(X'X)^{-1} X'y \\] \\[=Hy \\] where \\[H = X(X'X)^{-1} X' \\]\nThe matrix \\(H\\) is a square \\(n,n\\) matrix; the main diagonal indicates the extent to which any observation has leverage on the regression line by virtue of being large relative to the other \\(X\\)s.\nObservations where \\(h&gt; 2(k/N)\\) are often thought of as outliers with the potential to influence the regression estimates.\n\n\n\n\nA common indicator of discrepancy is to standardize residuals to see which fall outside of standard confidence bounds. This is problematic (for reasons set aside), so the replacement is Studentized residuals. About 95% of these residuals should fall between -2/+2, and those that do not may be discrepant or unusual, and worth scrutiny.\n\n\n\n\nThere are a number of measures of influence including DFBETA, DFFITS, and Cook’s Distance. In general, measures of influence evaluate the estimates progressively excluding each observation; observations that substantialliy change the estimates are more influential.\n\n\n\n\nCook’s D combines measures of discrepancy and leverage to indicate how influential an observation is:\n\\[ \\frac{s(e^2)}{k} \\cdot \\frac{h_i}{1-h_i}\\]\nwhere the first term is standardized residuals indicating discrepancy, and the second is \\(h\\), measuring leverage. If both are high, then influence is high; otherwise, influence is low.\n\n\n\n\nFox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\n\n\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal() \n\n\n\n\n\nLooking at the upper-middle of the plot, Turkey, Sudan, Russia, and Indonesia seem especially influential. On the x-axis, the UK, France, and Qatar have high leverage, so inordinately affect the regression line. On the y-axis, Turkey (several years), Italy, Mexico, and Indonesia are discrepant or unusual.\n\n\n\n\n\nWhy are observations outliers?\nWhat should we do with them?\n\n\n\n\n\n\nwhat do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?\n\n\n\n\n\nTwo most common nonlinear functions applied to variables in political science are:\n\nnatural log\npolynomial\n\n\n\nPermit nonlinear but monotonic1 changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\n\n\n\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)\n\n\n\n\nUsing data we’ve seen before, let’s look at the effect of GDP per capita on infant mortality rate. We’ll look at the effect of GDP per capita on infant mortality rate using a linear model, a log-linear model, and a polynomial model, compared below.\n\n\ncode\ncensor &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/data/censorcy.csv\", header=TRUE)\ncensor &lt;- subset(censor, select=-c(lngdp))\n\nggplot(data=censor, aes(x=gdppc, y=IMR)) +\n   geom_point(data=censor, aes(x=gdppc, y=IMR) ,color=\"green\") + \n   geom_text_repel(label=censor$ctry, size=3) +\n  stat_poly_line(data=censor, formula = y ~ x, se=FALSE, color=\"black\") +\n  stat_poly_line(data=censor, formula = y ~ log(x), se=FALSE, linetype=\"dashed\", color=\"red\" )   +\n  stat_poly_line(data=censor, formula = y ~ poly(x,2), se=FALSE, linetype=\"dashed\" )   +\n  labs(x=\"Polity\", y=\"Predicted Infant Mortality Rate\")+\n  theme_minimal()\n\n\n\n\n\nYou can see the linear model is a poor fit. The other two are better fits to the observed data (blue line is polynomial, red line is logarithmic.)\n\n\n\n\n\nadding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)\n\n\n\n\nWe know perfect collinearity is not much of a problem since we can’t estimate \\(\\widehat{\\beta}\\) at all in its presence - the matrix \\((X'X)^{-1}\\) is singular and thus can’t be inverted.\nWhat about correlation among \\(X\\) variables short of perfect collinearity?\n\n\nRecall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE.\n\n\n\n\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)\n\n\n\n\n\n\nData is never missing randomly.\nThe process generating missing data is systematic.\n\n\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\).\n\n\n\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates.\n\n\n\n\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data.\n\n\n\n\n\n\n\n\n\n\n\nFox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#model-fit",
    "href": "specification24.html#model-fit",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "We’re trying to explain variation. What does this look like?\n\nbest guess at variation in \\(y\\) would be its mean (constant-only model). with variables, variation is composed of two parts: \\(y = \\mu + \\epsilon\\). The first is \\(x\\beta\\), our systematic component; the second is the random component. \\(x\\) shapes the systematic part of \\(y\\).\ntotal variation in \\(y\\) is the sum of the variance of these components: \\(\\text{var}(y) = \\text{var}(x)\\beta + \\text{var}(e)\\)\nthe squared variation in \\(y\\) due to \\(x\\) is the MSS.\nthe squared variation in \\(e\\) is the RSS.\ntheir sum is the TSS.\n\n\n\nRecall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\).\n\\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\).\n\\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\)."
  },
  {
    "objectID": "specification24.html#model-fit-1",
    "href": "specification24.html#model-fit-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Recall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\). \\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\). \\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\)."
  },
  {
    "objectID": "specification24.html#model-fit-r2",
    "href": "specification24.html#model-fit-r2",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\[TSS= MSS+RSS\\]\nand\n\\[R^2 = \\frac{MSS}{TSS}\\] or\n\\[R^2 = 1- \\frac{RSS}{TSS}\\]"
  },
  {
    "objectID": "specification24.html#fit-r2_adj",
    "href": "specification24.html#fit-r2_adj",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\(R^2\\) increases as \\(k\\) increases. The adjusted \\(R^2\\) penalizes for additional regressors (though still increases with \\(k\\)):\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(N-1)}{(N-k-1)} \\]\nin the numerator, adjusting for \\(N\\) minus one for the constant, and in the denominator for \\(N-k-1\\).\n\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight\n\n\n\n\n\n\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\nbecause \\(R^2\\) does not have a sampling distribution, we cannot generally say an \\(R^2\\) value is large or small - relative to what? If, say, it were distributed normally, we could make claims about the probability of observing a given \\(R^2\\) value.\nit is possible to construct such a sampling distribution, but it seems noone ever does, so the value of the \\(R^2\\) is not that great."
  },
  {
    "objectID": "specification24.html#fit-r2_adj-1",
    "href": "specification24.html#fit-r2_adj-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight"
  },
  {
    "objectID": "specification24.html#fit-rmse",
    "href": "specification24.html#fit-rmse",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Root Mean Squared Error (standard error of the estimate):\n\\[RMSE = \\sqrt{\\frac{RSS}{(N-k)}}\\]\nIs the average (squared) error. It’s useful because it’s in the same units as \\(y\\), so interpretable as the average (squared) distance of an observation from the regression line."
  },
  {
    "objectID": "specification24.html#fit-f-test",
    "href": "specification24.html#fit-f-test",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "In the linear model, the F-test is probably the most useful tool for comparing (nested) models. Reviewing, models are nested iff:\ntheir samples are identical. the variables in one are a strict subset of those in the other.\nThe nested models are the Restricted and Unrestricted.\n\n\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or “normed” by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\).\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#summary(m1)\n# estimation sample to ensure restricted model2 is nested in unrestricted model 1\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittm1sample &lt;- itt %&gt;%  filter(used==\"TRUE\")\nm2 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+wdi_gdpc, data=itt)\n#summary(m2)\n\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),  column.labels=c(\"Model 1\", \"Model 2\"),  dep.var.caption=\"Dependent Variable: Scarring Torture\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Restricted Access, t-1\", \"Civil War\", \"Protests, t-1\", \"Polity\", \"&lt;p&gt;Polity&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;\", \"&lt;p&gt;Polity&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;\", \"GDP per capita\", \"Population\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Scarring Torture\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nRestricted Access, t-1\n\n\n11.012*** (0.993)\n\n\n11.092*** (0.992)\n\n\n\n\nCivil War\n\n\n7.841*** (1.618)\n\n\n7.741*** (1.617)\n\n\n\n\nProtests, t-1\n\n\n0.237** (0.077)\n\n\n0.249** (0.077)\n\n\n\n\nPolity\n\n\n-1.327 (0.790)\n\n\n0.021 (0.058)\n\n\n\n\n\nPolity2\n\n\n\n0.117 (0.075)\n\n\n\n\n\n\n\nPolity3\n\n\n\n-0.003 (0.002)\n\n\n\n\n\n\nGDP per capita\n\n\n0.00000 (0.00005)\n\n\n0.00002 (0.00004)\n\n\n\n\nPopulation\n\n\n9.072*** (2.459)\n\n\n5.039*** (0.825)\n\n\n\n\n\n\n\n\nObservations\n\n\n899\n\n\n899\n\n\n\n\nR2\n\n\n0.163\n\n\n0.160\n\n\n\n\nAdjusted R2\n\n\n0.156\n\n\n0.155\n\n\n\n\nF Statistic\n\n\n24.785*** (df = 7; 891)\n\n\n34.048*** (df = 5; 893)\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\n\ncode\nftest &lt;- anova(m1,m2)\n\nkable(ftest, digits=3)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n891\n93364.23\nNA\nNA\nNA\nNA\n\n\n893\n93684.63\n-2\n-320.405\n1.529\n0.217\n\n\n\n\n\ncode\n#stargazer(ftest$Res.Df, ftest$RSS, ftest$F, ftest$`Pr(&gt;F)`, type=\"html\")\n#,  single.row=TRUE, header=FALSE, digits=3, align=TRUE,  font.size=\"small\")"
  },
  {
    "objectID": "specification24.html#fit-f-test-1",
    "href": "specification24.html#fit-f-test-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "How do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or ``normed’’ by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\)."
  },
  {
    "objectID": "specification24.html#overview",
    "href": "specification24.html#overview",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "In the early 1990s, there was a significant (and very boring) debate over these measures of model fit. The useful parts focused on what we, as a discipline, are trying to accomplish in modeling. There was no real resolution. \\~\\\nThat same period was the early part of both a technological and methods revolution. Computing power democratized and increased exponentially. This enabled what some have called the MLE revolution. Whatever the case, debates over OLS fit diminished in relevance and frequency."
  },
  {
    "objectID": "specification24.html#outliers",
    "href": "specification24.html#outliers",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "How much do individual observations shape the regression estimates? This discussion draws an excellent monograph by Fox (1991).\n\nleverage - how much an observation affects the slope of the regression line - it might have great leverage, but be on the line, so not discrepant.\ndiscrepancy - how unusual an observation is.\ninfluence - how much an unusual observation affects the slope of the regression line. How much a discrepant observation exerts leverage.\n\n\n\n\n\\(h\\) is an element of the \\(hat\\) matrix:\n\\[ y = X \\beta \\] \\[= X(X'X)^{-1} X'y \\] \\[=Hy \\] where \\[H = X(X'X)^{-1} X' \\]\nThe matrix \\(H\\) is a square \\(n,n\\) matrix; the main diagonal indicates the extent to which any observation has leverage on the regression line by virtue of being large relative to the other \\(X\\)s.\nObservations where \\(h&gt; 2(k/N)\\) are often thought of as outliers with the potential to influence the regression estimates.\n\n\n\n\nA common indicator of discrepancy is to standardize residuals to see which fall outside of standard confidence bounds. This is problematic (for reasons set aside), so the replacement is Studentized residuals. About 95% of these residuals should fall between -2/+2, and those that do not may be discrepant or unusual, and worth scrutiny.\n\n\n\n\nThere are a number of measures of influence including DFBETA, DFFITS, and Cook’s Distance. In general, measures of influence evaluate the estimates progressively excluding each observation; observations that substantialliy change the estimates are more influential.\n\n\n\n\nCook’s D combines measures of discrepancy and leverage to indicate how influential an observation is:\n\\[ \\frac{s(e^2)}{k} \\cdot \\frac{h_i}{1-h_i}\\]\nwhere the first term is standardized residuals indicating discrepancy, and the second is \\(h\\), measuring leverage. If both are high, then influence is high; otherwise, influence is low.\n\n\n\n\nFox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\n\n\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal() \n\n\n\n\n\nLooking at the upper-middle of the plot, Turkey, Sudan, Russia, and Indonesia seem especially influential. On the x-axis, the UK, France, and Qatar have high leverage, so inordinately affect the regression line. On the y-axis, Turkey (several years), Italy, Mexico, and Indonesia are discrepant or unusual.\n\n\n\n\n\nWhy are observations outliers?\nWhat should we do with them?\n\n\n\n\n\n\nwhat do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?"
  },
  {
    "objectID": "specification24.html#outliers-1",
    "href": "specification24.html#outliers-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Fox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal()"
  },
  {
    "objectID": "specification24.html#thinking-about-outliers",
    "href": "specification24.html#thinking-about-outliers",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Why are observations outliers?\nWhat should we do with them?"
  },
  {
    "objectID": "specification24.html#model-outliers",
    "href": "specification24.html#model-outliers",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "what do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?"
  },
  {
    "objectID": "specification24.html#functional-form",
    "href": "specification24.html#functional-form",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Two most common nonlinear functions applied to variables in political science are:\n\nnatural log\npolynomial\n\n\n\nPermit nonlinear but monotonic1 changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\n\n\n\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)\n\n\n\n\nUsing data we’ve seen before, let’s look at the effect of GDP per capita on infant mortality rate. We’ll look at the effect of GDP per capita on infant mortality rate using a linear model, a log-linear model, and a polynomial model, compared below.\n\n\ncode\ncensor &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/data/censorcy.csv\", header=TRUE)\ncensor &lt;- subset(censor, select=-c(lngdp))\n\nggplot(data=censor, aes(x=gdppc, y=IMR)) +\n   geom_point(data=censor, aes(x=gdppc, y=IMR) ,color=\"green\") + \n   geom_text_repel(label=censor$ctry, size=3) +\n  stat_poly_line(data=censor, formula = y ~ x, se=FALSE, color=\"black\") +\n  stat_poly_line(data=censor, formula = y ~ log(x), se=FALSE, linetype=\"dashed\", color=\"red\" )   +\n  stat_poly_line(data=censor, formula = y ~ poly(x,2), se=FALSE, linetype=\"dashed\" )   +\n  labs(x=\"Polity\", y=\"Predicted Infant Mortality Rate\")+\n  theme_minimal()\n\n\n\n\n\nYou can see the linear model is a poor fit. The other two are better fits to the observed data (blue line is polynomial, red line is logarithmic.)"
  },
  {
    "objectID": "specification24.html#log-models",
    "href": "specification24.html#log-models",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)"
  },
  {
    "objectID": "specification24.html#polynomials",
    "href": "specification24.html#polynomials",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24.html#scaling-redux",
    "href": "specification24.html#scaling-redux",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "adding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24.html#collinearity",
    "href": "specification24.html#collinearity",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "We know perfect collinearity is not much of a problem since we can’t estimate \\(\\widehat{\\beta}\\) at all in its presence - the matrix \\((X'X)^{-1}\\) is singular and thus can’t be inverted.\nWhat about correlation among \\(X\\) variables short of perfect collinearity?\n\n\nRecall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE.\n\n\n\n\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24.html#collinearity-1",
    "href": "specification24.html#collinearity-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Recall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE."
  },
  {
    "objectID": "specification24.html#collinearity-is-a-data-problem",
    "href": "specification24.html#collinearity-is-a-data-problem",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "if the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24.html#missing-data",
    "href": "specification24.html#missing-data",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Data is never missing randomly.\nThe process generating missing data is systematic.\n\n\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\).\n\n\n\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates.\n\n\n\n\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24.html#missing-data-signifies-sample-problems",
    "href": "specification24.html#missing-data-signifies-sample-problems",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "If data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\)."
  },
  {
    "objectID": "specification24.html#dealing-with-missingness",
    "href": "specification24.html#dealing-with-missingness",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "This is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates."
  },
  {
    "objectID": "specification24.html#missingness",
    "href": "specification24.html#missingness",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "All this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24.html#dissertations",
    "href": "specification24.html#dissertations",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Fox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#fit-r2_adj-2",
    "href": "specification24.html#fit-r2_adj-2",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Because \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\ninference on \\(R^2_{adj}\\) is possible assuming it is a sample statistic estimating a population \\(R^2_{adj}\\). There are significant limits to what we can tell using this though."
  },
  {
    "objectID": "specification24s.html#model-fit",
    "href": "specification24s.html#model-fit",
    "title": "Model Specification and Fit",
    "section": "Model Fit",
    "text": "Model Fit\nWe’re trying to explain variation. What does this look like?\n\nbest guess at variation in \\(y\\) would be its mean (constant-only model). with variables, variation is composed of two parts: \\(y = \\mu + \\epsilon\\). The first is \\(x\\beta\\), our systematic component; the second is the random component. \\(x\\) shapes the systematic part of \\(y\\).\ntotal variation in \\(y\\) is the sum of the variance of these components: \\(\\text{var}(y) = \\text{var}(x)\\beta + \\text{var}(e)\\)\nthe squared variation in \\(y\\) due to \\(x\\) is the MSS.\nthe squared variation in \\(e\\) is the RSS.\ntheir sum is the TSS."
  },
  {
    "objectID": "specification24s.html#model-fit-1",
    "href": "specification24s.html#model-fit-1",
    "title": "Model Specification and Fit",
    "section": "Model fit",
    "text": "Model fit\nRecall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\). \\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\). \\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\)."
  },
  {
    "objectID": "specification24s.html#model-fit-r2",
    "href": "specification24s.html#model-fit-r2",
    "title": "Model Specification and Fit",
    "section": "Model Fit – \\(R^2\\)",
    "text": "Model Fit – \\(R^2\\)\n\\[TSS= MSS+RSS\\]\nand\n\\[R^2 = \\frac{MSS}{TSS}\\] or\n\\[R^2 = 1- \\frac{RSS}{TSS}\\]"
  },
  {
    "objectID": "specification24s.html#fit-r2_adj",
    "href": "specification24s.html#fit-r2_adj",
    "title": "Model Specification and Fit",
    "section": "Fit – \\(R^2_{adj}\\)",
    "text": "Fit – \\(R^2_{adj}\\)\n\\(R^2\\) increases as \\(k\\) increases. The adjusted \\(R^2\\) penalizes for additional regressors (though still increases with \\(k\\)):\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(N-1)}{(N-k-1)} \\]\nin the numerator, adjusting for \\(N\\) minus one for the constant, and in the denominator for \\(N-k-1\\).\n\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight\n\n\n\n\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\nbecause \\(R^2\\) does not have a sampling distribution, we cannot generally say an \\(R^2\\) value is large or small - relative to what? If, say, it were distributed normally, we could make claims about the probability of observing a given \\(R^2\\) value.\nit is possible to construct such a sampling distribution, but it seems noone ever does, so the value of the \\(R^2\\) is not that great."
  },
  {
    "objectID": "specification24s.html#fit-r2_adj-1",
    "href": "specification24s.html#fit-r2_adj-1",
    "title": "Model Specification and Fit",
    "section": "Fit – \\(R^2_{adj}\\)",
    "text": "Fit – \\(R^2_{adj}\\)\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight"
  },
  {
    "objectID": "specification24s.html#fit-r2_adj-2",
    "href": "specification24s.html#fit-r2_adj-2",
    "title": "Model Specification and Fit",
    "section": "Fit – \\(R^2_{adj}\\)",
    "text": "Fit – \\(R^2_{adj}\\)\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\ninference on \\(R^2_{adj}\\) is possible assuming it is a sample statistic estimating a population \\(R^2_{adj}\\). There are significant limits to what we can tell using this though."
  },
  {
    "objectID": "specification24s.html#fit-rmse",
    "href": "specification24s.html#fit-rmse",
    "title": "Model Specification and Fit",
    "section": "Fit – RMSE",
    "text": "Fit – RMSE\nRoot Mean Squared Error (standard error of the estimate):\n\\[RMSE = \\sqrt{\\frac{RSS}{(N-k)}}\\]\nIs the average (squared) error. It’s useful because it’s in the same units as \\(y\\), so interpretable as the average (squared) distance of an observation from the regression line."
  },
  {
    "objectID": "specification24s.html#fit-f-test",
    "href": "specification24s.html#fit-f-test",
    "title": "Model Specification and Fit",
    "section": "Fit – F-test",
    "text": "Fit – F-test\nIn the linear model, the F-test is probably the most useful tool for comparing (nested) models. Reviewing, models are nested iff:\ntheir samples are identical. the variables in one are a strict subset of those in the other.\nThe nested models are the Restricted and Unrestricted."
  },
  {
    "objectID": "specification24s.html#fit-f-test-1",
    "href": "specification24s.html#fit-f-test-1",
    "title": "Model Specification and Fit",
    "section": "Fit – F-test",
    "text": "Fit – F-test\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or ``normed’’ by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\)."
  },
  {
    "objectID": "specification24s.html#outliers",
    "href": "specification24s.html#outliers",
    "title": "Model Specification and Fit",
    "section": "Outliers",
    "text": "Outliers\nHow much do individual observations shape the regression estimates? This discussion draws an excellent monograph by Fox (1991).\n\nleverage - how much an observation affects the slope of the regression line - it might have great leverage, but be on the line, so not discrepant.\ndiscrepancy - how unusual an observation is.\ninfluence - how much an unusual observation affects the slope of the regression line. How much a discrepant observation exerts leverage."
  },
  {
    "objectID": "specification24s.html#outliers-1",
    "href": "specification24s.html#outliers-1",
    "title": "Model Specification and Fit",
    "section": "Outliers",
    "text": "Outliers\nFox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\nOutliers - Ill-treatment and Torture Data}\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal()"
  },
  {
    "objectID": "specification24s.html#thinking-about-outliers",
    "href": "specification24s.html#thinking-about-outliers",
    "title": "Model Specification and Fit",
    "section": "Thinking about outliers",
    "text": "Thinking about outliers\n\nWhy are observations outliers?\nWhat should we do with them?"
  },
  {
    "objectID": "specification24s.html#model-outliers",
    "href": "specification24s.html#model-outliers",
    "title": "Model Specification and Fit",
    "section": "Model outliers",
    "text": "Model outliers\n\nwhat do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?"
  },
  {
    "objectID": "specification24s.html#functional-form",
    "href": "specification24s.html#functional-form",
    "title": "Model Specification and Fit",
    "section": "Functional Form",
    "text": "Functional Form\nTwo most common nonlinear functions applied to variables in political science are:\n\nnatural log\npolynomial\n\nNatural Logs\nPermit nonlinear but monotonic1 changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\nPolynomials\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)\n\nExample\nUsing data we’ve seen before, let’s look at the effect of GDP per capita on infant mortality rate. We’ll look at the effect of GDP per capita on infant mortality rate using a linear model, a log-linear model, and a polynomial model, compared below.\n\n\ncode\ncensor &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/data/censorcy.csv\", header=TRUE)\ncensor &lt;- subset(censor, select=-c(lngdp))\n\nggplot(data=censor, aes(x=gdppc, y=IMR)) +\n   geom_point(data=censor, aes(x=gdppc, y=IMR) ,color=\"green\") + \n   geom_text_repel(label=censor$ctry, size=3) +\n  stat_poly_line(data=censor, formula = y ~ x, se=FALSE, color=\"black\") +\n  stat_poly_line(data=censor, formula = y ~ log(x), se=FALSE, linetype=\"dashed\", color=\"red\" )   +\n  stat_poly_line(data=censor, formula = y ~ poly(x,2), se=FALSE, linetype=\"dashed\" )   +\n  labs(x=\"Polity\", y=\"Predicted Infant Mortality Rate\")+\n  theme_minimal()\n\n\n\n\n\nYou can see the linear model is a poor fit. The other two are better fits to the observed data (blue line is polynomial, red line is logarithmic.)\nA function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once."
  },
  {
    "objectID": "specification24s.html#log-models",
    "href": "specification24s.html#log-models",
    "title": "Model Specification and Fit",
    "section": "Log models",
    "text": "Log models\nModel interpretations using natural logs of variables (from Wooldridge, p. 44)"
  },
  {
    "objectID": "specification24s.html#polynomials",
    "href": "specification24s.html#polynomials",
    "title": "Model Specification and Fit",
    "section": "Polynomials",
    "text": "Polynomials\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24s.html#scaling-redux",
    "href": "specification24s.html#scaling-redux",
    "title": "Model Specification and Fit",
    "section": "Scaling redux",
    "text": "Scaling redux\n\nadding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24s.html#collinearity",
    "href": "specification24s.html#collinearity",
    "title": "Model Specification and Fit",
    "section": "Collinearity",
    "text": "Collinearity\nWe know perfect collinearity is not much of a problem since we can’t estimate \\(\\widehat{\\beta}\\) at all in its presence - the matrix \\((X'X)^{-1}\\) is singular and thus can’t be inverted.\nWhat about correlation among \\(X\\) variables short of perfect collinearity?\nCorrelation among \\(X\\)s\nRecall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE.\n\nCollinearity is a data problem\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24s.html#collinearity-1",
    "href": "specification24s.html#collinearity-1",
    "title": "Model Specification and Fit",
    "section": "Collinearity",
    "text": "Collinearity\n\nAll else equal, the model is still BLUE. The estimates are unbiased.\nthe estimates are less precise than they otherwise would be.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each."
  },
  {
    "objectID": "specification24s.html#collinearity-is-a-data-problem",
    "href": "specification24s.html#collinearity-is-a-data-problem",
    "title": "Model Specification and Fit",
    "section": "Collinearity is a data problem",
    "text": "Collinearity is a data problem\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24s.html#missing-data",
    "href": "specification24s.html#missing-data",
    "title": "Model Specification and Fit",
    "section": "Missing Data",
    "text": "Missing Data\nData is never missing randomly.\nThe process generating missing data is systematic.\nMissing data signifies sample problems\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\).\nDealing with Missingness\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates.\n\nMissingness\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24s.html#missing-data-signifies-sample-problems",
    "href": "specification24s.html#missing-data-signifies-sample-problems",
    "title": "Model Specification and Fit",
    "section": "Missing data signifies sample problems",
    "text": "Missing data signifies sample problems\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have $ $ effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\)."
  },
  {
    "objectID": "specification24s.html#dealing-with-missingness",
    "href": "specification24s.html#dealing-with-missingness",
    "title": "Model Specification and Fit",
    "section": "Dealing with Missingness",
    "text": "Dealing with Missingness\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates."
  },
  {
    "objectID": "specification24s.html#missingness",
    "href": "specification24s.html#missingness",
    "title": "Model Specification and Fit",
    "section": "Missingness",
    "text": "Missingness\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24s.html#model-specification-1",
    "href": "specification24s.html#model-specification-1",
    "title": "Model Specification and Fit",
    "section": "Model Specification",
    "text": "Model Specification"
  },
  {
    "objectID": "specification24s.html#dissertations",
    "href": "specification24s.html#dissertations",
    "title": "Model Specification and Fit",
    "section": "Dissertations",
    "text": "Dissertations"
  },
  {
    "objectID": "specification24s.html#references",
    "href": "specification24s.html#references",
    "title": "Model Specification and Fit",
    "section": "References",
    "text": "References\n\n\nFox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#references",
    "href": "specification24.html#references",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Fox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#scaling-variables",
    "href": "specification24.html#scaling-variables",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "adding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24.html#natural-logs",
    "href": "specification24.html#natural-logs",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Permit nonlinear but monotonic changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)"
  },
  {
    "objectID": "specification24s.html#scaling-variables",
    "href": "specification24s.html#scaling-variables",
    "title": "Model Specification and Fit",
    "section": "Scaling Variables",
    "text": "Scaling Variables\n\nadding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24s.html#a-comment-on-dissertations",
    "href": "specification24s.html#a-comment-on-dissertations",
    "title": "Model Specification and Fit",
    "section": "A comment on dissertations",
    "text": "A comment on dissertations"
  },
  {
    "objectID": "specification24.html#x-montonicity",
    "href": "specification24.html#x-montonicity",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "A function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once.\n\n\nPermit nonlinear but :monotonic changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\n\n\n\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24s.html#x-montonicity",
    "href": "specification24s.html#x-montonicity",
    "title": "Model Specification and Fit",
    "section": ":x montonicity",
    "text": ":x montonicity\nA function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once.\nNatural Logs\nPermit nonlinear but :monotonic changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\nPolynomials\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24.html#footnotes",
    "href": "specification24.html#footnotes",
    "title": "Model Specification and Fit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once.↩︎"
  },
  {
    "objectID": "interactions24.html",
    "href": "interactions24.html",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "The models we’ve considered so far have generally assumed structural stability - that the intercepts and slopes are shared by all groups in the data.\nWe’ll consider two possibilities:\n\ngroups in the data have different intercepts - test this with group indicator variables (dummies).\ngroups in the data have different slopes with respect to the same variables - test this using multiplicative interactions\n\n\n\n\nSuppose the following regression:\n\\[Y=\\beta_0+\\beta_1(D)+\\varepsilon \\]\nwhere \\(D\\) is a dummy variable, and \\(y\\) is a continuous variable.\n\n\n\nNow, consider the conditional expected values of \\(Y\\):\n\\[E[Y|D_1=0]= \\beta_0 = \\bar{Y} |D=0 \\nonumber  \\]\n\\[E[Y|D_1=1]= \\beta_0 + \\beta_1 = \\bar{Y}|D=1  \\]\nSo these are just the conditional means of \\(Y\\); the conditions are given by the values of \\(D\\) and represent the subsamples for \\(D=0\\) and \\(D=1\\).\n\n\n\nThe dummy variable estimate represents the differential intercept, or the difference in the mean of \\(Y|D=0\\) and the mean of \\(Y|D=1\\). The estimate \\(\\beta_1\\) is the difference in height on the \\(y\\) axis between the two groups in \\(D\\). The actual intercepts are:\n\\[Y|D=0 = \\beta_0\\]\n\\[Y|D=1 = \\beta_0+\\beta_1\\]\n\n\ncode\n## differential intercepts ----\nset.seed(12345)\ndata &lt;- tibble(\nX &lt;- rnorm_multi(1000, 3, \n                     mu=c(0, 5, 0), \n                     sd=c(1, 5, .5),\n                     r = c(0,0, 0.0),\n                     varnames=c(\"x1\", \"x2\", \"e\"))\n  ) %&gt;%\n    mutate(x2= ifelse(x2&gt;median(x2),1,0)) %&gt;%\n    mutate(y = 1 + 1*x1 + 2*x2 + e)\n    m1 &lt;- (lm(y ~ x1 + as.factor(x2), data=data))\n  \n### intercept differences only  ----\n  \ndata1 &lt;- data %&gt;% mutate(x1=0)  \npredint &lt;- data.frame(data, predict(m1, interval=\"confidence\", se.fit=TRUE, newdata = data1))\npredint &lt;- predint %&gt;% mutate(ub=fit.fit +1.96*se.fit, lb=fit.fit -1.96*se.fit) #absurd CI for illustration \n\nggplot(data=predint, aes(x=x2, y=fit.fit)) +\n  geom_point(size = 1) +\n  geom_pointrange(aes(ymin=lb, ymax=ub)) +\n  scale_x_continuous(breaks = c(0,1)) +\n  labs ( colour = NULL, x = \"x2\", y =  \"Predicted xb\" ) +\n  theme_minimal()+\n  ggtitle(\"Differential Intercepts\")  +\n  annotate(\"text\", x = .1, y = 1.5, label = \"B0\" , size=3.5, colour=\"gray30\") +\n  annotate(\"text\", x = .9, y = 3.3, label = \"B0 + B1\", size=3.5, colour=\"gray30\") \n\n\n\n\n\n\n\n\nNote this logic extends to multiple indicator variables where \\(\\beta_0\\) measures the intercept for the excluded category (all indicators set to zero), and \\(\\beta_0+\\beta_1\\) measures the intercept for any other category where its indicator is set to one, all others to zero.\n\n\n\nThinking of the Ill-Treatment models we’ve been working with:\n\\[ Y=\\beta_0+\\beta_1(D_{1})+\\beta_{2}(D_{2}) + \\ldots + \\varepsilon \\]\nwhere \\(D_{1}=1\\) indicates a civil war, and \\(D_{2}=1\\) indicates the government restricts IGO access:\n\n\\(E[Y]\\) for a non civil war, no restriction country is \\(\\beta_{0}\\).\n\\(E[Y]\\) for a civil war state with no restrictions is \\(\\beta_{0}+\\beta_{1}\\)\n\\(E[Y]\\) for a non civil war state with restricted access is \\(\\beta_{0}+\\beta_{2}\\)\n\\(E[Y]\\) for a civil war state that restricts access is \\(\\beta_{0}+\\beta_{1}+\\beta_{2}\\)\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1) %&gt;%\n  mutate(interact= civilwar*lagRA)\n\n\nm3 &lt;- lm(scarring ~ lagRA + civilwar  + lagprotest + polity2 +wdi_gdpc+wdi_pop, data=itt)\n# summary(m1)\n# average predictions, end point boundaries\n# estimation sample\nitt$used &lt;- TRUE\nitt$used[na.action(m3)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\n#across 4 combos of RA and CW\n\npredictions &lt;- data.frame(case=seq(1,4,1), xb=0, se=0, ub=0, lb=0, model=0 )\n\navg &lt;- ittesample %&gt;% mutate(civilwar=0, lagRA=0 )\nall0 &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[1:1,2:3]&lt;- data.frame(xb=median(all0$fit.fit, na.rm = TRUE), seall0=median(all0$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=1, lagRA=0 )\ncw &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[2:2,2:3]&lt;- data.frame(xb=median(cw$fit.fit, na.rm = TRUE), seall0=median(cw$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=0, lagRA=1 )\nra &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[3:3,2:3]&lt;- data.frame(xb=median(ra$fit.fit, na.rm = TRUE), seall0=median(ra$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=1, lagRA=1 )\nall1 &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[4:4,2:3]&lt;- data.frame(xb=median(all1$fit.fit, na.rm = TRUE), seall0=median(all1$se.fit,na.rm = TRUE))\n\npredictions &lt;- predictions %&gt;% mutate(ub=xb +1.96*se , lb =xb -1.96*se)\n\n\n#plot\nggplot(data=predictions, aes(x=case, y=xb)) +\n  geom_pointrange(data=predictions, aes(ymin=lb, ymax=ub)) +\n  labs ( colour = NULL, x = \"\", y =  \"Expected Scarring Torture Reports\" ) +\n  guides(x=\"none\")+\n  annotate(\"text\", x = 1.6, y = 5, label = \"No restriction, no civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 2, y = 9.5, label = \"No restriction, civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 3, y = 13, label = \"Restriction, no civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 3.2, y = 22, label = \"Restriction and civil war\", size=3.5, colour=\"gray30\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s take this notion of differential intercepts one final step. Suppose we estimate a model of Ill-Treatment and include a dummy variable for each country (minus one for the excluded category).\n\n\ncode\n# fixed effects ----\nitt$c &lt;- as.factor(itt$ctryname)\nitt &lt;- itt %&gt;% filter(!is.na(c))\n# set ref category to US\n# itt &lt;- within(itt, cname &lt;- relevel(cname, ref = \"United States of America\"))\n\n# be sure NA and \"\" are not categories in factor var cname\nm4 &lt;- lm(scarring ~ c, data=itt%&gt;%filter(c!=\"\"))\n#modelsummary(m4)\n\nlibrary(stargazer)\nstargazer(m4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),  column.labels=c(\"Fixed Effects\"),  dep.var.caption=\"Dependent Variable: Scarring Torture\", dep.var.labels.include=FALSE, notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Scarring Torture\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\n\n\n\ncAlbania\n\n\n4.717 (3.487)\n\n\n\n\ncAlgeria\n\n\n-3.283 (3.487)\n\n\n\n\ncAngola\n\n\n6.444 (3.487)\n\n\n\n\ncArgentina\n\n\n3.990 (3.487)\n\n\n\n\ncArmenia\n\n\n4.644 (3.565)\n\n\n\n\ncAustralia\n\n\n-4.156 (3.565)\n\n\n\n\ncAustria\n\n\n-0.556 (3.565)\n\n\n\n\ncAzerbaijan\n\n\n3.263 (3.487)\n\n\n\n\ncBangladesh\n\n\n1.717 (3.487)\n\n\n\n\ncBelarus\n\n\n0.990 (3.487)\n\n\n\n\ncBelgium\n\n\n1.172 (3.487)\n\n\n\n\ncBenin\n\n\n-4.556 (6.065)\n\n\n\n\ncBolivia\n\n\n0.778 (3.658)\n\n\n\n\ncBosnia and Herzegovina\n\n\n-4.306 (4.662)\n\n\n\n\ncBrazil\n\n\n18.626*** (3.487)\n\n\n\n\ncBulgaria\n\n\n9.626** (3.487)\n\n\n\n\ncBurkina Faso\n\n\n-3.413 (3.910)\n\n\n\n\ncBurundi\n\n\n1.444 (3.487)\n\n\n\n\ncCambodia\n\n\n-3.656 (3.565)\n\n\n\n\ncCameroon\n\n\n-0.456 (3.565)\n\n\n\n\ncCanada\n\n\n-4.181 (3.770)\n\n\n\n\ncCentral African Republic\n\n\n5.778 (5.173)\n\n\n\n\ncChad\n\n\n-0.856 (3.565)\n\n\n\n\ncChile\n\n\n-0.856 (3.565)\n\n\n\n\ncChina\n\n\n28.354*** (3.487)\n\n\n\n\ncColombia\n\n\n5.172 (3.487)\n\n\n\n\ncCongo (Brazzaville, Republic of Congo)\n\n\n-2.356 (3.565)\n\n\n\n\ncCosta Rica\n\n\n-3.889 (5.173)\n\n\n\n\ncCote d’Ivoire\n\n\n-2.374 (3.487)\n\n\n\n\ncCroatia\n\n\n0.144 (3.565)\n\n\n\n\ncCuba\n\n\n2.354 (3.487)\n\n\n\n\ncCzech Republic\n\n\n-0.756 (3.565)\n\n\n\n\ncDemocratic Republic of the Congo (Zaire, Congo-Kinshasha)\n\n\n6.778 (4.089)\n\n\n\n\ncDenmark\n\n\n-5.056 (3.565)\n\n\n\n\ncDominican Republic\n\n\n-4.000 (3.658)\n\n\n\n\ncEast Timor\n\n\n-2.556 (5.173)\n\n\n\n\ncEcuador\n\n\n-0.828 (3.487)\n\n\n\n\ncEgypt\n\n\n5.354 (3.487)\n\n\n\n\ncEl Salvador\n\n\n-4.222 (4.089)\n\n\n\n\ncEritrea\n\n\n-3.681 (3.770)\n\n\n\n\ncEstonia\n\n\n-4.806 (4.662)\n\n\n\n\ncEthiopia\n\n\n-3.181 (3.770)\n\n\n\n\ncFinland\n\n\n-5.556 (5.173)\n\n\n\n\ncFrance\n\n\n2.535 (3.487)\n\n\n\n\ncGabon\n\n\n-4.889 (5.173)\n\n\n\n\ncGambia\n\n\n-3.444 (3.658)\n\n\n\n\ncGeorgia\n\n\n4.990 (3.487)\n\n\n\n\ncGermany\n\n\n9.172** (3.487)\n\n\n\n\ncGhana\n\n\n-3.270 (3.910)\n\n\n\n\ncGreece\n\n\n4.899 (3.487)\n\n\n\n\ncGuatemala\n\n\n-3.283 (3.487)\n\n\n\n\ncGuinea\n\n\n-1.556 (3.910)\n\n\n\n\ncGuinea-Bissau\n\n\n1.844 (4.328)\n\n\n\n\ncGuyana\n\n\n-3.681 (3.770)\n\n\n\n\ncHaiti\n\n\n-1.556 (3.658)\n\n\n\n\ncHonduras\n\n\n-3.856 (3.565)\n\n\n\n\ncHungary\n\n\n2.744 (3.565)\n\n\n\n\ncIndia\n\n\n4.899 (3.487)\n\n\n\n\ncIndonesia\n\n\n27.263*** (3.487)\n\n\n\n\ncIran\n\n\n0.535 (3.487)\n\n\n\n\ncIraq\n\n\n-2.756 (3.565)\n\n\n\n\ncIreland\n\n\n-4.556 (6.065)\n\n\n\n\ncIsrael\n\n\n5.444 (3.487)\n\n\n\n\ncItaly\n\n\n11.172** (3.487)\n\n\n\n\ncJamaica\n\n\n-1.919 (3.487)\n\n\n\n\ncJapan\n\n\n-3.646 (3.487)\n\n\n\n\ncJordan\n\n\n-1.010 (3.487)\n\n\n\n\ncKazakhstan\n\n\n-1.646 (3.487)\n\n\n\n\ncKenya\n\n\n8.535* (3.487)\n\n\n\n\ncKuwait\n\n\n-5.306 (3.770)\n\n\n\n\ncKyrgyzstan\n\n\n-3.889 (3.658)\n\n\n\n\ncLaos\n\n\n-2.737 (3.487)\n\n\n\n\ncLatvia\n\n\n-4.889 (5.173)\n\n\n\n\ncLebanon\n\n\n0.244 (4.328)\n\n\n\n\ncLesotho\n\n\n-3.556 (3.770)\n\n\n\n\ncLiberia\n\n\n1.819 (3.770)\n\n\n\n\ncLibyan Arab Jamahiriya\n\n\n-3.374 (3.487)\n\n\n\n\ncLithuania\n\n\n-4.556 (6.065)\n\n\n\n\ncMacedonia\n\n\n4.778 (3.658)\n\n\n\n\ncMadagascar\n\n\n-0.556 (6.065)\n\n\n\n\ncMalawi\n\n\n-4.889 (4.089)\n\n\n\n\ncMalaysia\n\n\n0.899 (3.487)\n\n\n\n\ncMali\n\n\n-4.556 (8.178)\n\n\n\n\ncMauritania\n\n\n-2.919 (3.487)\n\n\n\n\ncMauritius\n\n\n-3.556 (4.662)\n\n\n\n\ncMexico\n\n\n15.626*** (3.487)\n\n\n\n\ncMoldova\n\n\n-3.101 (3.487)\n\n\n\n\ncMongolia\n\n\n-4.556 (6.065)\n\n\n\n\ncMorocco\n\n\n-1.737 (3.487)\n\n\n\n\ncMozambique\n\n\n-1.556 (3.565)\n\n\n\n\ncMyanmar\n\n\n15.626*** (3.487)\n\n\n\n\ncNamibia\n\n\n-1.698 (3.910)\n\n\n\n\ncNepal\n\n\n7.263* (3.487)\n\n\n\n\ncNetherlands\n\n\n-4.556 (4.328)\n\n\n\n\ncNicaragua\n\n\n-4.389 (4.089)\n\n\n\n\ncNiger\n\n\n-2.056 (4.662)\n\n\n\n\ncNigeria\n\n\n1.717 (3.487)\n\n\n\n\ncNorth Korea\n\n\n-3.101 (3.487)\n\n\n\n\ncNorway\n\n\n-5.556 (6.065)\n\n\n\n\ncOman\n\n\n-4.756 (4.328)\n\n\n\n\ncPakistan\n\n\n1.081 (3.487)\n\n\n\n\ncPanama\n\n\n-5.056 (4.662)\n\n\n\n\ncPapua New Guinea\n\n\n-2.667 (3.658)\n\n\n\n\ncParaguay\n\n\n-2.465 (3.487)\n\n\n\n\ncPeru\n\n\n1.263 (3.487)\n\n\n\n\ncPhilippines\n\n\n2.808 (3.487)\n\n\n\n\ncPoland\n\n\n-3.756 (3.565)\n\n\n\n\ncPortugal\n\n\n4.354 (3.487)\n\n\n\n\ncQatar\n\n\n-4.931 (3.770)\n\n\n\n\ncRomania\n\n\n12.844*** (3.565)\n\n\n\n\ncRussian Federation\n\n\n23.717*** (3.487)\n\n\n\n\ncRwanda\n\n\n6.808 (3.487)\n\n\n\n\ncSaudi Arabia\n\n\n-0.828 (3.487)\n\n\n\n\ncSenegal\n\n\n2.444 (3.658)\n\n\n\n\ncSerbia and Montenegro\n\n\n-5.556 (8.178)\n\n\n\n\ncSierra Leone\n\n\n-1.841 (3.910)\n\n\n\n\ncSingapore\n\n\n-5.056 (4.662)\n\n\n\n\ncSlovakia\n\n\n0.111 (3.658)\n\n\n\n\ncSlovenia\n\n\n-3.956 (4.328)\n\n\n\n\ncSouth Africa\n\n\n-0.010 (3.487)\n\n\n\n\ncSouth Korea\n\n\n-2.222 (3.658)\n\n\n\n\ncSpain\n\n\n7.172* (3.487)\n\n\n\n\ncSudan\n\n\n10.354** (3.487)\n\n\n\n\ncSuriname\n\n\n-4.556 (8.178)\n\n\n\n\ncSwaziland\n\n\n-2.181 (3.770)\n\n\n\n\ncSweden\n\n\n-3.465 (3.487)\n\n\n\n\ncSwitzerland\n\n\n3.944 (3.565)\n\n\n\n\ncSyria\n\n\n5.808 (3.487)\n\n\n\n\ncTajikistan\n\n\n-2.101 (3.487)\n\n\n\n\ncTanzania\n\n\n1.544 (3.565)\n\n\n\n\ncThailand\n\n\n-3.756 (3.565)\n\n\n\n\ncTogo\n\n\n1.626 (3.487)\n\n\n\n\ncTrinidad and Tobago\n\n\n-2.306 (3.770)\n\n\n\n\ncTunisia\n\n\n3.081 (3.487)\n\n\n\n\ncTurkey\n\n\n54.172*** (3.487)\n\n\n\n\ncTurkmenistan\n\n\n1.081 (3.487)\n\n\n\n\ncUganda\n\n\n-0.556 (3.487)\n\n\n\n\ncUkraine\n\n\n-0.737 (3.487)\n\n\n\n\ncUnited Arab Emirates\n\n\n-4.681 (3.770)\n\n\n\n\ncUnited Kingdom\n\n\n2.263 (3.487)\n\n\n\n\ncUnited States of America\n\n\n18.899*** (3.487)\n\n\n\n\ncUruguay\n\n\n-5.222 (3.658)\n\n\n\n\ncUzbekistan\n\n\n14.354*** (3.487)\n\n\n\n\ncVenezuela\n\n\n2.944 (3.565)\n\n\n\n\ncViet Nam\n\n\n-4.556 (3.487)\n\n\n\n\ncYemen\n\n\n-1.465 (3.487)\n\n\n\n\ncZambia\n\n\n-1.828 (3.487)\n\n\n\n\ncZimbabwe\n\n\n3.717 (3.487)\n\n\n\n\nConstant\n\n\n5.556* (2.586)\n\n\n\n\n\n\n\n\nObservations\n\n\n1,317\n\n\n\n\nR2\n\n\n0.550\n\n\n\n\nAdjusted R2\n\n\n0.493\n\n\n\n\nF Statistic\n\n\n9.648*** (df = 148; 1168)\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\n\n\nHere’s a plot of the fixed effect coefficients - each is a differential intercept, measuring the difference from the reference category.\n\n\ncode\n# reference category is AFG by default \n# coefficients as data frame for plotting\ncoefs&lt;- data.frame(coef(summary(m4)))\ncoefs$country &lt;- rownames(coefs)\ncoefs$country &lt;- substr(coefs$country, 2, nchar(coefs$country))\ncoefs$country[coefs$country==\"Intercept)\"] &lt;- \"Intercept\"\ncoefs$sig &lt;-ifelse(coefs$`Pr...t..`&lt;.05, 1,0)\n\n\n## plot fixed effects ----\n\np &lt;- ggplot(coefs, aes(x = Estimate, y = country, color = factor(sig), label = country)) +\n  geom_point(size = 1) +\n  geom_errorbarh(aes(xmin = Estimate - 1.96 * Std..Error, xmax = Estimate + 1.96 * Std..Error)) +\n  geom_text_repel(data=coefs%&gt;%filter(sig==1),  size=2.5, force=5) + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs ( colour = NULL, y = \"\", x =  \"Expected Scarring Torture Reports\" ) +\n  ggtitle(\"Fixed Effects\", subtitle = \"Country Coefficients\")\n\np + scale_color_manual(values = c(\"red\", \"black\")) +\n  guides(color=\"none\")\n\n\n\n\n\nThe result is that each coefficient is an intercept for a particular country. Each measures the difference in that country’s intercept (or mean of \\(y\\)) from the excluded category, in this case, Afghanistan. Black bars are different from the excluded category at the .05 level.\nEach country coefficient plus the intercept is that country’s mean of \\(y\\), scarring torture . Looking, for instance at the US, the coefficient is about 19, so the US mean scarring torture reports is 19 plus the intercept (5.5), so about 24.5 (the actual mean for US scarring torture reports is 24.45).\n\n\n\nSuppose we have a regression with a dummy variable and a continuous variable:\n\\[y = \\beta_0 + \\beta_1 d_1 + \\beta_2 x_2 \\] The expected value of \\(y\\) for each group is:\n\\[E[y | d_1= 0] = \\beta_0 +  \\beta_2 x_2 \\]\n\\[E[y | d_1= 0] = \\beta_0 + \\beta_1 +  \\beta_2 x_2 \\]\nNote the shift in intercept.\n\n\ncode\nggplot(data=filter(data, x2==0), aes(x=x1, y=y)) +\n  geom_smooth(method=\"lm\", se=FALSE, color=\"black\") +\n  geom_smooth(data=filter(data, x2==1), aes(x=x1, y=y), method=\"lm\", se=FALSE, color=\"black\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = 0, y = 0, label = \"x2=0\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 4, label = \"x2=1\", size=3.5, colour=\"gray30\")+\n  ggtitle(\"Differential Intercepts, Continuous x1\")\n\n\n\n\n\nThe difference is only in intercept on \\(d\\), \\(\\beta_{1}\\) - the slope estimate for \\(x_2\\) is the same no matter the value of \\(d_{1}\\).\nPut differently, these two groups given by the dummy variable share the same slope, but have different y-intercepts.\n\n\n\n\n\n\nStructural Stability\n\n\n\nIn this regression, the slope on \\(x_2\\) is the same for all groups measured by indicator variables.\n\\[y=\\beta_0 + \\beta_1 d_1 +\\beta_2x_1+ \\varepsilon \\]\nAssuming a common slope is known as the structural stability assumption.\n\n\n\n\n\n\nDummy variables are useful for measuring differences between groups.\nDummy coefficients specifically measure the differences in levels (intercepts) between groups.\nDummies may capture differences between known, discrete groups, e.g. genders, parties, races, etc.\nDummies might capture unknown differences between units, say between states or countries - this is the foundation of fixed effects."
  },
  {
    "objectID": "interactions24.html#indicator-variables",
    "href": "interactions24.html#indicator-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Suppose the following regression:\n\\[Y=\\beta_0+\\beta_1(D)+\\varepsilon \\]\nwhere \\(D\\) is a dummy variable, and \\(y\\) is a continuous variable."
  },
  {
    "objectID": "interactions24.html#indicator-variables-1",
    "href": "interactions24.html#indicator-variables-1",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Now, consider the conditional expected values of \\(Y\\):\n\\[E[Y|D_1=0]= \\beta_0 = \\bar{Y} |D=0 \\nonumber  \\]\n\\[E[Y|D_1=1]= \\beta_0 + \\beta_1 = \\bar{Y}|D=1  \\]\nSo these are just the conditional means of \\(Y\\); the conditions are given by the values of \\(D\\) and represent the subsamples for \\(D=0\\) and \\(D=1\\)."
  },
  {
    "objectID": "interactions24.html#differential-intercept",
    "href": "interactions24.html#differential-intercept",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "The dummy variable estimate represents the differential intercept, or the difference in the mean of \\(Y|D=0\\) and the mean of \\(Y|D=1\\). The estimate \\(\\beta_1\\) is the difference in height on the \\(y\\) axis between the two groups in \\(D\\). The actual intercepts are:\n\\[Y|D=0 = \\beta_0\\]\n\\[Y|D=1 = \\beta_0+\\beta_1\\]\n\n\ncode\n## differential intercepts ----\nset.seed(12345)\ndata &lt;- tibble(\nX &lt;- rnorm_multi(1000, 3, \n                     mu=c(0, 5, 0), \n                     sd=c(1, 5, .5),\n                     r = c(0,0, 0.0),\n                     varnames=c(\"x1\", \"x2\", \"e\"))\n  ) %&gt;%\n    mutate(x2= ifelse(x2&gt;median(x2),1,0)) %&gt;%\n    mutate(y = 1 + 1*x1 + 2*x2 + e)\n    m1 &lt;- (lm(y ~ x1 + as.factor(x2), data=data))\n  \n### intercept differences only  ----\n  \ndata1 &lt;- data %&gt;% mutate(x1=0)  \npredint &lt;- data.frame(data, predict(m1, interval=\"confidence\", se.fit=TRUE, newdata = data1))\npredint &lt;- predint %&gt;% mutate(ub=fit.fit +1.96*se.fit, lb=fit.fit -1.96*se.fit) #absurd CI for illustration \n\nggplot(data=predint, aes(x=x2, y=fit.fit)) +\n  geom_point(size = 1) +\n  geom_pointrange(aes(ymin=lb, ymax=ub)) +\n  scale_x_continuous(breaks = c(0,1)) +\n  labs ( colour = NULL, x = \"x2\", y =  \"Predicted xb\" ) +\n  theme_minimal()+\n  ggtitle(\"Differential Intercepts\")  +\n  annotate(\"text\", x = .1, y = 1.5, label = \"B0\" , size=3.5, colour=\"gray30\") +\n  annotate(\"text\", x = .9, y = 3.3, label = \"B0 + B1\", size=3.5, colour=\"gray30\")"
  },
  {
    "objectID": "interactions24.html#differential-intercept-1",
    "href": "interactions24.html#differential-intercept-1",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Note this logic extends to multiple indicator variables where \\(\\beta_0\\) measures the intercept for the excluded category (all indicators set to zero), and \\(\\beta_0+\\beta_1\\) measures the intercept for any other category where its indicator is set to one, all others to zero."
  },
  {
    "objectID": "interactions24.html#multiple-indicator-variables",
    "href": "interactions24.html#multiple-indicator-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Thinking of the Ill-Treatment models we’ve been working with:\n\\[ Y=\\beta_0+\\beta_1(D_{1})+\\beta_{2}(D_{2}) + \\ldots + \\varepsilon \\]\nwhere \\(D_{1}=1\\) indicates a civil war, and \\(D_{2}=1\\) indicates the government restricts IGO access:\n\n\\(E[Y]\\) for a non civil war, no restriction country is \\(\\beta_{0}\\).\n\\(E[Y]\\) for a civil war state with no restrictions is \\(\\beta_{0}+\\beta_{1}\\)\n\\(E[Y]\\) for a non civil war state with restricted access is \\(\\beta_{0}+\\beta_{2}\\)\n\\(E[Y]\\) for a civil war state that restricts access is \\(\\beta_{0}+\\beta_{1}+\\beta_{2}\\)\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1) %&gt;%\n  mutate(interact= civilwar*lagRA)\n\n\nm3 &lt;- lm(scarring ~ lagRA + civilwar  + lagprotest + polity2 +wdi_gdpc+wdi_pop, data=itt)\n# summary(m1)\n# average predictions, end point boundaries\n# estimation sample\nitt$used &lt;- TRUE\nitt$used[na.action(m3)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\n#across 4 combos of RA and CW\n\npredictions &lt;- data.frame(case=seq(1,4,1), xb=0, se=0, ub=0, lb=0, model=0 )\n\navg &lt;- ittesample %&gt;% mutate(civilwar=0, lagRA=0 )\nall0 &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[1:1,2:3]&lt;- data.frame(xb=median(all0$fit.fit, na.rm = TRUE), seall0=median(all0$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=1, lagRA=0 )\ncw &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[2:2,2:3]&lt;- data.frame(xb=median(cw$fit.fit, na.rm = TRUE), seall0=median(cw$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=0, lagRA=1 )\nra &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[3:3,2:3]&lt;- data.frame(xb=median(ra$fit.fit, na.rm = TRUE), seall0=median(ra$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=1, lagRA=1 )\nall1 &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[4:4,2:3]&lt;- data.frame(xb=median(all1$fit.fit, na.rm = TRUE), seall0=median(all1$se.fit,na.rm = TRUE))\n\npredictions &lt;- predictions %&gt;% mutate(ub=xb +1.96*se , lb =xb -1.96*se)\n\n\n#plot\nggplot(data=predictions, aes(x=case, y=xb)) +\n  geom_pointrange(data=predictions, aes(ymin=lb, ymax=ub)) +\n  labs ( colour = NULL, x = \"\", y =  \"Expected Scarring Torture Reports\" ) +\n  guides(x=\"none\")+\n  annotate(\"text\", x = 1.6, y = 5, label = \"No restriction, no civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 2, y = 9.5, label = \"No restriction, civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 3, y = 13, label = \"Restriction, no civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 3.2, y = 22, label = \"Restriction and civil war\", size=3.5, colour=\"gray30\")+\n  theme_minimal()"
  },
  {
    "objectID": "interactions24.html#fixed-effects",
    "href": "interactions24.html#fixed-effects",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Let’s take this notion of differential intercepts one final step. Suppose we estimate a model of Ill-Treatment and include a dummy variable for each country (minus one for the excluded category).\n\n\ncode\n# fixed effects ----\nitt$c &lt;- as.factor(itt$ctryname)\nitt &lt;- itt %&gt;% filter(!is.na(c))\n# set ref category to US\n# itt &lt;- within(itt, cname &lt;- relevel(cname, ref = \"United States of America\"))\n\n# be sure NA and \"\" are not categories in factor var cname\nm4 &lt;- lm(scarring ~ c, data=itt%&gt;%filter(c!=\"\"))\n#modelsummary(m4)\n\nlibrary(stargazer)\nstargazer(m4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),  column.labels=c(\"Fixed Effects\"),  dep.var.caption=\"Dependent Variable: Scarring Torture\", dep.var.labels.include=FALSE, notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Scarring Torture\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\n\n\n\ncAlbania\n\n\n4.717 (3.487)\n\n\n\n\ncAlgeria\n\n\n-3.283 (3.487)\n\n\n\n\ncAngola\n\n\n6.444 (3.487)\n\n\n\n\ncArgentina\n\n\n3.990 (3.487)\n\n\n\n\ncArmenia\n\n\n4.644 (3.565)\n\n\n\n\ncAustralia\n\n\n-4.156 (3.565)\n\n\n\n\ncAustria\n\n\n-0.556 (3.565)\n\n\n\n\ncAzerbaijan\n\n\n3.263 (3.487)\n\n\n\n\ncBangladesh\n\n\n1.717 (3.487)\n\n\n\n\ncBelarus\n\n\n0.990 (3.487)\n\n\n\n\ncBelgium\n\n\n1.172 (3.487)\n\n\n\n\ncBenin\n\n\n-4.556 (6.065)\n\n\n\n\ncBolivia\n\n\n0.778 (3.658)\n\n\n\n\ncBosnia and Herzegovina\n\n\n-4.306 (4.662)\n\n\n\n\ncBrazil\n\n\n18.626*** (3.487)\n\n\n\n\ncBulgaria\n\n\n9.626** (3.487)\n\n\n\n\ncBurkina Faso\n\n\n-3.413 (3.910)\n\n\n\n\ncBurundi\n\n\n1.444 (3.487)\n\n\n\n\ncCambodia\n\n\n-3.656 (3.565)\n\n\n\n\ncCameroon\n\n\n-0.456 (3.565)\n\n\n\n\ncCanada\n\n\n-4.181 (3.770)\n\n\n\n\ncCentral African Republic\n\n\n5.778 (5.173)\n\n\n\n\ncChad\n\n\n-0.856 (3.565)\n\n\n\n\ncChile\n\n\n-0.856 (3.565)\n\n\n\n\ncChina\n\n\n28.354*** (3.487)\n\n\n\n\ncColombia\n\n\n5.172 (3.487)\n\n\n\n\ncCongo (Brazzaville, Republic of Congo)\n\n\n-2.356 (3.565)\n\n\n\n\ncCosta Rica\n\n\n-3.889 (5.173)\n\n\n\n\ncCote d’Ivoire\n\n\n-2.374 (3.487)\n\n\n\n\ncCroatia\n\n\n0.144 (3.565)\n\n\n\n\ncCuba\n\n\n2.354 (3.487)\n\n\n\n\ncCzech Republic\n\n\n-0.756 (3.565)\n\n\n\n\ncDemocratic Republic of the Congo (Zaire, Congo-Kinshasha)\n\n\n6.778 (4.089)\n\n\n\n\ncDenmark\n\n\n-5.056 (3.565)\n\n\n\n\ncDominican Republic\n\n\n-4.000 (3.658)\n\n\n\n\ncEast Timor\n\n\n-2.556 (5.173)\n\n\n\n\ncEcuador\n\n\n-0.828 (3.487)\n\n\n\n\ncEgypt\n\n\n5.354 (3.487)\n\n\n\n\ncEl Salvador\n\n\n-4.222 (4.089)\n\n\n\n\ncEritrea\n\n\n-3.681 (3.770)\n\n\n\n\ncEstonia\n\n\n-4.806 (4.662)\n\n\n\n\ncEthiopia\n\n\n-3.181 (3.770)\n\n\n\n\ncFinland\n\n\n-5.556 (5.173)\n\n\n\n\ncFrance\n\n\n2.535 (3.487)\n\n\n\n\ncGabon\n\n\n-4.889 (5.173)\n\n\n\n\ncGambia\n\n\n-3.444 (3.658)\n\n\n\n\ncGeorgia\n\n\n4.990 (3.487)\n\n\n\n\ncGermany\n\n\n9.172** (3.487)\n\n\n\n\ncGhana\n\n\n-3.270 (3.910)\n\n\n\n\ncGreece\n\n\n4.899 (3.487)\n\n\n\n\ncGuatemala\n\n\n-3.283 (3.487)\n\n\n\n\ncGuinea\n\n\n-1.556 (3.910)\n\n\n\n\ncGuinea-Bissau\n\n\n1.844 (4.328)\n\n\n\n\ncGuyana\n\n\n-3.681 (3.770)\n\n\n\n\ncHaiti\n\n\n-1.556 (3.658)\n\n\n\n\ncHonduras\n\n\n-3.856 (3.565)\n\n\n\n\ncHungary\n\n\n2.744 (3.565)\n\n\n\n\ncIndia\n\n\n4.899 (3.487)\n\n\n\n\ncIndonesia\n\n\n27.263*** (3.487)\n\n\n\n\ncIran\n\n\n0.535 (3.487)\n\n\n\n\ncIraq\n\n\n-2.756 (3.565)\n\n\n\n\ncIreland\n\n\n-4.556 (6.065)\n\n\n\n\ncIsrael\n\n\n5.444 (3.487)\n\n\n\n\ncItaly\n\n\n11.172** (3.487)\n\n\n\n\ncJamaica\n\n\n-1.919 (3.487)\n\n\n\n\ncJapan\n\n\n-3.646 (3.487)\n\n\n\n\ncJordan\n\n\n-1.010 (3.487)\n\n\n\n\ncKazakhstan\n\n\n-1.646 (3.487)\n\n\n\n\ncKenya\n\n\n8.535* (3.487)\n\n\n\n\ncKuwait\n\n\n-5.306 (3.770)\n\n\n\n\ncKyrgyzstan\n\n\n-3.889 (3.658)\n\n\n\n\ncLaos\n\n\n-2.737 (3.487)\n\n\n\n\ncLatvia\n\n\n-4.889 (5.173)\n\n\n\n\ncLebanon\n\n\n0.244 (4.328)\n\n\n\n\ncLesotho\n\n\n-3.556 (3.770)\n\n\n\n\ncLiberia\n\n\n1.819 (3.770)\n\n\n\n\ncLibyan Arab Jamahiriya\n\n\n-3.374 (3.487)\n\n\n\n\ncLithuania\n\n\n-4.556 (6.065)\n\n\n\n\ncMacedonia\n\n\n4.778 (3.658)\n\n\n\n\ncMadagascar\n\n\n-0.556 (6.065)\n\n\n\n\ncMalawi\n\n\n-4.889 (4.089)\n\n\n\n\ncMalaysia\n\n\n0.899 (3.487)\n\n\n\n\ncMali\n\n\n-4.556 (8.178)\n\n\n\n\ncMauritania\n\n\n-2.919 (3.487)\n\n\n\n\ncMauritius\n\n\n-3.556 (4.662)\n\n\n\n\ncMexico\n\n\n15.626*** (3.487)\n\n\n\n\ncMoldova\n\n\n-3.101 (3.487)\n\n\n\n\ncMongolia\n\n\n-4.556 (6.065)\n\n\n\n\ncMorocco\n\n\n-1.737 (3.487)\n\n\n\n\ncMozambique\n\n\n-1.556 (3.565)\n\n\n\n\ncMyanmar\n\n\n15.626*** (3.487)\n\n\n\n\ncNamibia\n\n\n-1.698 (3.910)\n\n\n\n\ncNepal\n\n\n7.263* (3.487)\n\n\n\n\ncNetherlands\n\n\n-4.556 (4.328)\n\n\n\n\ncNicaragua\n\n\n-4.389 (4.089)\n\n\n\n\ncNiger\n\n\n-2.056 (4.662)\n\n\n\n\ncNigeria\n\n\n1.717 (3.487)\n\n\n\n\ncNorth Korea\n\n\n-3.101 (3.487)\n\n\n\n\ncNorway\n\n\n-5.556 (6.065)\n\n\n\n\ncOman\n\n\n-4.756 (4.328)\n\n\n\n\ncPakistan\n\n\n1.081 (3.487)\n\n\n\n\ncPanama\n\n\n-5.056 (4.662)\n\n\n\n\ncPapua New Guinea\n\n\n-2.667 (3.658)\n\n\n\n\ncParaguay\n\n\n-2.465 (3.487)\n\n\n\n\ncPeru\n\n\n1.263 (3.487)\n\n\n\n\ncPhilippines\n\n\n2.808 (3.487)\n\n\n\n\ncPoland\n\n\n-3.756 (3.565)\n\n\n\n\ncPortugal\n\n\n4.354 (3.487)\n\n\n\n\ncQatar\n\n\n-4.931 (3.770)\n\n\n\n\ncRomania\n\n\n12.844*** (3.565)\n\n\n\n\ncRussian Federation\n\n\n23.717*** (3.487)\n\n\n\n\ncRwanda\n\n\n6.808 (3.487)\n\n\n\n\ncSaudi Arabia\n\n\n-0.828 (3.487)\n\n\n\n\ncSenegal\n\n\n2.444 (3.658)\n\n\n\n\ncSerbia and Montenegro\n\n\n-5.556 (8.178)\n\n\n\n\ncSierra Leone\n\n\n-1.841 (3.910)\n\n\n\n\ncSingapore\n\n\n-5.056 (4.662)\n\n\n\n\ncSlovakia\n\n\n0.111 (3.658)\n\n\n\n\ncSlovenia\n\n\n-3.956 (4.328)\n\n\n\n\ncSouth Africa\n\n\n-0.010 (3.487)\n\n\n\n\ncSouth Korea\n\n\n-2.222 (3.658)\n\n\n\n\ncSpain\n\n\n7.172* (3.487)\n\n\n\n\ncSudan\n\n\n10.354** (3.487)\n\n\n\n\ncSuriname\n\n\n-4.556 (8.178)\n\n\n\n\ncSwaziland\n\n\n-2.181 (3.770)\n\n\n\n\ncSweden\n\n\n-3.465 (3.487)\n\n\n\n\ncSwitzerland\n\n\n3.944 (3.565)\n\n\n\n\ncSyria\n\n\n5.808 (3.487)\n\n\n\n\ncTajikistan\n\n\n-2.101 (3.487)\n\n\n\n\ncTanzania\n\n\n1.544 (3.565)\n\n\n\n\ncThailand\n\n\n-3.756 (3.565)\n\n\n\n\ncTogo\n\n\n1.626 (3.487)\n\n\n\n\ncTrinidad and Tobago\n\n\n-2.306 (3.770)\n\n\n\n\ncTunisia\n\n\n3.081 (3.487)\n\n\n\n\ncTurkey\n\n\n54.172*** (3.487)\n\n\n\n\ncTurkmenistan\n\n\n1.081 (3.487)\n\n\n\n\ncUganda\n\n\n-0.556 (3.487)\n\n\n\n\ncUkraine\n\n\n-0.737 (3.487)\n\n\n\n\ncUnited Arab Emirates\n\n\n-4.681 (3.770)\n\n\n\n\ncUnited Kingdom\n\n\n2.263 (3.487)\n\n\n\n\ncUnited States of America\n\n\n18.899*** (3.487)\n\n\n\n\ncUruguay\n\n\n-5.222 (3.658)\n\n\n\n\ncUzbekistan\n\n\n14.354*** (3.487)\n\n\n\n\ncVenezuela\n\n\n2.944 (3.565)\n\n\n\n\ncViet Nam\n\n\n-4.556 (3.487)\n\n\n\n\ncYemen\n\n\n-1.465 (3.487)\n\n\n\n\ncZambia\n\n\n-1.828 (3.487)\n\n\n\n\ncZimbabwe\n\n\n3.717 (3.487)\n\n\n\n\nConstant\n\n\n5.556* (2.586)\n\n\n\n\n\n\n\n\nObservations\n\n\n1,317\n\n\n\n\nR2\n\n\n0.550\n\n\n\n\nAdjusted R2\n\n\n0.493\n\n\n\n\nF Statistic\n\n\n9.648*** (df = 148; 1168)\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05"
  },
  {
    "objectID": "interactions24.html#dummy-variables",
    "href": "interactions24.html#dummy-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Dummy variables are useful for measuring differences between groups.\nDummy coefficients specifically measure the differences in levels (intercepts) between groups.\nDummies may capture differences between known, discrete groups, e.g. genders, parties, races, etc.\nDummies might capture unknown differences between units, say between states or countries - this is the foundation of fixed effects."
  },
  {
    "objectID": "interactions24.html#structural-stability",
    "href": "interactions24.html#structural-stability",
    "title": "Differential Intercepts and Interactions",
    "section": "Structural Stability",
    "text": "Structural Stability\nIf this is the case, then the marginal effect of \\(x_2\\) is not \\(\\beta_2\\); instead, it is .5 for one group, and 1.7 for the other.\nRecall, the marginal effect of \\(x_2\\) in the model where we assume structural stability:\n\\[y = \\beta_0 + \\beta_1 d_1 + \\beta_2 x_2 \\]\nis \\(\\beta_2\\)."
  },
  {
    "objectID": "interactions24.html#structural-stability-1",
    "href": "interactions24.html#structural-stability-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Structural Stability",
    "text": "Structural Stability\nWe relax the structural stability assumption by modeling a multiplicative interaction:\n\\[y = \\beta_0 + \\beta_1 d_1 + \\beta_2 x_2 + \\beta_3 (d_1x_2)\\]\nliterally multiplying \\(d_1\\) and \\(x_2\\) together, and including that new variable in the model. \\(\\beta_3\\) measures the difference in slope between the two groups in \\(d_1\\)."
  },
  {
    "objectID": "interactions24.html#multiplicative-interactions-1",
    "href": "interactions24.html#multiplicative-interactions-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Multiplicative Interactions",
    "text": "Multiplicative Interactions\nSupposing the following model:\n\\[y = \\beta_0 + \\beta_1 d_1 + \\beta_2 x_2 + \\beta_3 (d_1x_2)\\]\nThe expected value of \\(y\\) for the case where \\(d_1=0\\) is:\n\\[E[y|x, d_1=0] = \\beta_0 + \\beta_2 x_2\\]\nWhere \\(d_1=1\\), the expected value of \\(y\\) is:\n\\[E[y|x; x|d_1=1] = \\beta_0+\\beta_1d_1 + \\beta_2 x_2  +\\beta_3 x_2d_1 \\]\nHere, the expected value of \\(y\\) depends on the intercept given by \\(\\beta_0\\) and \\(\\beta_1\\), and on slopes determined by \\(x_2\\), but those slopes differ depending on the value of \\(d_1\\).\n\n\n\n\n\n\nMarginal Effects\n\n\n\nAn important insight: the interaction coefficient, \\(\\beta_3\\), measures the difference in slopes between the groups represented by \\(d=0\\) and \\(d=1\\)."
  },
  {
    "objectID": "interactions24.html#multiplicative-interactions-2",
    "href": "interactions24.html#multiplicative-interactions-2",
    "title": "Differential Intercepts and Interactions",
    "section": "Multiplicative Interactions",
    "text": "Multiplicative Interactions\n\\[E[Y|x; x|D_1=1] = \\beta_{0}+\\beta_{1}(D_1)+\\beta_{2}(x_{1}) +\\beta_{3}x_1D_1 \\]\nHere, the expected value of \\(Y\\) depends on the intercept given by \\(\\beta_0\\) and \\(\\beta_1\\), \\alert{and on slopes determined by \\(x_1\\), but those slopes differ depending on the value of \\(D_1\\).\n\n\n\n\n\n\nMarginal Effects\n\n\n\nAn important insight: the interaction coefficient, \\(\\beta_3\\), measures the difference in slopes between \\(D=1\\) and \\(D=0\\)."
  },
  {
    "objectID": "interactions24.html#what-does-a-hypothesis-look-like",
    "href": "interactions24.html#what-does-a-hypothesis-look-like",
    "title": "Differential Intercepts and Interactions",
    "section": "What does a hypothesis look like?",
    "text": "What does a hypothesis look like?\nHypothesis: An increase in \\(x_2\\) will produce a larger or faster increase in \\(y\\) if \\(d_1\\) is present than when it is absent.\n\\[Y = \\beta_0+\\beta_1 d_1+ \\beta_2 x_2  +\\beta_3 x_2d_1 +\\varepsilon \\]\n\nHypothesis on Scarring Torture\nGiving this some substance, again thinking of the ITT data, we might hypothesize that:\nIncreases in protests will produce larger or faster increases in scarring torture in states that restrict IGO access than in states that do not."
  },
  {
    "objectID": "interactions24.html#marginal-effects",
    "href": "interactions24.html#marginal-effects",
    "title": "Differential Intercepts and Interactions",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nIn the interactive model, the marginal effect of \\(d_1\\) is:\n\\[\\frac{\\partial{y}}{\\partial{d_1}}= \\beta_1+\\beta_3*x_2 \\]\nNote this depends on the values of \\(x_2\\), allowing for the possibility the effect of \\(x_2\\) accelerates or slows.\nThe marginal effect of \\(x_1\\) is:\n\\[\\frac{\\partial{y}}{\\partial{x_2}}= \\beta_2+\\beta_3*d_1 \\]\nIt should make sense that if \\(d_1=0\\), the effect is just \\(\\beta_2\\). If \\(d_1=1\\), we adjust the slope by \\(\\beta_3\\), thereby allowing the slopes to be different for the two groups in \\(d_1\\), thereby relaxing structural stability.\n\n\ncode\navg &lt;- data %&gt;% mutate(x2=0, interaction=0)\nint0 &lt;- data.frame(x1=avg$x1, predict(m2, interval=\"confidence\", se.fit=TRUE, newdata = avg))\navg &lt;- data %&gt;% mutate(x2=1, interaction=x1) \nint1 &lt;- data.frame(x1=avg$x1, predict(m2, interval=\"confidence\", se.fit=TRUE, newdata = avg))\n\nallpred &lt;- data.frame(x1=int0$x1, fit0=int0$fit.fit, fit1=int1$fit.fit, me=int1$fit.fit-int0$fit.fit)\n\nggplot() +\n  geom_line(data=allpred, aes(x=x1, y=fit0), linetype=\"dashed\") +\n  geom_line(data=allpred, aes(x=x1, y=fit1), linetype=\"dashed\") +\n  geom_line(data=allpred, aes(x=x1, y=me), color=\"blue\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  annotate(\"text\", x = 0, y = -2, label = \"x2=0\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 5, label = \"x2=1\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 2.5, y = 6, label = \"marginal \\n\\n effect\", size=2.5, colour=\"gray30\")+\n  theme_minimal()\n\n\n\n\n\nIn this case, the marginal effect of \\(x_2\\) is the difference between the two lines. The dashed lines are the predicted values of \\(y\\) for \\(x_1\\) given \\(x_2=0\\) and \\(x_2=1\\). The solid line is the difference between the two, the marginal effect of \\(x_2\\) across the values of \\(x_1\\)"
  },
  {
    "objectID": "interactions24.html#marginal-effects-1",
    "href": "interactions24.html#marginal-effects-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\n\ncode\ndf &lt;- df %&gt;% mutate(protest=seq(1,40,1),\n  MEprotest=m4$coefficients[2]+m4$coefficients[10]*protest, se = sqrt(vcov(m4)[2,2]+vcov(m4)[10,10]*protest^2+2*vcov(m4)[10,2]*protest), ub=MEprotest+1.96*se, lb=MEprotest-1.96*se)\n\nggplot() +\n  geom_line(data=df, aes(x=protest, y=MEprotest) ) +\n  geom_ribbon(data=df, aes(x=protest, ymin=lb, ymax=ub),fill = \"grey70\", alpha = .4, ) +\n  labs( x = \"Protests\", y =  \"Marginal Effect of Access Restriction on Scarring Torture\")"
  },
  {
    "objectID": "interactions24.html#marginal-effects-2",
    "href": "interactions24.html#marginal-effects-2",
    "title": "Differential Intercepts and Interactions",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nSo we would say this is the marginal effect of \\(x_1\\) across the values of \\(D_1\\). Note that all interpretation is conditional.\n\\[\\frac{\\partial{Y}}{\\partial{x_{1}}}= \\beta_{1}+\\beta_{3}*D_1 \\]\nThe marginal effect of \\(D_1\\) across the values of \\(x_1\\) is\n\\[\\frac{\\partial{Y}}{\\partial{D_{1}}}= \\beta_{2}+\\beta_{3}*x_1 \\]\nNotice again, if \\(x_1=0\\), the marginal effect of a change in \\(D_1\\) is just \\(\\beta_2\\).\n\n\ncode\navg &lt;- data %&gt;% mutate(x2=0, interaction=0)\nint0 &lt;- data.frame(x1=avg$x1, predict(m2, interval=\"confidence\", se.fit=TRUE, newdata = avg))\navg &lt;- data %&gt;% mutate(x2=1, interaction=x1) \nint1 &lt;- data.frame(x1=avg$x1, predict(m2, interval=\"confidence\", se.fit=TRUE, newdata = avg))\n\nallpred &lt;- data.frame(x1=int0$x1, fit0=int0$fit.fit, fit1=int1$fit.fit, me=int1$fit.fit-int0$fit.fit)\n\nggplot() +\n  geom_line(data=allpred, aes(x=x1, y=fit0), linetype=\"dashed\") +\n  geom_line(data=allpred, aes(x=x1, y=fit1), linetype=\"dashed\") +\n  geom_line(data=allpred, aes(x=x1, y=me), color=\"blue\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  annotate(\"text\", x = 0, y = -2, label = \"x2=0\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 5, label = \"x2=1\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 2.5, y = 6, label = \"marginal \\n\\n effect\", size=2.5, colour=\"gray30\")+\n  theme_minimal()\n\n\n\n\n\nIn this case, the marginal effect of \\(x_2\\) is the difference between the two lines. The dashed lines are the predicted values of \\(y\\) for \\(x_1\\) given \\(x_2=0\\) and \\(x_2=1\\). The solid line is the difference between the two, the marginal effect of \\(x_2\\) across the values of \\(x_1\\)"
  },
  {
    "objectID": "interactions24.html#conditionality",
    "href": "interactions24.html#conditionality",
    "title": "Differential Intercepts and Interactions",
    "section": "Conditionality",
    "text": "Conditionality\nAll effects in this model are now conditional - we can no longer talk about independent or direct effects of either \\(D_1\\) or \\(x_1\\) in this model. The marginal effects should also give us some hints about hypothesis testing:\n\\[ \\frac{\\partial{y}}{\\partial{d_1}}= \\beta_1+\\beta_3*x_2 \\]\nNote the question is not whether \\(\\beta_1\\) or \\(\\beta_3\\) is different from zero. Instead, the question is whether \\(\\beta_{1}+\\beta_{3}\\) is different from zero. This is a lot like asking whether \\(\\beta_{1}+\\beta_{3} = \\beta_1\\). If so, structural stability holds; if not, it fails."
  },
  {
    "objectID": "interactions24.html#inference",
    "href": "interactions24.html#inference",
    "title": "Differential Intercepts and Interactions",
    "section": "Inference",
    "text": "Inference\nThink about the hypothesis that the slope for \\(D_1=0\\) is different from the slope for \\(D_1=1\\) - in the marginal effect, the slope for \\(D_1=0\\) is \\(\\beta_1\\); for \\(D_1=1\\) is \\(\\beta_{1}+\\beta_{3}\\).\nWhether the interaction coefficient, \\(\\beta_{3}\\) is different from zero does not matter. What matters is whether the combination of \\(\\beta_{1}+\\beta_{3}\\) is different from zero.\nTo evaluate the hypothesis \\(\\beta_{1}+\\beta_{3} \\neq 0\\), we need a standard error for this quantity."
  },
  {
    "objectID": "interactions24.html#inference-1",
    "href": "interactions24.html#inference-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Inference",
    "text": "Inference\nWe can’t just add the two standard errors together, but constructing one from the variance-covariance matrix of \\(\\beta\\) is easy:\n\\[ se(\\beta_{1}+ \\beta_{3})= \\sqrt{var(\\beta_{1})+ X_{2}^{2}var(\\beta_{3})+2X_{2}cov(\\beta_{1},\\beta_{3})} \\]"
  },
  {
    "objectID": "interactions24.html#references",
    "href": "interactions24.html#references",
    "title": "Differential Intercepts and Interactions",
    "section": "References",
    "text": "References\n\n\nBrambor, T., W. R. Clark, and M. Golder. 2006. “Understanding Interaction Models: Improving Empirical Analyses.” Political Analysis 14 (1): 63–82."
  },
  {
    "objectID": "interactions24s.html#indicator-variables",
    "href": "interactions24s.html#indicator-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "Indicator Variables",
    "text": "Indicator Variables\nSuppose the following regression:\n\\[Y=\\beta_0+\\beta_1(D)+\\varepsilon \\]\nwhere \\(D\\) is a dummy variable, and \\(y\\) is a continuous variable."
  },
  {
    "objectID": "interactions24s.html#indicator-variables-1",
    "href": "interactions24s.html#indicator-variables-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Indicator Variables",
    "text": "Indicator Variables\nNow, consider the conditional expected values of \\(Y\\):\n\\[E[Y|D_1=0]= \\beta_0 = \\bar{Y} |D=0 \\nonumber  \\]\n\\[E[Y|D_1=1]= \\beta_0 + \\beta_1 = \\bar{Y}|D=1  \\]\nSo these are just the conditional means of \\(Y\\); the conditions are given by the values of \\(D\\) and represent the subsamples for \\(D=0\\) and \\(D=1\\)."
  },
  {
    "objectID": "interactions24s.html#differential-intercept",
    "href": "interactions24s.html#differential-intercept",
    "title": "Differential Intercepts and Interactions",
    "section": "Differential Intercept",
    "text": "Differential Intercept\nThe dummy variable estimate represents the differential intercept, or the difference in the mean of \\(Y|D=0\\) and the mean of \\(Y|D=1\\). The estimate \\(\\beta_1\\) is the difference in height on the \\(y\\) axis between the two groups in \\(D\\). The actual intercepts are:\n\\[Y|D=0 = \\beta_0\\]\n\\[Y|D=1 = \\beta_0+\\beta_1\\]\n\n\ncode\n## differential intercepts ----\nset.seed(12345)\ndata &lt;- tibble(\nX &lt;- rnorm_multi(1000, 3, \n                     mu=c(0, 5, 0), \n                     sd=c(1, 5, .5),\n                     r = c(0,0, 0.0),\n                     varnames=c(\"x1\", \"x2\", \"e\"))\n  ) %&gt;%\n    mutate(x2= ifelse(x2&gt;median(x2),1,0)) %&gt;%\n    mutate(y = 1 + 1*x1 + 2*x2 + e)\n    m1 &lt;- (lm(y ~ x1 + as.factor(x2), data=data))\n  \n### intercept differences only  ----\n  \ndata1 &lt;- data %&gt;% mutate(x1=0)  \npredint &lt;- data.frame(data, predict(m1, interval=\"confidence\", se.fit=TRUE, newdata = data1))\npredint &lt;- predint %&gt;% mutate(ub=fit.fit +10.96*se.fit, lb=fit.fit -10.96*se.fit) #absurd CI for illustration \n\n\nggplot(data=predint, aes(x=x2, y=fit.fit)) +\n  geom_point(size = 1) +\n  geom_pointrange(aes(ymin=lb, ymax=ub)) +\n  scale_x_continuous(breaks = c(0,1)) +\n  labs ( colour = NULL, x = \"x2\", y =  \"Predicted xb\" ) +\n  theme_minimal()+\n  ggtitle(\"Differential Intercepts\")"
  },
  {
    "objectID": "interactions24s.html#differential-intercept-1",
    "href": "interactions24s.html#differential-intercept-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Differential Intercept",
    "text": "Differential Intercept\nNote this logic extends to multiple indicator variables where \\(\\beta_0\\) measures the intercept for the excluded category (all indicators set to zero), and \\(\\beta_0+\\beta_1\\) measures the intercept for any other category where its indicator is set to one, all others to zero."
  },
  {
    "objectID": "interactions24s.html#multiple-indicator-variables",
    "href": "interactions24s.html#multiple-indicator-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "Multiple Indicator Variables",
    "text": "Multiple Indicator Variables\nThinking of the Ill-Treatment models we’ve been working with:\n\\[ Y=\\beta_0+\\beta_1(D_{1})+\\beta_{2}(D_{2}) + \\ldots + \\varepsilon \\]\nwhere \\(D_{1}=1\\) indicates a civil war, and \\(D_{2}=1\\) indicates the government restricts IGO access:\n\n\\(E[Y]\\) for a non civil war, no restriction country is \\(\\beta_{0}\\).\n\\(E[Y]\\) for a civil war state with no restrictions is \\(\\beta_{0}+\\beta_{1}\\)\n\\(E[Y]\\) for a non civil war state with restricted access is \\(\\beta_{0}+\\beta_{2}\\)\n\\(E[Y]\\) for a civil war state that restricts access is \\(\\beta_{0}+\\beta_{1}+\\beta_{2}\\)\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1) %&gt;%\n  mutate(interact= civilwar*lagRA)\n\n\nm3 &lt;- lm(scarring ~ lagRA + civilwar  + lagprotest + polity2 +wdi_gdpc+wdi_pop, data=itt)\n# summary(m1)\n# average predictions, end point boundaries\n# estimation sample\nitt$used &lt;- TRUE\nitt$used[na.action(m3)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\n#across 4 combos of RA and CW\n\npredictions &lt;- data.frame(case=seq(1,4,1), xb=0, se=0, ub=0, lb=0, model=0 )\n\navg &lt;- ittesample %&gt;% mutate(civilwar=0, lagRA=0 )\nall0 &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[1:1,2:3]&lt;- data.frame(xb=median(all0$fit.fit, na.rm = TRUE), seall0=median(all0$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=1, lagRA=0 )\ncw &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[2:2,2:3]&lt;- data.frame(xb=median(cw$fit.fit, na.rm = TRUE), seall0=median(cw$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=0, lagRA=1 )\nra &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[3:3,2:3]&lt;- data.frame(xb=median(ra$fit.fit, na.rm = TRUE), seall0=median(ra$se.fit,na.rm = TRUE))\n\navg &lt;- ittesample %&gt;% mutate(civilwar=1, lagRA=1 )\nall1 &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, newdata = avg))\npredictions[4:4,2:3]&lt;- data.frame(xb=median(all1$fit.fit, na.rm = TRUE), seall0=median(all1$se.fit,na.rm = TRUE))\n\npredictions &lt;- predictions %&gt;% mutate(ub=xb +1.96*se , lb =xb -1.96*se)\n\n\n#plot\nggplot(data=predictions, aes(x=case, y=xb)) +\n  geom_pointrange(data=predictions, aes(ymin=lb, ymax=ub)) +\n  labs ( colour = NULL, x = \"\", y =  \"Expected Scarring Torture Reports\" ) +\n  guides(x=\"none\")+\n  annotate(\"text\", x = 1.6, y = 5, label = \"No restriction, no civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 2, y = 9.5, label = \"No restriction, civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 3, y = 13, label = \"Restriction, no civil war\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 3.2, y = 22, label = \"Restriction and civil war\", size=3.5, colour=\"gray30\")+\n  theme_minimal()"
  },
  {
    "objectID": "interactions24s.html#fixed-effects",
    "href": "interactions24s.html#fixed-effects",
    "title": "Differential Intercepts and Interactions",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nLet’s take this notion of differential intercepts one final step. Suppose we estimate a model of Ill-Treatment and include a dummy variable for each country (minus one for the excluded category).\n\n\ncode\n# fixed effects ----\nitt$c &lt;- as.factor(itt$ctryname)\nitt &lt;- itt %&gt;% filter(!is.na(c))\n# set ref category to US\n# itt &lt;- within(itt, cname &lt;- relevel(cname, ref = \"United States of America\"))\n\n# be sure NA and \"\" are not categories in factor var cname\nm4 &lt;- lm(scarring ~ c, data=itt%&gt;%filter(c!=\"\"))\n#modelsummary(m4)\n\nlibrary(stargazer)\nstargazer(m4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),  column.labels=c(\"Fixed Effects\"),  dep.var.caption=\"Dependent Variable: Scarring Torture\", dep.var.labels.include=FALSE, notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Scarring Torture\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\n\n\n\ncAlbania\n\n\n4.717 (3.487)\n\n\n\n\ncAlgeria\n\n\n-3.283 (3.487)\n\n\n\n\ncAngola\n\n\n6.444 (3.487)\n\n\n\n\ncArgentina\n\n\n3.990 (3.487)\n\n\n\n\ncArmenia\n\n\n4.644 (3.565)\n\n\n\n\ncAustralia\n\n\n-4.156 (3.565)\n\n\n\n\ncAustria\n\n\n-0.556 (3.565)\n\n\n\n\ncAzerbaijan\n\n\n3.263 (3.487)\n\n\n\n\ncBangladesh\n\n\n1.717 (3.487)\n\n\n\n\ncBelarus\n\n\n0.990 (3.487)\n\n\n\n\ncBelgium\n\n\n1.172 (3.487)\n\n\n\n\ncBenin\n\n\n-4.556 (6.065)\n\n\n\n\ncBolivia\n\n\n0.778 (3.658)\n\n\n\n\ncBosnia and Herzegovina\n\n\n-4.306 (4.662)\n\n\n\n\ncBrazil\n\n\n18.626*** (3.487)\n\n\n\n\ncBulgaria\n\n\n9.626** (3.487)\n\n\n\n\ncBurkina Faso\n\n\n-3.413 (3.910)\n\n\n\n\ncBurundi\n\n\n1.444 (3.487)\n\n\n\n\ncCambodia\n\n\n-3.656 (3.565)\n\n\n\n\ncCameroon\n\n\n-0.456 (3.565)\n\n\n\n\ncCanada\n\n\n-4.181 (3.770)\n\n\n\n\ncCentral African Republic\n\n\n5.778 (5.173)\n\n\n\n\ncChad\n\n\n-0.856 (3.565)\n\n\n\n\ncChile\n\n\n-0.856 (3.565)\n\n\n\n\ncChina\n\n\n28.354*** (3.487)\n\n\n\n\ncColombia\n\n\n5.172 (3.487)\n\n\n\n\ncCongo (Brazzaville, Republic of Congo)\n\n\n-2.356 (3.565)\n\n\n\n\ncCosta Rica\n\n\n-3.889 (5.173)\n\n\n\n\ncCote d’Ivoire\n\n\n-2.374 (3.487)\n\n\n\n\ncCroatia\n\n\n0.144 (3.565)\n\n\n\n\ncCuba\n\n\n2.354 (3.487)\n\n\n\n\ncCzech Republic\n\n\n-0.756 (3.565)\n\n\n\n\ncDemocratic Republic of the Congo (Zaire, Congo-Kinshasha)\n\n\n6.778 (4.089)\n\n\n\n\ncDenmark\n\n\n-5.056 (3.565)\n\n\n\n\ncDominican Republic\n\n\n-4.000 (3.658)\n\n\n\n\ncEast Timor\n\n\n-2.556 (5.173)\n\n\n\n\ncEcuador\n\n\n-0.828 (3.487)\n\n\n\n\ncEgypt\n\n\n5.354 (3.487)\n\n\n\n\ncEl Salvador\n\n\n-4.222 (4.089)\n\n\n\n\ncEritrea\n\n\n-3.681 (3.770)\n\n\n\n\ncEstonia\n\n\n-4.806 (4.662)\n\n\n\n\ncEthiopia\n\n\n-3.181 (3.770)\n\n\n\n\ncFinland\n\n\n-5.556 (5.173)\n\n\n\n\ncFrance\n\n\n2.535 (3.487)\n\n\n\n\ncGabon\n\n\n-4.889 (5.173)\n\n\n\n\ncGambia\n\n\n-3.444 (3.658)\n\n\n\n\ncGeorgia\n\n\n4.990 (3.487)\n\n\n\n\ncGermany\n\n\n9.172** (3.487)\n\n\n\n\ncGhana\n\n\n-3.270 (3.910)\n\n\n\n\ncGreece\n\n\n4.899 (3.487)\n\n\n\n\ncGuatemala\n\n\n-3.283 (3.487)\n\n\n\n\ncGuinea\n\n\n-1.556 (3.910)\n\n\n\n\ncGuinea-Bissau\n\n\n1.844 (4.328)\n\n\n\n\ncGuyana\n\n\n-3.681 (3.770)\n\n\n\n\ncHaiti\n\n\n-1.556 (3.658)\n\n\n\n\ncHonduras\n\n\n-3.856 (3.565)\n\n\n\n\ncHungary\n\n\n2.744 (3.565)\n\n\n\n\ncIndia\n\n\n4.899 (3.487)\n\n\n\n\ncIndonesia\n\n\n27.263*** (3.487)\n\n\n\n\ncIran\n\n\n0.535 (3.487)\n\n\n\n\ncIraq\n\n\n-2.756 (3.565)\n\n\n\n\ncIreland\n\n\n-4.556 (6.065)\n\n\n\n\ncIsrael\n\n\n5.444 (3.487)\n\n\n\n\ncItaly\n\n\n11.172** (3.487)\n\n\n\n\ncJamaica\n\n\n-1.919 (3.487)\n\n\n\n\ncJapan\n\n\n-3.646 (3.487)\n\n\n\n\ncJordan\n\n\n-1.010 (3.487)\n\n\n\n\ncKazakhstan\n\n\n-1.646 (3.487)\n\n\n\n\ncKenya\n\n\n8.535* (3.487)\n\n\n\n\ncKuwait\n\n\n-5.306 (3.770)\n\n\n\n\ncKyrgyzstan\n\n\n-3.889 (3.658)\n\n\n\n\ncLaos\n\n\n-2.737 (3.487)\n\n\n\n\ncLatvia\n\n\n-4.889 (5.173)\n\n\n\n\ncLebanon\n\n\n0.244 (4.328)\n\n\n\n\ncLesotho\n\n\n-3.556 (3.770)\n\n\n\n\ncLiberia\n\n\n1.819 (3.770)\n\n\n\n\ncLibyan Arab Jamahiriya\n\n\n-3.374 (3.487)\n\n\n\n\ncLithuania\n\n\n-4.556 (6.065)\n\n\n\n\ncMacedonia\n\n\n4.778 (3.658)\n\n\n\n\ncMadagascar\n\n\n-0.556 (6.065)\n\n\n\n\ncMalawi\n\n\n-4.889 (4.089)\n\n\n\n\ncMalaysia\n\n\n0.899 (3.487)\n\n\n\n\ncMali\n\n\n-4.556 (8.178)\n\n\n\n\ncMauritania\n\n\n-2.919 (3.487)\n\n\n\n\ncMauritius\n\n\n-3.556 (4.662)\n\n\n\n\ncMexico\n\n\n15.626*** (3.487)\n\n\n\n\ncMoldova\n\n\n-3.101 (3.487)\n\n\n\n\ncMongolia\n\n\n-4.556 (6.065)\n\n\n\n\ncMorocco\n\n\n-1.737 (3.487)\n\n\n\n\ncMozambique\n\n\n-1.556 (3.565)\n\n\n\n\ncMyanmar\n\n\n15.626*** (3.487)\n\n\n\n\ncNamibia\n\n\n-1.698 (3.910)\n\n\n\n\ncNepal\n\n\n7.263* (3.487)\n\n\n\n\ncNetherlands\n\n\n-4.556 (4.328)\n\n\n\n\ncNicaragua\n\n\n-4.389 (4.089)\n\n\n\n\ncNiger\n\n\n-2.056 (4.662)\n\n\n\n\ncNigeria\n\n\n1.717 (3.487)\n\n\n\n\ncNorth Korea\n\n\n-3.101 (3.487)\n\n\n\n\ncNorway\n\n\n-5.556 (6.065)\n\n\n\n\ncOman\n\n\n-4.756 (4.328)\n\n\n\n\ncPakistan\n\n\n1.081 (3.487)\n\n\n\n\ncPanama\n\n\n-5.056 (4.662)\n\n\n\n\ncPapua New Guinea\n\n\n-2.667 (3.658)\n\n\n\n\ncParaguay\n\n\n-2.465 (3.487)\n\n\n\n\ncPeru\n\n\n1.263 (3.487)\n\n\n\n\ncPhilippines\n\n\n2.808 (3.487)\n\n\n\n\ncPoland\n\n\n-3.756 (3.565)\n\n\n\n\ncPortugal\n\n\n4.354 (3.487)\n\n\n\n\ncQatar\n\n\n-4.931 (3.770)\n\n\n\n\ncRomania\n\n\n12.844*** (3.565)\n\n\n\n\ncRussian Federation\n\n\n23.717*** (3.487)\n\n\n\n\ncRwanda\n\n\n6.808 (3.487)\n\n\n\n\ncSaudi Arabia\n\n\n-0.828 (3.487)\n\n\n\n\ncSenegal\n\n\n2.444 (3.658)\n\n\n\n\ncSerbia and Montenegro\n\n\n-5.556 (8.178)\n\n\n\n\ncSierra Leone\n\n\n-1.841 (3.910)\n\n\n\n\ncSingapore\n\n\n-5.056 (4.662)\n\n\n\n\ncSlovakia\n\n\n0.111 (3.658)\n\n\n\n\ncSlovenia\n\n\n-3.956 (4.328)\n\n\n\n\ncSouth Africa\n\n\n-0.010 (3.487)\n\n\n\n\ncSouth Korea\n\n\n-2.222 (3.658)\n\n\n\n\ncSpain\n\n\n7.172* (3.487)\n\n\n\n\ncSudan\n\n\n10.354** (3.487)\n\n\n\n\ncSuriname\n\n\n-4.556 (8.178)\n\n\n\n\ncSwaziland\n\n\n-2.181 (3.770)\n\n\n\n\ncSweden\n\n\n-3.465 (3.487)\n\n\n\n\ncSwitzerland\n\n\n3.944 (3.565)\n\n\n\n\ncSyria\n\n\n5.808 (3.487)\n\n\n\n\ncTajikistan\n\n\n-2.101 (3.487)\n\n\n\n\ncTanzania\n\n\n1.544 (3.565)\n\n\n\n\ncThailand\n\n\n-3.756 (3.565)\n\n\n\n\ncTogo\n\n\n1.626 (3.487)\n\n\n\n\ncTrinidad and Tobago\n\n\n-2.306 (3.770)\n\n\n\n\ncTunisia\n\n\n3.081 (3.487)\n\n\n\n\ncTurkey\n\n\n54.172*** (3.487)\n\n\n\n\ncTurkmenistan\n\n\n1.081 (3.487)\n\n\n\n\ncUganda\n\n\n-0.556 (3.487)\n\n\n\n\ncUkraine\n\n\n-0.737 (3.487)\n\n\n\n\ncUnited Arab Emirates\n\n\n-4.681 (3.770)\n\n\n\n\ncUnited Kingdom\n\n\n2.263 (3.487)\n\n\n\n\ncUnited States of America\n\n\n18.899*** (3.487)\n\n\n\n\ncUruguay\n\n\n-5.222 (3.658)\n\n\n\n\ncUzbekistan\n\n\n14.354*** (3.487)\n\n\n\n\ncVenezuela\n\n\n2.944 (3.565)\n\n\n\n\ncViet Nam\n\n\n-4.556 (3.487)\n\n\n\n\ncYemen\n\n\n-1.465 (3.487)\n\n\n\n\ncZambia\n\n\n-1.828 (3.487)\n\n\n\n\ncZimbabwe\n\n\n3.717 (3.487)\n\n\n\n\nConstant\n\n\n5.556* (2.586)\n\n\n\n\n\n\n\n\nObservations\n\n\n1,317\n\n\n\n\nR2\n\n\n0.550\n\n\n\n\nAdjusted R2\n\n\n0.493\n\n\n\n\nF Statistic\n\n\n9.648*** (df = 148; 1168)\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05"
  },
  {
    "objectID": "interactions24s.html#dummy-variables",
    "href": "interactions24s.html#dummy-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\nDummy variables are useful for measuring differences between groups.\nDummy coefficients specifically measure the differences in levels (intercepts) between groups.\nDummies may capture differences between known, discrete groups, e.g. genders, parties, races, etc.\nDummies might capture unknown differences between units, say between states or countries - this is the foundation of fixed effects, an important topic coming up soon."
  },
  {
    "objectID": "interactions24s.html#structural-stability",
    "href": "interactions24s.html#structural-stability",
    "title": "Differential Intercepts and Interactions",
    "section": "Structural Stability",
    "text": "Structural Stability\nIn other words, the marginal effect of \\(x_2\\) is not \\(\\beta_2\\); instead, it is .5 for one group, and 1.7 for the other.\nRecall, the marginal effect of \\(x_2\\) in the model where we assume structural stability:\n\\[y = \\beta_0 + \\beta_1 d_1 + \\beta_2 x_2 \\]\nis \\(\\beta_2\\)."
  },
  {
    "objectID": "interactions24s.html#structural-stability-1",
    "href": "interactions24s.html#structural-stability-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Structural Stability",
    "text": "Structural Stability\nWe relax the structural stability assumption by modeling a multiplicative interaction:\n\\[y = \\beta_0 + \\beta_1 d_1 + \\beta_2 x_2 + \\beta_3 (d_1x_2)\\]\nliterally multiplying \\(d_1\\) and \\(x_2\\) together, and including that new variable in the model. \\(\\beta_3\\) measures the differential slope between the two groups in \\(d_1\\)."
  },
  {
    "objectID": "interactions24s.html#multiplicative-interactions-1",
    "href": "interactions24s.html#multiplicative-interactions-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Multiplicative Interactions",
    "text": "Multiplicative Interactions\n\\[E[Y|X, D_1=0, D_1=1] = \\beta_{0}+\\beta_{1}(D_1)+\\beta_{2}(x_{1})\\]\nIn this expression, the expectation of \\(Y\\) is conditional on \\(X\\) and on \\(D_1=0\\) or \\(D_1=1\\). This is the differential intercept circumstance, assuming a shared slope."
  },
  {
    "objectID": "interactions24s.html#multiplicative-interactions-2",
    "href": "interactions24s.html#multiplicative-interactions-2",
    "title": "Differential Intercepts and Interactions",
    "section": "Multiplicative Interactions",
    "text": "Multiplicative Interactions\n\\[E[Y|x; x|D_1=1] = \\beta_{0}+\\beta_{1}(D_1)+\\beta_{2}(x_{1}) +\\beta_{3}x_1D_1 \\]\nHere, the expected value of \\(Y\\) depends on the intercept given by \\(\\beta_0\\) and \\(\\beta_1\\), \\alert{and on slopes determined by \\(x_1\\), but those slopes differ depending on the value of \\(D_1\\).\n\n\n\n\n\n\nMarginal Effects\n\n\nAn important insight: the interaction coefficient, \\(\\beta_3\\), measures the difference in slopes between \\(D=1\\) and \\(D=0\\)."
  },
  {
    "objectID": "interactions24s.html#what-does-a-hypothesis-look-like",
    "href": "interactions24s.html#what-does-a-hypothesis-look-like",
    "title": "Differential Intercepts and Interactions",
    "section": "What does a hypothesis look like?",
    "text": "What does a hypothesis look like?\nHypothesis: An increase in \\(x_{1}\\) will produce a larger or faster increase in \\(Y\\) if \\(D_{1}\\) is present than when it is absent.\n\\[Y = \\beta_{0}+\\beta_{1}(x_1)+\\beta_{2}(D_{1}) +\\beta_{3}x_1D_{1} +\\varepsilon \\]\n\\[ \\text{if} ~D_{1}=0, \\nonumber \\] \\[ \\hat{Y}= \\beta_{0}+\\beta_{1}(x_{1})  \\] \\[\\text{if} ~D_{1}=1, \\nonumber \\\\  \\nonumber \\] \\[\\hat{Y}=(\\beta_{0}+\\beta_{2}) + (\\beta_{1}+\\beta_{3}x_{1})D_1  \\]\nHypothesis on Scarring Torture\nGiving this some substance, again thinking of the ITT data, we might hypothesize that:\nIncreases in protests will produce larger increases in scarring torture in states that restrict IGO access than in states that do not."
  },
  {
    "objectID": "interactions24s.html#marginal-effects",
    "href": "interactions24s.html#marginal-effects",
    "title": "Differential Intercepts and Interactions",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nIn the interactive model, the marginal effect of \\(x_{1}\\) is:\n\\[\\frac{\\partial{Y}}{\\partial{x_{1}}}= \\beta_{1}+\\beta_{3}*D_1 \\]\nBut note, this depends on the value of \\(D_1\\) - when \\(D_{1}=0\\), \\(\\beta_3\\) drops out. So the marginal effect of \\(x_1\\) depends explicitly on the value of \\(D_1\\)."
  },
  {
    "objectID": "interactions24s.html#marginal-effects-1",
    "href": "interactions24s.html#marginal-effects-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\\[\\frac{\\partial{Y}}{\\partial{x_{1}}}= \\beta_{1}+\\beta_{3}*D_1 \\]\nIt should make sense that if \\(D_1=0\\), the slope is just \\(\\beta_1\\). If \\(D_1=1\\), we adjust the slope by \\(\\beta_3\\), thereby allowing the slopes to be different for the two groups in \\(D_1\\), thereby relaxing structural stability."
  },
  {
    "objectID": "interactions24s.html#marginal-effects-2",
    "href": "interactions24s.html#marginal-effects-2",
    "title": "Differential Intercepts and Interactions",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nSo we would say this is the marginal effect of \\(x_1\\) across the values of \\(D_1\\). Note that all interpretation is conditional.\n\\[\\frac{\\partial{Y}}{\\partial{x_{1}}}= \\beta_{1}+\\beta_{3}*D_1 \\]\nThe marginal effect of \\(D_1\\) across the values of \\(x_1\\) is\n\\[\\frac{\\partial{Y}}{\\partial{D_{1}}}= \\beta_{2}+\\beta_{3}*x_1 \\]\nNotice again, if \\(x_1=0\\), the marginal effect of a change in \\(D_1\\) is just \\(\\beta_2\\).\n\n\ncode\navg &lt;- data %&gt;% mutate(x2=0, interaction=0)\nint0 &lt;- data.frame(x1=avg$x1, predict(m2, interval=\"confidence\", se.fit=TRUE, newdata = avg))\navg &lt;- data %&gt;% mutate(x2=1, interaction=x1) \nint1 &lt;- data.frame(x1=avg$x1, predict(m2, interval=\"confidence\", se.fit=TRUE, newdata = avg))\n\nallpred &lt;- data.frame(x1=int0$x1, fit0=int0$fit.fit, fit1=int1$fit.fit, me=int1$fit.fit-int0$fit.fit)\n\nggplot() +\n  geom_line(data=allpred, aes(x=x1, y=fit0), linetype=\"dashed\") +\n  geom_line(data=allpred, aes(x=x1, y=fit1), linetype=\"dashed\") +\n  geom_line(data=allpred, aes(x=x1, y=me), color=\"blue\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  annotate(\"text\", x = 0, y = -2, label = \"x2=0\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 5, label = \"x2=1\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 2.5, y = 6, label = \"marginal \\n\\n effect\", size=2.5, colour=\"gray30\")+\n  theme_minimal()\n\n\n\nIn this case, the marginal effect of \\(x_2\\) is the difference between the two lines. The dashed lines are the predicted values of \\(y\\) for \\(x_1\\) given \\(x_2=0\\) and \\(x_2=1\\). The solid line is the difference between the two, the marginal effect of \\(x_2\\) across the values of \\(x_1\\)"
  },
  {
    "objectID": "interactions24s.html#conditionality",
    "href": "interactions24s.html#conditionality",
    "title": "Differential Intercepts and Interactions",
    "section": "Conditionality",
    "text": "Conditionality\nAll effects in this model are now conditional - we can no longer talk about independent or direct effects of either \\(D_1\\) or \\(x_1\\) in this model. The marginal effects should also give us some hints about hypothesis testing:\n\\[ \\frac{\\partial{Y}}{\\partial{x_{1}}}= \\beta_{1}+\\beta_{3}*D_1 \\]\nNote the question is not whether \\(\\beta_1\\) or \\(\\beta_3\\) is different from zero. Instead, the question is whether \\(\\beta_{1}+\\beta_{3}\\) is different from zero. This is a lot like asking whether \\(\\beta_{1}+\\beta_{3} = \\beta_1\\). If so, structural stability holds; if not, it fails."
  },
  {
    "objectID": "interactions24s.html#inference",
    "href": "interactions24s.html#inference",
    "title": "Differential Intercepts and Interactions",
    "section": "Inference",
    "text": "Inference\nThink about the hypothesis that the slope for \\(D_1=0\\) is different from the slope for \\(D_1=1\\) - in the marginal effect, the slope for \\(D_1=0\\) is \\(\\beta_1\\); for \\(D_1=1\\) is \\(\\beta_{1}+\\beta_{3}\\).\nWhether the interaction coefficient, \\(\\beta_{3}\\) is different from zero does not matter. What matters is whether the combination of \\(\\beta_{1}+\\beta_{3}\\) is different from zero.\nTo evaluate the hypothesis \\(\\beta_{1}+\\beta_{3} \\neq 0\\), we need a standard error for this quantity."
  },
  {
    "objectID": "interactions24s.html#inference-1",
    "href": "interactions24s.html#inference-1",
    "title": "Differential Intercepts and Interactions",
    "section": "Inference",
    "text": "Inference\nWe can’t just add the two standard errors together, but constructing one from the variance-covariance matrix of \\(\\beta\\) is easy:\n\\[ se(\\beta_{1}+ \\beta_{3})= \\sqrt{var(\\beta_{1})+ X_{2}^{2}var(\\beta_{3})+2X_{2}cov(\\beta_{1},\\beta_{3})} \\]"
  },
  {
    "objectID": "interactions24s.html#references",
    "href": "interactions24s.html#references",
    "title": "Differential Intercepts and Interactions",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "interactions24.html#measurement-matters",
    "href": "interactions24.html#measurement-matters",
    "title": "Differential Intercepts and Interactions",
    "section": "Measurement matters",
    "text": "Measurement matters"
  },
  {
    "objectID": "interactions24s.html#measurement-matters",
    "href": "interactions24s.html#measurement-matters",
    "title": "Differential Intercepts and Interactions",
    "section": "Measurement matters",
    "text": "Measurement matters"
  },
  {
    "objectID": "interactions24.html#inference-on-marginal-effects",
    "href": "interactions24.html#inference-on-marginal-effects",
    "title": "Differential Intercepts and Interactions",
    "section": "Inference on Marginal Effects",
    "text": "Inference on Marginal Effects\nWe can’t just add the two standard errors together, but constructing one from the variance-covariance matrix of \\(\\beta\\) is easy:\n\\[ se(\\beta_{1}+ \\beta_{3})= \\sqrt{var(\\beta_{1})+ X_{2}^{2}var(\\beta_{3})+2X_{2}cov(\\beta_{1},\\beta_{3})} \\]\nNote these vary across \\(x\\) because, just as the marginal effect is not constant across \\(x\\), neither is the standard error of the marginal effect."
  },
  {
    "objectID": "interactions24s.html#inference-on-marginal-effects",
    "href": "interactions24s.html#inference-on-marginal-effects",
    "title": "Differential Intercepts and Interactions",
    "section": "Inference on Marginal Effects",
    "text": "Inference on Marginal Effects\nWe can’t just add the two standard errors together, but constructing one from the variance-covariance matrix of \\(\\beta\\) is easy:\n\\[ se(\\beta_{1}+ \\beta_{3})= \\sqrt{var(\\beta_{1})+ X_{2}^{2}var(\\beta_{3})+2X_{2}cov(\\beta_{1},\\beta_{3})} \\]\nNote these vary across \\(x\\) because, just as the marginal effect is not constant across \\(x\\), neither is the standard error of the marginal effect."
  },
  {
    "objectID": "interactions24.html#intercept-shifts-with-continuous-variables",
    "href": "interactions24.html#intercept-shifts-with-continuous-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Suppose we have a regression with a dummy variable and a continuous variable:\n\\[y = \\beta_0 + \\beta_1 d_1 + \\beta_2 x_2 \\] The expected value of \\(y\\) for each group is:\n\\[E[y | d_1= 0] = \\beta_0 +  \\beta_2 x_2 \\]\n\\[E[y | d_1= 0] = \\beta_0 + \\beta_1 +  \\beta_2 x_2 \\]\nNote the shift in intercept.\n\n\ncode\nggplot(data=filter(data, x2==0), aes(x=x1, y=y)) +\n  geom_smooth(method=\"lm\", se=FALSE, color=\"black\") +\n  geom_smooth(data=filter(data, x2==1), aes(x=x1, y=y), method=\"lm\", se=FALSE, color=\"black\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = 0, y = 0, label = \"x2=0\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 4, label = \"x2=1\", size=3.5, colour=\"gray30\")+\n  ggtitle(\"Differential Intercepts, Continuous x1\")\n\n\n\n\n\nThe difference is only in intercept on \\(d\\), \\(\\beta_{1}\\) - the slope estimate for \\(x_2\\) is the same no matter the value of \\(d_{1}\\).\nPut differently, these two groups given by the dummy variable share the same slope, but have different y-intercepts.\n\n\n\n\n\n\nStructural Stability\n\n\n\nIn this regression, the slope on \\(x_2\\) is the same for all groups measured by indicator variables.\n\\[y=\\beta_0 + \\beta_1 d_1 +\\beta_2x_1+ \\varepsilon \\]\nAssuming a common slope is known as the structural stability assumption."
  },
  {
    "objectID": "interactions24s.html#intercept-shifts-with-continuous-variables",
    "href": "interactions24s.html#intercept-shifts-with-continuous-variables",
    "title": "Differential Intercepts and Interactions",
    "section": "Intercept Shifts with Continuous Variables",
    "text": "Intercept Shifts with Continuous Variables\nSuppose we have a regression with a dummy variable and a continuous variable:\n\\[ Y=\\beta_0+\\beta_1(D_{1})+\\beta_{2}(X_{1, i}) + \\varepsilon \\]\n\\[E[Y_{i}|D_{1}=0] = \\beta_{0}+\\beta_{2}(X_{1, i}) \\]\n\\[E[Y_{i}|D_{1}=1] = \\beta_{0}+\\beta_{1}+ \\beta_{2}(X_{1, i}) \\]\n\n\ncode\nggplot(data=filter(data, x2==0), aes(x=x1, y=y)) +\n  geom_smooth(method=\"lm\", se=FALSE, color=\"black\") +\n  geom_smooth(data=filter(data, x2==1), aes(x=x1, y=y), method=\"lm\", se=FALSE, color=\"black\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = 0, y = 0, label = \"x2=0\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 4, label = \"x2=1\", size=3.5, colour=\"gray30\")+\n  ggtitle(\"Differential Intercepts, Continuous x1\")\n\n\n\nThe difference is only in intercept, \\(\\beta_{1}\\) - the slope estimate for \\(X_{1}\\) is the same no matter the value of \\(D_{1}\\).\nPut differently, these two groups given by the dummy variable share the same slope, but have different y-intercepts.\n\n\n\n\n\n\nStructural Stability\n\n\nIn this regression, the slope on \\(X_1\\) is the same for all groups measured by indicator variables.\n\\[Y=\\beta_0+\\beta_1(D_{1})+\\beta_{2}(X_{1, i}) + \\varepsilon \\]\nAssuming a common slope is known as the structural stability assumption."
  },
  {
    "objectID": "interactions24.html#illustration",
    "href": "interactions24.html#illustration",
    "title": "Differential Intercepts and Interactions",
    "section": "Illustration",
    "text": "Illustration\n\n\ncode\ndata &lt;- data %&gt;% mutate(interaction=x1*x2) %&gt;%\n  mutate(yi = 1 + 1*x1 + 2*x2 + 1.5*interaction + e)\nm1 &lt;- (lm(y ~ x1 + x2, data=data))\nm2 &lt;- (lm(yi ~ x1 + x2 + x1:x2, data=data))\n#summary(m2)  \n\n#using sjPlot\np1 &lt;- plot_model(m1, type=\"eff\", terms=c(\"x1\", \"x2\"), show.values = TRUE, show.p = TRUE, title=\"Differential Intercept\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  guides(colour=\"none\") +\n  annotate(\"text\", x = 0, y = -1, label = \"x2=0\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 5, label = \"x2=1\", size=2.5, colour=\"gray30\")+ theme_minimal()\n  \np2 &lt;-  plot_model(m2, type=\"eff\", terms=c(\"x1\", \"x2\"), show.values = TRUE, show.p = TRUE, title=\"Interaction, x1*x2\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  guides(colour=\"none\") +\n  annotate(\"text\", x = 0, y = -2, label = \"x2=0\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 5, label = \"x2=1\", size=2.5, colour=\"gray30\")+\n  theme_minimal()\n\np1/p2\n\n\n\n\n\nIn the first panel, we see the differential intercepts - the slope on \\(x_1\\) is the same for both groups, but the intercepts are different. This is structural stability. The bottom panel relaxes the structural stability assumption, allowing the slope on \\(x_1\\) to differ between the two groups."
  },
  {
    "objectID": "interactions24.html#fixed-effects-plot",
    "href": "interactions24.html#fixed-effects-plot",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "Here’s a plot of the fixed effect coefficients - each is a differential intercept, measuring the difference from the reference category.\n\n\ncode\n# reference category is AFG by default \n# coefficients as data frame for plotting\ncoefs&lt;- data.frame(coef(summary(m4)))\ncoefs$country &lt;- rownames(coefs)\ncoefs$country &lt;- substr(coefs$country, 2, nchar(coefs$country))\ncoefs$country[coefs$country==\"Intercept)\"] &lt;- \"Intercept\"\ncoefs$sig &lt;-ifelse(coefs$`Pr...t..`&lt;.05, 1,0)\n\n\n## plot fixed effects ----\n\np &lt;- ggplot(coefs, aes(x = Estimate, y = country, color = factor(sig), label = country)) +\n  geom_point(size = 1) +\n  geom_errorbarh(aes(xmin = Estimate - 1.96 * Std..Error, xmax = Estimate + 1.96 * Std..Error)) +\n  geom_text_repel(data=coefs%&gt;%filter(sig==1),  size=2.5, force=5) + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs ( colour = NULL, y = \"\", x =  \"Expected Scarring Torture Reports\" ) +\n  ggtitle(\"Fixed Effects\", subtitle = \"Country Coefficients\")\n\np + scale_color_manual(values = c(\"red\", \"black\")) +\n  guides(color=\"none\")\n\n\n\n\n\nThe result is that each coefficient is an intercept for a particular country. Each measures the difference in that country’s intercept (or mean of \\(y\\)) from the excluded category, in this case, Afghanistan. Black bars are different from the excluded category at the .05 level.\nEach country coefficient plus the intercept is that country’s mean of \\(y\\), scarring torture . Looking, for instance at the US, the coefficient is about 19, so the US mean scarring torture reports is 19 plus the intercept (5.5), so about 24.5 (the actual mean for US scarring torture reports is 24.45)."
  },
  {
    "objectID": "interactions24s.html#fixed-effects-plot",
    "href": "interactions24s.html#fixed-effects-plot",
    "title": "Differential Intercepts and Interactions",
    "section": "Fixed Effects Plot",
    "text": "Fixed Effects Plot\n\n\ncode\n# reference category is AFG by default \n# coefficients as data frame for plotting\ncoefs&lt;- data.frame(coef(summary(m4)))\ncoefs$country &lt;- rownames(coefs)\ncoefs$country &lt;- substr(coefs$country, 6, nchar(coefs$country))\ncoefs$country[coefs$country==\"rcept)\"] &lt;- \"Intercept\"\ncoefs$sig &lt;-ifelse(coefs$`Pr...t..`&lt;.05, 1,0)\n\n\n## plot fixed effects ----\n\np &lt;- ggplot(coefs, aes(x = Estimate, y = country, color = factor(sig), label = country)) +\n  geom_point(size = 1) +\n  geom_errorbarh(aes(xmin = Estimate - 1.96 * Std..Error, xmax = Estimate + 1.96 * Std..Error)) +\n  geom_text_repel(data=coefs%&gt;%filter(sig==1),  size=2.5, force=5) + \n  theme_bw(base_size = 10)+\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs ( colour = NULL, y = \"\", x =  \"Expected Scarring Torture Reports\" ) +\n  ggtitle(\"Fixed Effects\", subtitle = \"Country Coefficients\")\n\np + scale_color_manual(values = c(\"red\", \"black\")) +\n  guides(color=\"none\")\n\n\n\nThe result is that each coefficient is an intercept for a particular country. Each measures the difference in that country’s intercept (or mean of \\(y\\)) from the excluded category, in this case, Afghanistan. Black bars are different from the excluded categorya at the .05 level.\nEach country coefficient plus the intercept is that country’s mean of \\(y\\), scarring torture . Looking, for instance at the US, the coefficient is about 19, so the US mean scarring torture reports is 19 plus the intercept (5.5), so about 24.5 (the actual mean for US scarring torture reports is 24.45)."
  },
  {
    "objectID": "interactions24s.html#illustration",
    "href": "interactions24s.html#illustration",
    "title": "Differential Intercepts and Interactions",
    "section": "Illustration",
    "text": "Illustration\n\n\ncode\ndata &lt;- data %&gt;% mutate(interaction=x1*x2) %&gt;%\n  mutate(yi = 1 + 1*x1 + 2*x2 + 1.5*interaction + e)\nm1 &lt;- (lm(y ~ x1 + x2, data=data))\nm2 &lt;- (lm(yi ~ x1 + x2 + x1:x2, data=data))\n#summary(m2)  \n\n#using sjPlot\np1 &lt;- plot_model(m1, type=\"eff\", terms=c(\"x1\", \"x2\"), show.values = TRUE, show.p = TRUE, title=\"Differential Intercept\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  guides(colour=\"none\") +\n  annotate(\"text\", x = 0, y = -1, label = \"x2=0\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 5, label = \"x2=1\", size=2.5, colour=\"gray30\")+ theme_minimal()\n  \np2 &lt;-  plot_model(m2, type=\"eff\", terms=c(\"x1\", \"x2\"), show.values = TRUE, show.p = TRUE, title=\"Interaction, x1*x2\") +\n  labs ( colour = NULL, x = \"x1\", y =  \"Predicted xb\" ) +\n  guides(colour=\"none\") +\n  annotate(\"text\", x = 0, y = -2, label = \"x2=0\", size=2.5, colour=\"gray30\")+\n  annotate(\"text\", x = 0, y = 5, label = \"x2=1\", size=2.5, colour=\"gray30\")+\n  theme_minimal()\n\np1/p2\n\n\n\nIn the first panel, we see the differential intercepts - the slope on \\(x_1\\) is the same for both groups, but the intercepts are different. This is structural stability. The bottom panel relaxes the structural stability assumption, allowing the slope on \\(x_1\\) to differ between the two groups."
  },
  {
    "objectID": "limiteddv24.html",
    "href": "limiteddv24.html",
    "title": "Binary \\(y\\) variables",
    "section": "",
    "text": "What happens if \\(y\\) is binary?\nDoes OLS work? This model is known as the linear probability model.\nWhat are the problems?\nLogit, Probit as solutions.\n\n\n\n\nThe LPM is the OLS model estimated with a binary dependent variable.\nThis is generally frowned upon.\n\n\n\nThe model we’ve worked with so far is\n\\[y_i=F(\\beta_0 +  \\beta_1 X_1 + \\beta_2 X_2 \\ldots + \\beta_k X_k + \\epsilon_i) \\]\nwhere \\(F\\) is a linear function, so\n\\[\\hat{y_i}=\\hat{\\beta_0} +  \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 \\ldots + \\hat{\\beta_k} X_k \\nonumber\\\\ \\nonumber\\\\\n=x\\hat{\\beta} \\]\n\\(F\\) is linear, so the model is linear in parameters. \\(x\\hat{\\beta}\\) is the linear prediction and is the , the conditional expected value of \\(y\\).\nNow, consider a situation where \\(y\\) is binary, such that,\n\\[  y = \\left\\{ \\begin{array}{ll}\n         1 \\\\\n         0  \n         \\end{array}\n     \\right. \\]\nand\n\\[ y^* = \\left\\{ \\begin{array}{ll}\n          \\pi_{i}\\\\\n          1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\nwhere \\(y_i^*\\) is what we wish we could measure (say, the probability \\(y_i=1\\)), though we can only measure \\(y_i\\). Now, \\(y^*\\) is going to be our principle quantity of interest.\nSuppose the regression model\n\\[y_i =F(\\beta_0 +  \\beta_1 X_1 + \\beta_2 X_2 \\ldots + \\beta_k X_k + \\epsilon_i) \\]\nwhere \\(F\\) is a nonlinear function relating the linear prediction, \\(x\\beta\\) to \\(y_i^*\\).\n\\[\\widehat{y^*} = F(x\\widehat{\\beta})=F(\\hat{\\beta_0} +  \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 \\ldots + \\hat{\\beta_k} X_k ) \\]\n\\(F\\) is a nonlinear function, so the model is nonlinear in parameters. \\(x\\hat{\\beta}\\) is the linear prediction, but it is not the quantity of interest. Instead, the quantity of interest is \\(F(x\\hat{\\beta})\\) which is equal to \\(y_i^*\\).\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe difference between this model and the OLS linear model is simply that we must transform the linear prediction, \\(x\\hat{\\beta}\\), by \\(F\\) in order to produce predictions. Put differently, we want to"
  },
  {
    "objectID": "limiteddv24.html#goals",
    "href": "limiteddv24.html#goals",
    "title": "Binary \\(y\\) variables",
    "section": "",
    "text": "What happens if \\(y\\) is binary?\nDoes OLS work? This model is known as the linear probability model.\nWhat are the problems?\nLogit, Probit as solutions."
  },
  {
    "objectID": "limiteddv24.html#linear-probability-model",
    "href": "limiteddv24.html#linear-probability-model",
    "title": "Binary \\(y\\) variables",
    "section": "",
    "text": "The LPM is the OLS model estimated with a binary dependent variable.\nThis is generally frowned upon."
  },
  {
    "objectID": "limiteddv24.html#linear-model",
    "href": "limiteddv24.html#linear-model",
    "title": "Binary \\(y\\) variables",
    "section": "",
    "text": "The model we’ve worked with so far is\n\\[y_i=F(\\beta_0 +  \\beta_1 X_1 + \\beta_2 X_2 \\ldots + \\beta_k X_k + \\epsilon_i) \\]\nwhere \\(F\\) is a linear function, so\n\\[\\hat{y_i}=\\hat{\\beta_0} +  \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 \\ldots + \\hat{\\beta_k} X_k \\nonumber\\\\ \\nonumber\\\\\n=x\\hat{\\beta} \\]\n\\(F\\) is linear, so the model is linear in parameters. \\(x\\hat{\\beta}\\) is the linear prediction and is the , the conditional expected value of \\(y\\).\nNow, consider a situation where \\(y\\) is binary, such that,\n\\[  y = \\left\\{ \\begin{array}{ll}\n         1 \\\\\n         0  \n         \\end{array}\n     \\right. \\]\nand\n\\[ y^* = \\left\\{ \\begin{array}{ll}\n          \\pi_{i}\\\\\n          1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\nwhere \\(y_i^*\\) is what we wish we could measure (say, the probability \\(y_i=1\\)), though we can only measure \\(y_i\\). Now, \\(y^*\\) is going to be our principle quantity of interest.\nSuppose the regression model\n\\[y_i =F(\\beta_0 +  \\beta_1 X_1 + \\beta_2 X_2 \\ldots + \\beta_k X_k + \\epsilon_i) \\]\nwhere \\(F\\) is a nonlinear function relating the linear prediction, \\(x\\beta\\) to \\(y_i^*\\).\n\\[\\widehat{y^*} = F(x\\widehat{\\beta})=F(\\hat{\\beta_0} +  \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 \\ldots + \\hat{\\beta_k} X_k ) \\]\n\\(F\\) is a nonlinear function, so the model is nonlinear in parameters. \\(x\\hat{\\beta}\\) is the linear prediction, but it is not the quantity of interest. Instead, the quantity of interest is \\(F(x\\hat{\\beta})\\) which is equal to \\(y_i^*\\).\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe difference between this model and the OLS linear model is simply that we must transform the linear prediction, \\(x\\hat{\\beta}\\), by \\(F\\) in order to produce predictions. Put differently, we want to"
  },
  {
    "objectID": "limiteddv24.html#why-leave-the-robust-ols-model",
    "href": "limiteddv24.html#why-leave-the-robust-ols-model",
    "title": "Binary \\(y\\) variables",
    "section": "",
    "text": "Why not use the linear model when \\(y\\) is binary?\n\nbecause we fail to satisfy the OLS assumptions.\nbecause the residuals are not Normal.\nbecause \\(y\\) is not Normal.\nbecause \\(y\\) is limited.\n\nLimited \\(y\\) variables are \\(y\\)s where our measurement is limited by the realities of the world. Such variables are rarely normal, often not continuous, and often observable indicators of unobservable things - this is all true of binary variables."
  },
  {
    "objectID": "limiteddv24.html#limited-dependent-variables",
    "href": "limiteddv24.html#limited-dependent-variables",
    "title": "Binary \\(y\\) variables",
    "section": "Limited Dependent Variables",
    "text": "Limited Dependent Variables\nWhy would we measure \\(y_i\\) rather than \\(y_i^*\\)?\nLimited dependent variables are usually limited in the sense that we cannot observe the range of the variable or the characteristic of the variable we want to observe. We are limited to observing \\(y_i\\), and so must estimate \\(y_i^*\\)."
  },
  {
    "objectID": "limiteddv24.html#examples-of-limited-dvs",
    "href": "limiteddv24.html#examples-of-limited-dvs",
    "title": "Binary \\(y\\) variables",
    "section": "Examples of Limited DVs",
    "text": "Examples of Limited DVs\n\nbinary variables: 0=peace, 1=war; 0=vote, 1=don’t vote.\nunordered or nominal categorical variables: type of car you prefer: Honda, Toyota, Ford, Buick; policy choices; consumer choices.\nordered variables that take on few values: some survey responses.\n\n-discrete count variables; number of episodes of scarring torture in a country-year, 0, 1, 2, 3, , \\(\\infty\\)\n\ntime to failure; how long a civil war lasts; how long a patient survives disease; how long a leader survives in office."
  },
  {
    "objectID": "limiteddv24.html#binary-dependent-variables",
    "href": "limiteddv24.html#binary-dependent-variables",
    "title": "Binary \\(y\\) variables",
    "section": "Binary dependent variables",
    "text": "Binary dependent variables\nGenerally, we conceive of a binary variable as being the observable manifestation of some underlying, latent, unobserved continuous variable.\nIf we could adequately observe (and measure) the underlying continuous variable, we’d use some form of OLS regression to analyze that variable."
  },
  {
    "objectID": "limiteddv24.html#why-not-use-ols",
    "href": "limiteddv24.html#why-not-use-ols",
    "title": "Binary \\(y\\) variables",
    "section": "",
    "text": "\\[ \\mathbf{y}=\\mathbf{X \\beta} + \\mathbf{u} \\]\nwhere we are principally interested in the conditional expectation of \\(y\\), \\(E(y_{i}|\\mathbf{x_{i}})\\) where we want to interpret that expectation as a conditional probability, \\(Pr(y=1|\\mathbf{x_{i}})\\); we focus on the probability the outcome occurs (i.e., \\(y\\) is equal to one)."
  },
  {
    "objectID": "limiteddv24.html#linear-probability-model-1",
    "href": "limiteddv24.html#linear-probability-model-1",
    "title": "Binary \\(y\\) variables",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nThe linear probability model (LPM) is the OLS linear regression with a binary dependent variable.\nThe main justification for the LPM is OLS is unbiased (by Gauss Markov). But \\(\\ldots\\)\n\npredictions are nonsensical (linear, unbounded, measures of \\(\\hat{y}\\) rather than \\(y^*\\)).\ndisturbances are non-normal, and heteroskedastic.\nrelation or mapping of \\(x\\beta\\) and \\(y\\) are the wrong functional form (linear)."
  },
  {
    "objectID": "limiteddv24.html#on-linearity",
    "href": "limiteddv24.html#on-linearity",
    "title": "Binary \\(y\\) variables",
    "section": "On Linearity",
    "text": "On Linearity\nIn the linear model, \\(\\hat{y_i}=x_i\\beta\\). This makes sense because \\(y = y^*\\). Put differently, \\(y\\) is continuous, unbounded, (assumed) normal, and is an “unlimited” measure of the concept we intend to measure.\nIn binary models, \\(y \\neq y^*\\), because our observation of \\(y\\) is limited such that we can only observe its presence or absence. We have two different realizations of the same variable: \\(y\\) is the limited but observed variable; \\(y^*\\) is the unlimited variable we want to measure, but cannot because it is unobservable.\nThe goal of these models is to use \\(y\\) in the regression in order to get estimates of \\(y^*\\). Those estimates of \\(y^*\\) are our principle quantity of interest in the binary variable model."
  },
  {
    "objectID": "limiteddv24.html#linking-xwidehatbeta-and-y",
    "href": "limiteddv24.html#linking-xwidehatbeta-and-y",
    "title": "Binary \\(y\\) variables",
    "section": "Linking \\(x\\widehat{\\beta}\\) and \\(y^*\\)",
    "text": "Linking \\(x\\widehat{\\beta}\\) and \\(y^*\\)\nWe can produce the linear prediction, \\(x\\widehat{\\beta}\\), but we need to transform it to produce estimates of \\(y^*\\). To do so, we use a link function to map \\(x_i\\beta\\) onto the probability space, \\(y^*\\). This means \\(\\widehat{y_i} \\neq x\\widehat{\\beta}\\). Instead,\n\\[y^* = F(x_i\\beta)\\]\nWhere \\(F\\) is a continuous, sigmoid probability CDF. This is how we get estimates of our quantity of interest, \\(y^*\\)."
  },
  {
    "objectID": "limiteddv24.html#non-linear-change-in-pry1-across-values-of-x",
    "href": "limiteddv24.html#non-linear-change-in-pry1-across-values-of-x",
    "title": "Binary \\(y\\) variables",
    "section": "Non linear change in Pr(y=1) across values of \\(x\\)",
    "text": "Non linear change in Pr(y=1) across values of \\(x\\)\nIn the LPM, the relationship between \\(Pr(y=1)\\) and \\(X\\) is linear, so the rate of change toward \\(Pr(y=1)\\) is constant across all values of \\(X\\).\nThis means that the rate of change approaching one (or approaching zero) is exactly the same as the rate of change anywhere else in the distribution.\nFor example, this means that the change from .99 to 1.00 is just as likely as the change from .50 to .51; is this sensible for a bounded latent variable (probability)?"
  },
  {
    "objectID": "limiteddv24.html#linear-and-sigmoid-functions-at-limits",
    "href": "limiteddv24.html#linear-and-sigmoid-functions-at-limits",
    "title": "Binary \\(y\\) variables",
    "section": "Linear and Sigmoid Functions at Limits",
    "text": "Linear and Sigmoid Functions at Limits\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\nggplot() + \n  geom_line(aes(x=z, y=l), color=\"black\")+\n  geom_line(aes(x=z, y=s1), color=\"red\")+\n  geom_line(aes(x=z, y=s2), color=\"green\")+\n  labs(title=\"Linear and Sigmoid Functions\", x=\"z\", y=\"F(z)\")+\n  theme_minimal()+\n  annotate(\"text\", x=0, y=.75, label=\"Normal\", color=\"green\")+\n  annotate(\"text\", x=-3, y=.1, label=\"Logistic\", color=\"red\")"
  },
  {
    "objectID": "limiteddv24.html#non-constant-change-in-pry1-across-values-of-z",
    "href": "limiteddv24.html#non-constant-change-in-pry1-across-values-of-z",
    "title": "Binary \\(y\\) variables",
    "section": "Non constant change in Pr(y=1) across values of \\(z\\)",
    "text": "Non constant change in Pr(y=1) across values of \\(z\\)\nAnimating the change in \\(Pr(y=1)\\) across values of \\(z\\) for the linear and sigmoid functions.\n\n\ncode\nlibrary(gganimate)\n\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\")+\n  geom_line(aes(x=z, y=s1), color=\"red\")+\n  geom_line(aes(x=z, y=s2), color=\"green\")+\n  labs(title=\"Rates of change\", x=\"z\", y=\"F(z)\")+\n  theme_minimal()+\n  transition_reveal(z)"
  },
  {
    "objectID": "limiteddv24.html#binomial-log-likelihood-function",
    "href": "limiteddv24.html#binomial-log-likelihood-function",
    "title": "Binary \\(y\\) variables",
    "section": "Binomial Log-Likelihood Function}",
    "text": "Binomial Log-Likelihood Function}\nWrite the binomial in terms of data (so observations, \\(i\\) on \\(y\\)):\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nParameterizing - writing in terms of variables and effects, \\(x_i\\widehat{\\beta}\\):\n\\[ Pr(y=1| F(x\\widehat{\\beta})) = F(x_i\\widehat{\\beta})^{y_i} (1-F(x_i\\widehat{\\beta}))^{1-y_i} \\]"
  },
  {
    "objectID": "limiteddv24.html#binomial-llf",
    "href": "limiteddv24.html#binomial-llf",
    "title": "Binary \\(y\\) variables",
    "section": "Binomial LLF",
    "text": "Binomial LLF\nBut adding is easier than multiplying, so take the natural log of the whole thing:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\] \nin terms of \\(x_i\\widehat{\\beta}\\)\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]"
  },
  {
    "objectID": "limiteddv24.html#binomial-llf-1",
    "href": "limiteddv24.html#binomial-llf-1",
    "title": "Binary \\(y\\) variables",
    "section": "Binomial LLF",
    "text": "Binomial LLF\nBut adding is easier than multiplying, so take the natural log of the whole thing:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\] \nin terms of \\(x_i\\widehat{\\beta}\\)\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]"
  },
  {
    "objectID": "limiteddv24.html#binomial-llf-2",
    "href": "limiteddv24.html#binomial-llf-2",
    "title": "Binary \\(y\\) variables",
    "section": "Binomial LLF",
    "text": "Binomial LLF\nThis is the binomial log-likelihood function - we estimate it by maximum likelihood - just another technology (alongside OLS, Bayesian modeling, and others), for estimating unknowns.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "limiteddv24.html#link-function",
    "href": "limiteddv24.html#link-function",
    "title": "Binary \\(y\\) variables",
    "section": "Link Function",
    "text": "Link Function\nWe parameterized \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nand now need to choose an appropriate link function such that:\n\\(\\pi_i\\) is bounded [0,1]. \\(x_i \\widehat{\\beta}\\) can range \\(-\\infty, +\\infty\\) and map onto that [0,1] interval.\nA relatively large number of sigmoid shaped probability functions will satisfy these needs."
  },
  {
    "objectID": "limiteddv24.html#link-functions",
    "href": "limiteddv24.html#link-functions",
    "title": "Binary \\(y\\) variables",
    "section": "Link Functions",
    "text": "Link Functions\nWe parameterized \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nand now need to choose an appropriate link function such that:\n\\(\\pi_i\\) is bounded [0,1]. \\(x_i \\widehat{\\beta}\\) can range \\(-\\infty, +\\infty\\) and map onto that [0,1] interval.\nA relatively large number of sigmoid shaped probability functions will satisfy these needs."
  },
  {
    "objectID": "limiteddv24.html#probit-and-logit-llfs",
    "href": "limiteddv24.html#probit-and-logit-llfs",
    "title": "Binary \\(y\\) variables",
    "section": "Probit and Logit LLFs",
    "text": "Probit and Logit LLFs\nProbit - link between \\(x\\hat{\\beta}\\) and \\(Pr(y=1)\\) is standard normal CDF:\n\\[\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\]\nLogit looks like this:\n\\[\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\]"
  },
  {
    "objectID": "limiteddv24.html#which-link-function",
    "href": "limiteddv24.html#which-link-function",
    "title": "Binary \\(y\\) variables",
    "section": "Which link function?",
    "text": "Which link function?\nSo, between the logit and probit models, which do we choose?\nUnless we have theoretical expectations that inform us about the data in the tails of the distribution (we don’t), then it makes no difference which estimation procedure you select. Choose what you find most comfortable, most readily interpretable."
  },
  {
    "objectID": "limiteddv24.html#quantities-of-interest",
    "href": "limiteddv24.html#quantities-of-interest",
    "title": "Binary \\(y\\) variables",
    "section": "Quantities of Interest",
    "text": "Quantities of Interest\nIn the linear model, the main quantity of interest is \\(x_i\\hat{\\beta}\\). Here, we need \\(x_i\\hat{\\beta}\\), but then need to transform by the link function, \\(F\\):\n\n\\(F(x_i\\hat{\\beta})\\)\nMeasures of uncertainty about \\(F(x_i\\hat{\\beta})\\)"
  },
  {
    "objectID": "limiteddv24.html#confidence-intervals",
    "href": "limiteddv24.html#confidence-intervals",
    "title": "Binary \\(y\\) variables",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nEnd point transformation is straightforward as in the linear model:\n\nestimate the model.\ngenerate the linear prediction, \\(x\\hat{\\beta}\\).\ngenerate the standard error of the prediction\ngenerate linear predictions of the upper bound (\\(x\\hat{\\beta}+c*se_p\\)) and lower bound (\\(x\\hat{\\beta}-c*se_p\\)), where \\(c\\) is the critical value , \\(\\sim N(0,1)\\).\ngenerate the predictions at upper, lower, and central points by \\(F(x\\hat{\\beta}\\pm c*se_p)\\) and \\(F(x\\hat{\\beta})\\)\nplot the upper and lower quantities of interest, perhaps around the central point."
  },
  {
    "objectID": "limiteddv24.html#predictions",
    "href": "limiteddv24.html#predictions",
    "title": "Binary \\(y\\) variables",
    "section": "Predictions",
    "text": "Predictions\nWe do this exactly as in the linear model, with the one modification that we map the linear prediction, \\(x_i\\hat{\\beta}\\) onto the probability space by the link function. Here, for instance, is the at-means approach:\n\nestimate the model\ngenerate the linear prediction, \\(x\\beta\\), using the at-means data, varying the \\(x\\) of interest, and model estimates, \\(\\widehat{\\beta}\\).\ngenerate the standard errors of the linear predictions the usual way.\ngenerate upper and lower bounds of the linear prediction.\nmap those linear predictions and the boundaries onto \\(y^*\\), the latent probability space: \\(F(x \\widehat{\\beta})\\)\nplot the predictions and measures of uncertainty against the variable of interest."
  },
  {
    "objectID": "limiteddv24.html#transformation-by-f-logistic",
    "href": "limiteddv24.html#transformation-by-f-logistic",
    "title": "Binary \\(y\\) variables",
    "section": "Transformation by \\(F\\), Logistic",
    "text": "Transformation by \\(F\\), Logistic\nSo you’ve got the three quantities we’ve been working with all semester:\n\\(x\\beta\\) \\(x\\beta + 1.96*se\\) \\(x\\beta - 1.96*se\\)\nFor the logit, transform as\n\\[Pr(y=1) =  \\frac{1}{1+exp(-x\\beta)} \\] \\[ ub = \\frac{1}{1+exp(-(x\\beta + 1.96*se))} \\] \\[ lb = \\frac{1}{1+exp(-(x\\beta - 1.96*se))} \\]"
  },
  {
    "objectID": "limiteddv24.html#transformation-by-f-standard-normal",
    "href": "limiteddv24.html#transformation-by-f-standard-normal",
    "title": "Binary \\(y\\) variables",
    "section": "Transformation by \\(F\\), Standard Normal",
    "text": "Transformation by \\(F\\), Standard Normal\nSo you’ve got the three quantities we’ve been working with all semester:\n\\(x\\beta\\) \\(x\\beta + 1.96*se\\) \\(x\\beta - 1.96*se\\)\nFor the probit, transform as\n\\[Pr(y=1) =  \\Phi(x\\beta) \\] \\[ ub = \\Phi(x\\beta + 1.96*se) \\] \\[ lb = \\Phi(x\\beta - 1.96*se) \\]"
  },
  {
    "objectID": "limiteddv24.html#alternatively-ldots",
    "href": "limiteddv24.html#alternatively-ldots",
    "title": "Binary \\(y\\) variables",
    "section": "Alternatively \\(\\ldots\\)",
    "text": "Alternatively \\(\\ldots\\)\nFor example,\n\nestimate the model.\nsimulate the distribution of \\(\\widehat{\\beta}\\).\nidentify the 2.5 and 97.5 percentiles.\nuse those estimates for \\(\\widehat{\\beta}\\) and interesting values of \\(X\\) to generate linear predictions at upper and lower bounds.\ntransform these linear predictions \\(F(x \\widehat{\\beta})\\), mapping them onto the appropriate space.\nplot against \\(x\\)."
  },
  {
    "objectID": "interactions24.html#goals",
    "href": "interactions24.html#goals",
    "title": "Differential Intercepts and Interactions",
    "section": "",
    "text": "The models we’ve considered so far have generally assumed structural stability - that the intercepts and slopes are shared by all groups in the data.\nWe’ll consider two possibilities:\n\ngroups in the data have different intercepts - test this with group indicator variables (dummies).\ngroups in the data have different slopes with respect to the same variables - test this using multiplicative interactions"
  },
  {
    "objectID": "limiteddv24.html#binomial-likelihood-function",
    "href": "limiteddv24.html#binomial-likelihood-function",
    "title": "Binary \\(y\\) variables",
    "section": "Binomial Likelihood Function",
    "text": "Binomial Likelihood Function\nWrite the binomial in terms of data (so observations, \\(i\\) on \\(y\\)):\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nParameterizing - writing in terms of variables and effects, \\(x_i\\widehat{\\beta}\\):\n\\[ Pr(y=1| F(x\\widehat{\\beta})) = F(x_i\\widehat{\\beta})^{y_i} (1-F(x_i\\widehat{\\beta}))^{1-y_i} \\]"
  },
  {
    "objectID": "limiteddv24.html#binomial-likelihood-function-1",
    "href": "limiteddv24.html#binomial-likelihood-function-1",
    "title": "Binary \\(y\\) variables",
    "section": "Binomial Likelihood Function",
    "text": "Binomial Likelihood Function\nWe want to know the probability of observing all the data - so a joint probability:\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\\right]\\] \nin terms of \\(x_i\\widehat{\\beta}\\):\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ F(x_i\\widehat{\\beta}) ^{y_i} (1-F(x_i\\widehat{\\beta}))^{1-y_i}\\right]\\]"
  },
  {
    "objectID": "limiteddv24.html#binomial-log-likelihood",
    "href": "limiteddv24.html#binomial-log-likelihood",
    "title": "Binary \\(y\\) variables",
    "section": "Binomial Log-Likelihood",
    "text": "Binomial Log-Likelihood\nThis is the binomial log-likelihood function - we estimate it by maximum likelihood - just another technology (alongside OLS, Bayesian modeling, and others), for estimating unknowns.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "limiteddv24.html#link-functions-1",
    "href": "limiteddv24.html#link-functions-1",
    "title": "Binary \\(y\\) variables",
    "section": "Link Functions",
    "text": "Link Functions\nRemember, the link function maps or transforms the linear prediction on the sigmoid probability space, and obeys the bounds of 0,1.\nThe two main link functions you’ll ever see or use are the standard normal (probit)\n\\[Pr(y_i=1 | X) = \\Phi(x_i\\widehat{\\beta}) \\]\nand the logistic (logit) CDFs.\n\\[Pr(y_i=1 | X) = \\frac{1}{1+exp^{-(x_i\\widehat{\\beta})}} \\]"
  },
  {
    "objectID": "limiteddv24.html#transformation-by-f",
    "href": "limiteddv24.html#transformation-by-f",
    "title": "Binary \\(y\\) variables",
    "section": "Transformation by \\(F\\)",
    "text": "Transformation by \\(F\\)\nSo you’ve got the three quantities we’ve been working with all semester:\n\n\\(x\\widehat{\\beta}\\)\n\\(x\\widehat{\\beta} + 1.96*se\\)\n\\(x\\widehat{\\beta} - 1.96*se\\)\n\nAll transform the linear prediction and standard error of the linear prediction. For the logit, transform as\n\\[Pr(y=1) =  \\frac{1}{1+exp(-x\\beta)} \\] \\[ ub = \\frac{1}{1+exp(-(x\\beta + 1.96*se))} \\] \\[ lb = \\frac{1}{1+exp(-(x\\beta - 1.96*se))} \\]\nFor the probit, transform as\n\\[Pr(y=1) =  \\Phi(x\\beta) \\] \\[ ub = \\Phi(x\\beta + 1.96*se) \\] \\[ lb = \\Phi(x\\beta - 1.96*se) \\]\nAny of the prediction methods we’ve learned are the same here - just remember to transform the quantities by \\(F\\)."
  },
  {
    "objectID": "limiteddv24.html#average-effects-logit",
    "href": "limiteddv24.html#average-effects-logit",
    "title": "Binary \\(y\\) variables",
    "section": "Average effects (logit)",
    "text": "Average effects (logit)\nAverage effects are often a better choice because they represent the data more completely than central tendency can (in the at-mean effects). Here are average effects (using the logit estimates) across the range of polity, and for pairs of states that share borders and those that do not.\n\n\ncode\n#avg effects\n\n#identify the estimation sample\ndp$used &lt;- TRUE\ndp$used[na.action(m1)] &lt;- FALSE\ndpesample &lt;- dp %&gt;%  filter(used==\"TRUE\")\n\npolity &lt;- 0\nmedxbd0 &lt;- 0\nubxbd0 &lt;- 0\nlbxbd0 &lt;- 0\n# medse &lt;- 0\n# medxbd1 &lt;- 0\n# ubxbd1 &lt;- 0\n# lbxbd1 &lt;- 0\n\nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 0\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nnoborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 1\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nggplot() +\n  geom_ribbon(data=noborder, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=noborder, aes(x=polity, y=medxbd0))+\n  geom_ribbon(data=border, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=border, aes(x=polity, y=medxbd0))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Average Effects\")"
  },
  {
    "objectID": "limiteddv24.html#example---democratic-peace",
    "href": "limiteddv24.html#example---democratic-peace",
    "title": "Binary \\(y\\) variables",
    "section": "Example - Democratic Peace",
    "text": "Example - Democratic Peace\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n# column.labels=c(\"Model 1\", \"Model 2\"),\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,   star.cutoffs=c(0.05,0.01,0.001),  dep.var.caption=\"Militarized Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Low Polity Score\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nMilitarized Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nLow Polity Score\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nLog Likelihood\n\n\n-3,500.973\n\n\n-3,511.493\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05"
  },
  {
    "objectID": "limiteddv24.html#at-mean-predictions-logit",
    "href": "limiteddv24.html#at-mean-predictions-logit",
    "title": "Binary \\(y\\) variables",
    "section": "At-mean predictions (logit)",
    "text": "At-mean predictions (logit)\n\n\ncode\n#summary(m1)\n#confint(m1)\n\n#new data frame for MEM prediction\nmem &lt;- data.frame(deml= c(seq(-10,10,1)), \n                  border=0, caprat=median(dp$caprat), ally=0)\n\n# type=\"link\" produces the linear predictions; transform by hand below w/EPT\nmem  &lt;-data.frame(mem, predict(m1, type=\"link\", newdata=mem, se=TRUE))\n\nmem &lt;- cbind(mem,lb=plogis(mem$fit-1.96*mem$se.fit),\n             ub=plogis(mem$fit+1.96*mem$se.fit), \n             p=plogis(mem$fit))\n\nggplot(mem, aes(x=deml, y=p)) +\n  geom_line() +\n  geom_ribbon(data=mem, aes(x=deml, ymin=lb, ymax=ub),fill = \"grey70\", alpha = .4, ) +\n  labs(x=\"Polity Score\", y=\"Pr(Dispute) (95% confidence interval)\")"
  },
  {
    "objectID": "limiteddv24.html#simulation",
    "href": "limiteddv24.html#simulation",
    "title": "Binary \\(y\\) variables",
    "section": "Simulation",
    "text": "Simulation\nSimulation is an especially good approach for nonlinear models:\n\nestimate the model.\n\\(m\\) times (say, 1000 times), simulate the distribution of \\(\\widehat{\\beta}\\).\ngenerate the \\(m\\) linear predictions, \\(x\\widehat{\\beta}\\).\ntransform by the appropriate link function (logistic, standard normal CDF).\nidentify the 2.5, 50, and 97.5 percentiles.\nplot against \\(x\\).\n\n\n\ncode\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n#draws from multivariate normal using probit model estimates\nsimP &lt;- data.frame(MASS::mvrnorm(1000, coef(m2), vcov(m2)))\n\n#Logit predictions\nlogitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simL$X.Intercept.+ simL$border*0+simL$deml*(i-11)+simL$caprat*median(dp$caprat)+simL$ally*0, probs=c(.025, .5, .975))\nlogitprobs[i,] &lt;- data.frame(dem=i, lb=plogis(simpreds[1]), med=plogis(simpreds[2]), ub=plogis(simpreds[3]))\n}\n\n#Probit predictions\nprobitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simP$X.Intercept.+ simP$border*0+simP$deml*(i-11)+simP$caprat*median(dp$caprat)+simP$ally*0, probs=c(.025, .5, .975))\nprobitprobs[i,] &lt;- data.frame(dem=i, lb=pnorm(simpreds[1]), med=pnorm(simpreds[2]), ub=pnorm(simpreds[3]))\n}\n\nlogit &lt;- ggplot() +\n  geom_ribbon(data=logitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=logitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Logit Predictions\")\n\nprobit &lt;- ggplot() +\n  geom_ribbon(data=probitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=probitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Probit Predictions\")\n\nlogit+probit"
  },
  {
    "objectID": "limiteddv24.html#example---democratic-peace-data",
    "href": "limiteddv24.html#example---democratic-peace-data",
    "title": "Binary \\(y\\) variables",
    "section": "Example - Democratic Peace data",
    "text": "Example - Democratic Peace data\nWe’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a mililtarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n# column.labels=c(\"Model 1\", \"Model 2\"),\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,   star.cutoffs=c(0.05,0.01,0.001),  dep.var.caption=\"Militarized Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Low Polity Score\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nMilitarized Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nLow Polity Score\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nLog Likelihood\n\n\n-3,500.973\n\n\n-3,511.493\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nNotice there are no substantive differences between the logit and probit. As in the linear model, we can make statements about direction and significance looking at the model estimates. We can’t say much about magnitude because the coefficients are not transformed by \\(F\\) - this is where we turn to quantities of interest."
  },
  {
    "objectID": "limiteddv24.html#simulating-combinations-of-binary-variables",
    "href": "limiteddv24.html#simulating-combinations-of-binary-variables",
    "title": "Binary \\(y\\) variables",
    "section": "Simulating combinations of binary variables",
    "text": "Simulating combinations of binary variables\nLet’s look at the differences among the four combinations of the binary variables, border and ally.\n\n\ncode\n## simulating for binary combinations ----\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n\n\nlogitprobs &lt;- data.frame(b0a0= numeric(0) , b1a0=numeric(0), b0a1= numeric(0), b1a1=numeric(0))\n\n  b0a0 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb1a0 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb0a1 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\nb1a1 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\n\nlogitprobs &lt;- data.frame(b0a0, b1a0, b0a1, b1a1)\n\nggplot() +\n  geom_density(data=logitprobs, aes(x=b0a0), fill=\"grey70\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a0), fill=\"grey70\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b0a1), fill=\"grey70\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a1), fill=\"grey70\", alpha = .4, ) +\n  labs ( colour = NULL, x = \"Pr(Dispute)\", y =  \"Density\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = .07, y = 150, label = \"No border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .13, y = 70, label = \"Border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .04, y = 200, label = \"No border, allies\", color = \"black\") +\n  annotate(\"text\", x = .09, y = 50, label = \"Border, allies\", color = \"black\") +\n  ggtitle(\"Logit Predictions\")"
  },
  {
    "objectID": "limiteddv24.html#running-example---democratic-peace-data",
    "href": "limiteddv24.html#running-example---democratic-peace-data",
    "title": "Binary \\(y\\) variables",
    "section": "Running example - Democratic Peace data",
    "text": "Running example - Democratic Peace data\nAs a running example, I’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a mililtarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls.\n\nPredictions out of bounds\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nmols &lt;-lm(dispute ~ border+deml+caprat+ally, data=dp )\nolspreds &lt;- predict(mols)\n\ndf &lt;- data.frame(logitpreds, olspreds, dispute=as.factor(dp$dispute))\n\nggplot(df, aes(x=logitpreds, y=olspreds, color=dispute)) + \n  geom_point()+\n  labs(title=\"Predictions from Logit and OLS\", x=\"Logit Predictions\", y=\"OLS Predictions\")+\n  geom_hline(yintercept=0)+\n  theme_minimal() +\n  annotate(\"text\", x=.05, y=-.05, label=\"2,147 Predictions out of bounds\", color=\"red\")\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(df, aes(x=olspreds)) + \n  geom_density(alpha=.5)+\n  labs(title=\"Density of OLS Predictions\", x=\"Predictions\", y=\"Density\")+\n  theme_minimal()+\ngeom_vline(xintercept=0, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nHeteroskedastic Residuals\n\n\ncode\ndf &lt;- data.frame(df, mols$residuals)\n \nggplot(df, aes(x=mols.residuals, color=dispute)) + \n  geom_density()+\n  labs(title=\"Density of OLS Residuals\", x=\"Residuals\", y=\"Density\")+\n  theme_minimal()+\n  geom_vline(xintercept=0, linetype=\"dashed\")"
  },
  {
    "objectID": "limiteddv24.html#when-is-the-lpm-reasonable",
    "href": "limiteddv24.html#when-is-the-lpm-reasonable",
    "title": "Binary \\(y\\) variables",
    "section": "When is the LPM Reasonable?",
    "text": "When is the LPM Reasonable?"
  },
  {
    "objectID": "limiteddv24.html#example---modeling-the-democratic-peace",
    "href": "limiteddv24.html#example---modeling-the-democratic-peace",
    "title": "Binary \\(y\\) variables",
    "section": "Example - Modeling the Democratic Peace",
    "text": "Example - Modeling the Democratic Peace\nHere’s a model of the democratic peace.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n# column.labels=c(\"Model 1\", \"Model 2\"),\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,   star.cutoffs=c(0.05,0.01,0.001),  dep.var.caption=\"Militarized Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Low Polity Score\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nMilitarized Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nLow Polity Score\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nLog Likelihood\n\n\n-3,500.973\n\n\n-3,511.493\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nNotice there are no substantive differences between the logit and probit. As in the linear model, we can make statements about direction and significance looking at the model estimates. We can’t say much about magnitude because the coefficients are not transformed by \\(F\\) - this is where we turn to quantities of interest."
  },
  {
    "objectID": "interactions24.html#expected-values",
    "href": "interactions24.html#expected-values",
    "title": "Differential Intercepts and Interactions",
    "section": "Expected values",
    "text": "Expected values\n\n\ncode\n#  continuous/binary interaction ----\n# now, interact protests and restricted access\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1) %&gt;%\n  mutate(interact= civilwar*lagRA)\nm4 &lt;- lm(scarring ~  lagprotest + as.factor(lagRA) + lagprotest:lagRA +civilwar  + p1+p2+p3 +wdi_gdpc+wdi_pop, data=itt)\n#summary(m4)\n\n# average predictions, end point boundaries\n\n# estimation sample\nitt$used &lt;- TRUE\nitt$used[na.action(m4)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\n\n# loop over number of protests\npred_data &lt;-ittesample\nprotests &lt;-0\nmedxbr0 &lt;-0\nubxbr0 &lt;-0\nlbxbr0 &lt;-0\nmedxbr1 &lt;-0\nubxbr1 &lt;-0\nlbxbr1 &lt;-0\npred_data$lagRA &lt;- 0\nfor(p in seq(1,40,1)) {\n  pred_data$lagprotest &lt;- p \n  protests[p] &lt;- p \n  allpreds &lt;- data.frame(predict(m4, interval=\"confidence\", se.fit=TRUE, newdata = pred_data))  \n  medxbr0[p] &lt;- median(allpreds$fit.fit, na.rm=TRUE)\n  ubxbr0[p] &lt;- median(allpreds$fit.fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbr0[p] &lt;- median(allpreds$fit.fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\npred_data$lagRA &lt;- 1\nfor(p in seq(1,40,1))  {\n  pred_data$lagprotest &lt;- p\n  allpreds &lt;- data.frame(predict(m4, interval=\"confidence\", se.fit=TRUE, newdata = pred_data))  \n  medxbr1[p] &lt;- median(allpreds$fit.fit, na.rm=TRUE)\n  ubxbr1[p] &lt;- median(allpreds$fit.fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbr1[p] &lt;- median(allpreds$fit.fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n\ndf &lt;- data.frame(medxbr0, ubxbr0,lbxbr0,medxbr1, ubxbr1, lbxbr1, protests)\n\n\n#plotting\n\nggplot() +\n  geom_ribbon(data=df, aes(x=protests, ymin=lbxbr0, ymax=ubxbr0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=df, aes(x=protests, ymin=lbxbr1, ymax=ubxbr1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= df, aes(x=protests, y=medxbr0))+\n  geom_line(data= df, aes(x=protests, y=medxbr1))+\n  labs ( colour = NULL, x = \"Protests Against Government\", y =  \"Expected Scarring Torture Reports\" ) +\n  annotate(\"text\", x = 8, y = 30, label = \"Restricted Access\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 8, y = 11, label = \"Unrestricted Access\", size=3.5, colour=\"gray30\")"
  },
  {
    "objectID": "panel24.html",
    "href": "panel24.html",
    "title": "Panel Data",
    "section": "",
    "text": "So far, we’ve paid little attention to the structure of the data we analyze. It’s often the case our data have two dimensions:\n\nspace\ntime\n\nData that vary on both dimensions are known variously as panel, pooled, or cross-sectional time-series data. Panel or pooled data combine cross sections observed over time such that we observe each cross-section, each time period.\nThe two dimensional structure of panel data creates issues and opportunities we’ll explore here.\n\n\nModels using pooled or panel data must conform to the same i.i.d. assumptions as cross sectional or time series data, but now in two dimensions. So we might have serial correlation in each of \\(j\\) cross sections. Moreover, the cross sections may vary in unknown ways.\nIn this lecture, I want to introduce the structure of panel data, point to the problems that arise in it, and focus on some solutions.\n\n\n\nThe most common feature of pooled data is that there are unobserved effects across both space (across cross-section) and time (within cross-section), and their exclusion damages the estimates. The most common solutions to these problems are fixed and random effects. Let’s begin by thinking about the data.\n\n\n\nPanel or pooled data have repeated observations on the same set of cross-sectional units, so the data for a each cross-section \\(i\\) over \\(t\\) would look like this:\n\\[\n\\mathbf{y_i}=\n\\left[  \\begin{matrix}\ny_{i1}\\\\\n\\vdots\\\\\ny_{iT}  \\nonumber\n\\end{matrix}  \\right]\n\\mathbf{X_i}=\n\\left[  \\begin{matrix}\nX_{i1}^1 & X_{i1}^2 &\\cdots &X_{i1}^k\\\\\nX_{i2}^1& X_{i2}^2 &\\cdots &X_{i2}^k\\\\\n&&\\vdots\\\\\nX_{iT}^1 &X_{iT}^2 &\\cdots & X_{iT}^k \\nonumber\n\\end{matrix}  \\right]\n\\mathbf{\\epsilon_i}=\n\\left[ \\begin{matrix}\n\\epsilon_{i1}\\\\\n\\vdots\\\\\n\\epsilon_{iT}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nwhere the subscripts \\(i,T\\) indicate the \\(i^{th}\\) individual (cross-section) at time \\(T=t\\), and the superscript indicates the \\(k^{th}\\) regressor in the matrix.\nPanel data is stacked by cross-sectional unit and then by time, so\n\\[\n\\mathbf{y}=\n\\left[  \\begin{matrix}\ny_{11}\\\\\n\\vdots\\\\\ny_{1T} \\\\\ny_{21}\\\\\n\\vdots\\\\\ny_{2T} \\\\\n\\vdots\\\\\ny_{n1}\\\\\n\\vdots\\\\\ny_{nT}\\nonumber\n\\end{matrix}  \\right]\n\\mathbf{X}=\n\\left[  \\begin{matrix}\nX_{11}^1 & X_{11}^2 &\\cdots &X_{11}^k\\\\\nX_{12}^1& X_{12}^2 &\\cdots &X_{12}^k\\\\\n&&\\vdots\\\\\nX_{1T}^1 &X_{1T}^2 &\\cdots & X_{1T}^k \\\\\nX_{21}^1 & X_{21}^2 &\\cdots &X_{21}^k\\\\\nX_{22}^1& X_{22}^2 &\\cdots &X_{22}^k\\\\\n&&\\vdots\\\\\nX_{2T}^1 &X_{2T}^2 &\\cdots & X_{2T}^k \\\\\n\\vdots&\\vdots&\\vdots\\\\\nX_{n1}^1 & X_{n1}^2 &\\cdots &X_{n1}^k\\\\\nX_{n2}^1& X_{n2}^2 &\\cdots &X_{n2}^k\\\\\n&&\\vdots\\\\\nX_{nT}^1 &X_{nT}^2 &\\cdots & X_{nT}^k \\nonumber\n\\end{matrix}  \\right]\n\\mathbf{\\epsilon_i}=\n\\left[ \\begin{matrix}\n\\epsilon_{11}\\\\\n\\vdots\\\\\n\\epsilon_{1T} \\\\\n\\epsilon_{21}\\\\\n\\vdots\\\\\n\\epsilon_{2T} \\\\\n\\vdots\\\\\n\\epsilon_{n1}\\\\\n\\vdots\\\\\n\\epsilon_{nT}\\nonumber\n\\end{matrix}  \\right]\n\\]\n\n\n\nHere is an example of panel data, the US states over the years 1975-1993. The data contain measures of various crime rates, unemployment, per capita income, and other characteristics of the US states.\n\ncode\nstates &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L8_panel/code/USstatedata.dta\")\n\nest &lt;- states %&gt;% dplyr::select(statename, year, murder, unemp, prcapinc, south, hsdip)  %&gt;%\n  rename('murder rate' = murder,\n         'unemployment rate' = unemp,\n         'p/c income' = prcapinc,\n         'hs diploma %' = hsdip) %&gt;%\n  slice(10:25) \n  knitr::kable(est) \n\n\n\n\n\n\n\n\n\n\n\n\n\nstatename\nyear\nmurder rate\nunemployment rate\np/c income\nsouth\nhs diploma %\n\n\n\n\nAlabama\n1984\n9.4\n11.1\n10130.540\n1\n66.9\n\n\nAlabama\n1985\n9.8\n8.9\n10505.100\n1\n66.9\n\n\nAlabama\n1986\n10.1\n9.8\n11405.080\n1\n66.9\n\n\nAlabama\n1987\n9.3\n7.8\n12100.640\n1\n66.9\n\n\nAlabama\n1988\n9.9\n7.2\n12983.180\n1\n66.9\n\n\nAlabama\n1989\n10.2\n7.0\n14252.850\n1\n66.9\n\n\nAlabama\n1990\n11.6\n6.8\n14867.610\n1\n66.9\n\n\nAlabama\n1991\n11.5\n7.2\n15667.160\n1\n66.9\n\n\nAlabama\n1992\n11.0\n7.3\n16610.750\n1\n66.9\n\n\nAlabama\n1993\n11.6\n7.5\n17211.670\n1\n66.9\n\n\nAlaska\n1975\n12.2\n6.7\n9813.514\n0\n86.6\n\n\nAlaska\n1976\n11.3\n8.0\n10665.820\n0\n86.6\n\n\nAlaska\n1977\n10.8\n9.4\n9674.242\n0\n86.6\n\n\nAlaska\n1978\n12.9\n11.2\n10947.630\n0\n86.6\n\n\nAlaska\n1979\n13.3\n9.2\n11422.890\n0\n86.6\n\n\nAlaska\n1980\n9.7\n9.5\n12565.000\n0\n86.6"
  },
  {
    "objectID": "panel24.html#panel-data---varying-on-space-and-time",
    "href": "panel24.html#panel-data---varying-on-space-and-time",
    "title": "Panel Data",
    "section": "",
    "text": "Pooled or panel data must conform to the same i.i.d. assumptions as cross sectional or time series data, but now in two dimensions. So we might have serial correlation in each of \\(j\\) cross sections. Moreover, the cross sections may vary in unknown ways.\nIn this lecture, I want to introduce the structure of panel data, point to the problems that arise in it, and focus on some solutions."
  },
  {
    "objectID": "panel24.html#unobserved-heterogeneity",
    "href": "panel24.html#unobserved-heterogeneity",
    "title": "Panel Data",
    "section": "",
    "text": "The most common feature of pooled data is that there are unobserved effects across both space (across cross-section) and time (within cross-section), and their exclusion damages the estimates. The most common solutions to these problems are fixed and random effects. Let’s begin by thinking about the data."
  },
  {
    "objectID": "panel24.html#panels",
    "href": "panel24.html#panels",
    "title": "Panel Data",
    "section": "",
    "text": "Panel or pooled data have repeated observations on the same set of cross-sectional units, so the data for a each cross-section \\(i\\) over \\(t\\) would look like this:\n\\[\n\\mathbf{y_i}=\n\\left[  \\begin{matrix}\ny_{i1}\\\\\n\\vdots\\\\\ny_{iT}  \\nonumber\n\\end{matrix}  \\right]\n\\mathbf{X_i}=\n\\left[  \\begin{matrix}\nX_{i1}^1 & X_{i1}^2 &\\cdots &X_{i1}^k\\\\\nX_{i2}^1& X_{i2}^2 &\\cdots &X_{i2}^k\\\\\n&&\\vdots\\\\\nX_{iT}^1 &X_{iT}^2 &\\cdots & X_{iT}^k \\nonumber\n\\end{matrix}  \\right]\n\\mathbf{\\epsilon_i}=\n\\left[ \\begin{matrix}\n\\epsilon_{i1}\\\\\n\\vdots\\\\\n\\epsilon_{iT}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nwhere the subscripts \\(i,T\\) indicate the \\(i^{th}\\) individual (cross-section) at time \\(T=t\\), and the superscript indicates the \\(k^{th}\\) regressor in the matrix.\nPanel data is stacked by cross-sectional unit and then by time, so\n\\[\n\\mathbf{y}=\n\\left[  \\begin{matrix}\ny_{11}\\\\\n\\vdots\\\\\ny_{1T} \\\\\ny_{21}\\\\\n\\vdots\\\\\ny_{2T} \\\\\n\\vdots\\\\\ny_{n1}\\\\\n\\vdots\\\\\ny_{nT}\\nonumber\n\\end{matrix}  \\right]\n\\mathbf{X}=\n\\left[  \\begin{matrix}\nX_{11}^1 & X_{11}^2 &\\cdots &X_{11}^k\\\\\nX_{12}^1& X_{12}^2 &\\cdots &X_{12}^k\\\\\n&&\\vdots\\\\\nX_{1T}^1 &X_{1T}^2 &\\cdots & X_{1T}^k \\\\\nX_{21}^1 & X_{21}^2 &\\cdots &X_{21}^k\\\\\nX_{22}^1& X_{22}^2 &\\cdots &X_{22}^k\\\\\n&&\\vdots\\\\\nX_{2T}^1 &X_{2T}^2 &\\cdots & X_{2T}^k \\\\\n\\vdots&\\vdots&\\vdots\\\\\nX_{n1}^1 & X_{n1}^2 &\\cdots &X_{n1}^k\\\\\nX_{n2}^1& X_{n2}^2 &\\cdots &X_{n2}^k\\\\\n&&\\vdots\\\\\nX_{nT}^1 &X_{nT}^2 &\\cdots & X_{nT}^k \\nonumber\n\\end{matrix}  \\right]\n\\mathbf{\\epsilon_i}=\n\\left[ \\begin{matrix}\n\\epsilon_{11}\\\\\n\\vdots\\\\\n\\epsilon_{1T} \\\\\n\\epsilon_{21}\\\\\n\\vdots\\\\\n\\epsilon_{2T} \\\\\n\\vdots\\\\\n\\epsilon_{n1}\\\\\n\\vdots\\\\\n\\epsilon_{nT}\\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "panel24.html#panels-1",
    "href": "panel24.html#panels-1",
    "title": "Panel Data",
    "section": "",
    "text": "As usual, the model will produce a vector of coefficients, \\(\\beta_{(1,k)}\\). We could simply stack these data as illustrated above and estimate an OLS model, effectively ignoring the panel nature of the data. If we do so, we are assuming the model conforms to the GM assumptions, and specifically that the errors are identically and independently distributed across panels,\n\\[\n\\epsilon_{it} \\sim iid(0, \\sigma^2) \\nonumber\n\\]\nThis means that for any cross-section, the errors are not correlated, and that the errors are constant both across time (within individual) and across individuals.\nIf this were true, it would not really matter how we stacked the data, because none of the observations would be related to one another.\nWith panel data, it’s difficult to believe the errors are i.i.d. in the pooled OLS model."
  },
  {
    "objectID": "panel24.html#the-problem",
    "href": "panel24.html#the-problem",
    "title": "Panel Data",
    "section": "",
    "text": "In pooled cross sectional time series data (pooled, panel, tscs), we assume the errors are i.i.d.\nThey rarely are. Because of \\(\\ldots\\)"
  },
  {
    "objectID": "panel24.html#unobserved-heterogeneity-1",
    "href": "panel24.html#unobserved-heterogeneity-1",
    "title": "Panel Data",
    "section": "",
    "text": "Unobserved heterogeneity is that the units or time periods vary in ways we cannot directly model. If the units are US states, we might control for wealth, crime, political control, etc. but there are still non-random differences among the states we cannot measure. As a result, the model is misspecified."
  },
  {
    "objectID": "panel24.html#the-problem---unobserved-heterogeneity",
    "href": "panel24.html#the-problem---unobserved-heterogeneity",
    "title": "Panel Data",
    "section": "",
    "text": "All cross-sections share the same intercept.\n\n\ncode\nset.seed(8675309)\nX &lt;- data.frame(matrix(NA, nrow = 5, ncol = 0))\nX &lt;- X %&gt;% mutate(ind = row_number()*10)\nexpand_r &lt;- function(df, ...) {\n  as.data.frame(lapply(df, rep, ...))\n}\nX &lt;- expand_r(X, times = 4)\nX &lt;- data.frame(ind= sort(X$ind)) \n\nX &lt;- X %&gt;% mutate(x= row_number(), e=rnorm(nrow(X)))\n\nX &lt;- X %&gt;% mutate(y = ind + x  + e)\npool &lt;- lm(y ~ x , data = X) \n#summary(pool)\nintercepts &lt;- lm(y ~ x + as.factor(ind), data = X) \n#summary(intercepts)\n\npoolfit &lt;- predict(pool, X)\npredictions &lt;- data.frame(X, poolfit, predict(intercepts, interval = \"confidence\"))\n\n#shared intercept\nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + theme_minimal() + labs(title = \"Pooled Data\", subtitle = \"All cross-sections share the same intercept\", x=\"x\", y=\"y\") \n\n\n\n\n\n\n\n\n\nUnit specific intercepts.\n\n\ncode\n#different intercepts \nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + geom_point(aes(x=x, y=fit, group=ind)) + theme_minimal() + labs(title = \"Pooled Data\", subtitle = \"Different intercepts\", x=\"x\", y=\"y\")"
  },
  {
    "objectID": "panel24.html#unit-specific-intercepts-wrong-slope-simpsons-paradox",
    "href": "panel24.html#unit-specific-intercepts-wrong-slope-simpsons-paradox",
    "title": "Panel Data",
    "section": "Unit specific intercepts, wrong slope (Simpson’s paradox)",
    "text": "Unit specific intercepts, wrong slope (Simpson’s paradox)\nHere, by removing the between variation (via unit intercepts), we’ve changed the slope of the relationship. This is a form of Simpson’s paradox, where the relationship within each unit is different from the pooled relationship.\n\n\ncode\nX &lt;- X %&gt;% mutate(ind=ind*-1, y = ind + x  + e)\npool &lt;- lm(y ~ x , data = X)\n#summary(pool)\nintercepts &lt;- lm(y ~ x + as.factor(ind), data = X)\n#summary(intercepts)\n\npoolfit &lt;- predict(pool, X)\npredictions &lt;- data.frame(X, poolfit, predict(intercepts, interval = \"confidence\"))\n\n#different intercepts, wrong slope\nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + geom_point(aes(x=x, y=fit, group=ind)) + theme_minimal() + labs(title = \"Simpson's Paradox\", subtitle = \"Different intercepts, wrong slope\", x=\"x\", y=\"y\")"
  },
  {
    "objectID": "panel24.html#two-dimensions",
    "href": "panel24.html#two-dimensions",
    "title": "Panel Data",
    "section": "Two dimensions",
    "text": "Two dimensions\nPanel data have space (cross-section or unit) and time dimensions. Differences among cross-sections produce “between” variation, while differences over time create “within” variation.\nImagine that the cross-sectional or between variation has a different relationship to \\(y\\) than does the time-wise or within variation. The panel heterogeneity in this case is both two-dimensional and complex.\nHere are data on the US states over the years 1975-1993 - let’s look at the murder rate per 100,000 as it relates to assaults per 100,000 population. First, let’s just look at the pooled model, ignoring the panel structure; that is, we’re just using OLS on the panel data.\n\n\ncode\nstates &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L8_panel/code/USstatedata.dta\")\n\n# pooled model \n\nggplot(data=states, aes(x=assault, y=murder)) +\n  geom_point()+\n   guides(color = FALSE, label = FALSE) + \n  # scale_color_manual(values = c('black','blue','red','purple')) + \n  geom_smooth(method = 'lm', aes(color = NULL, label = NULL), se = FALSE) +\n  labs(x=\"Assaults per 100,000\", y=\"Murders per 100,000\")+\n  ggtitle(\"Pooled model\")\n\n\n\n\n\n\n\n\n\nNow, let’s define the two parts of the variation in panel data:\n\nbetween - refers to variation between units, but removing any timewise (within unit) differences; we might do this by taking the means of variables for each state. In the example below, you can see that between US states, the relationship between assault and murder is positive.\nwithin - refers to the variation over time within each unit, removing any differences among units; we can remove the between variation by taking means of variables over time. Below, you can see that within states (so over time), the relationship between assault and murder is negative.\n\nThe two panels below, again using murder and assault rates, break the within and between dimensions apart. The left panel is the between variation removing all the within variation; it’s the relationship of the unit means - i.e., the state mean of murder regressed on the state mean of assault (as if these were cross-sectional data). The right panel is the within variation, removing the between variation, the relationship of the year-means of murder and assault ignoring the cross-sectional differences.\n\n\ncode\n# between removing all within variation \nxstates &lt;- states %&gt;% group_by(statename) %&gt;% summarise(bmurder=mean(murder, na.rm=TRUE), bassault=mean(assault, na.rm=TRUE))\n\nb &lt;- ggplot(data=xstates, aes(x=bassault, y=bmurder)) +\n  geom_point()+\n  # guides(color = FALSE) + \n  # scale_color_manual(values = c('black','blue','red','purple')) + \n  geom_smooth(method = 'lm', aes(color = NULL, label = NULL), se = FALSE)+    \n  geom_text_repel(aes(label=statename),size=2.5, force=5) +\n  labs(x=\"Assaults per 100,000\", y=\"Murders per 100,000\")+\n  ggtitle(\"Between model\")\n\n# within removing all between\ntstates &lt;- states %&gt;% group_by(year) %&gt;% summarise(wmurder=mean(murder, na.rm=TRUE), wassault=mean(assault, na.rm=TRUE))\n\nw &lt;- ggplot(data=tstates, aes(x=wassault, y=wmurder)) +\n  geom_point()+\n  # guides(color = FALSE) + \n  # scale_color_manual(values = c('black','blue','red','purple')) + \n  geom_smooth(method = 'lm', aes(color = NULL, label = NULL), se = FALSE)+    \n  geom_text_repel(aes(label=year),size=2.5, force=5) +\n  labs(x=\"Assaults per 100,000\", y=\"Murders per 100,000\")+\n  ggtitle(\"Within model\")\n\n\nb + w  \n\n\n\n\n\n\n\n\n\nThe between relationship is positive; the within relationship is negative, illustrating the importance of thinking about the dimensionality of panel data. The unobserved heterogeneity in these two dimensions can produce some interesting possibilities."
  },
  {
    "objectID": "cv/LICENSE.html",
    "href": "cv/LICENSE.html",
    "title": "GNU General Public License",
    "section": "",
    "text": "Version 3, 29 June 2007\nCopyright © 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nThe GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers’ and authors’ protection, the GPL clearly explains that there is no warranty for this free software. For both users’ and authors’ sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users’ freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow.\n\n\n\n\n\n“This License” refers to version 3 of the GNU General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\na) The work must carry prominent notices stating that you modified it, and giving a relevant date.\nb) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\nc) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nd) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\na) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nb) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nc) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nd) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\ne) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\na) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nb) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nc) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nd) Limiting the use for publicity purposes of names of licensors or authors of the material; or\ne) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nf) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS\n\n\n\n\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type 'show c' for details.\nThe hypothetical commands show w and show c should show the appropriate parts of the General Public License. Of course, your program’s commands might be different; for a GUI interface, you would use an “about box”.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see &lt;http://www.gnu.org/licenses/&gt;.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read &lt;http://www.gnu.org/philosophy/why-not-lgpl.html&gt;."
  },
  {
    "objectID": "cv/LICENSE.html#preamble",
    "href": "cv/LICENSE.html#preamble",
    "title": "GNU General Public License",
    "section": "",
    "text": "The GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers’ and authors’ protection, the GPL clearly explains that there is no warranty for this free software. For both users’ and authors’ sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users’ freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow."
  },
  {
    "objectID": "cv/LICENSE.html#terms-and-conditions",
    "href": "cv/LICENSE.html#terms-and-conditions",
    "title": "GNU General Public License",
    "section": "",
    "text": "“This License” refers to version 3 of the GNU General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\na) The work must carry prominent notices stating that you modified it, and giving a relevant date.\nb) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\nc) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nd) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\na) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nb) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nc) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nd) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\ne) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\na) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nb) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nc) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nd) Limiting the use for publicity purposes of names of licensors or authors of the material; or\ne) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nf) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS"
  },
  {
    "objectID": "cv/LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "href": "cv/LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "title": "GNU General Public License",
    "section": "",
    "text": "If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type 'show c' for details.\nThe hypothetical commands show w and show c should show the appropriate parts of the General Public License. Of course, your program’s commands might be different; for a GUI interface, you would use an “about box”.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see &lt;http://www.gnu.org/licenses/&gt;.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read &lt;http://www.gnu.org/philosophy/why-not-lgpl.html&gt;."
  },
  {
    "objectID": "panel24.html#unobserved-heterogeneity-2",
    "href": "panel24.html#unobserved-heterogeneity-2",
    "title": "Panel Data",
    "section": "",
    "text": "All cross-sections share the same intercept.\n\n\ncode\nset.seed(8675309)\nX &lt;- data.frame(matrix(NA, nrow = 5, ncol = 0))\nX &lt;- X %&gt;% mutate(ind = row_number()*10)\nexpand_r &lt;- function(df, ...) {\n  as.data.frame(lapply(df, rep, ...))\n}\nX &lt;- expand_r(X, times = 4)\nX &lt;- data.frame(ind= sort(X$ind)) \n\nX &lt;- X %&gt;% mutate(x= row_number(), e=rnorm(nrow(X)))\n\nX &lt;- X %&gt;% mutate(y = ind + x  + e)\npool &lt;- lm(y ~ x , data = X) \n#summary(pool)\nintercepts &lt;- lm(y ~ x + as.factor(ind), data = X) \n#summary(intercepts)\n\npoolfit &lt;- predict(pool, X)\npredictions &lt;- data.frame(X, poolfit, predict(intercepts, interval = \"confidence\"))\n\n#shared intercept\nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + theme_minimal() + labs(title = \"Pooled Data\", subtitle = \"All cross-sections share the same intercept\", x=\"x\", y=\"y\") \n\n\n\n\n\n\n\n\n\nUnit specific intercepts.\n\n\ncode\n#different intercepts \nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + geom_point(aes(x=x, y=fit, group=ind)) + theme_minimal() + labs(title = \"Pooled Data\", subtitle = \"Different intercepts\", x=\"x\", y=\"y\")"
  },
  {
    "objectID": "panel24.html#the-pooled-model",
    "href": "panel24.html#the-pooled-model",
    "title": "Panel Data",
    "section": "The pooled model",
    "text": "The pooled model\nSimulating data, here again is the pooled model, ignoring unit or timewise heterogeneity, so assuming all share the same intercept and slope.\n\n\ncode\nset.seed(8675309)\nX &lt;- data.frame(matrix(NA, nrow = 5, ncol = 0))\nX &lt;- X %&gt;% mutate(ind = row_number()*10)\nexpand_r &lt;- function(df, ...) {\n  as.data.frame(lapply(df, rep, ...))\n}\nX &lt;- expand_r(X, times = 4)\nX &lt;- data.frame(ind= sort(X$ind)) \n\nX &lt;- X %&gt;% mutate(x= row_number(), e=rnorm(nrow(X)))\n\nX &lt;- X %&gt;% mutate(y = ind + x  + e)\npool &lt;- lm(y ~ x , data = X) \n#summary(pool)\nintercepts &lt;- lm(y ~ x + as.factor(ind), data = X) \n#summary(intercepts)\n\npoolfit &lt;- predict(pool, X)\npredictions &lt;- data.frame(X, poolfit, predict(intercepts, interval = \"confidence\"))\n\n#shared intercept\nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + theme_minimal() + labs(title = \"Pooled Data\", subtitle = \"All cross-sections share the same intercept\", x=\"x\", y=\"y\")"
  },
  {
    "objectID": "panel24.html#unit-specific-intercepts.",
    "href": "panel24.html#unit-specific-intercepts.",
    "title": "Panel Data",
    "section": "",
    "text": "Now, relax the assumption the intercepts are the same; this is like the within model insofar as it removes the between variation.\n\n\ncode\n#different intercepts \nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + geom_point(aes(x=x, y=fit, group=ind)) + theme_minimal() + labs(title = \"Pooled Data\", subtitle = \"Different intercepts\", x=\"x\", y=\"y\")"
  },
  {
    "objectID": "panel24.html#unit-specific-slopes-and-intercepts",
    "href": "panel24.html#unit-specific-slopes-and-intercepts",
    "title": "Panel Data",
    "section": "Unit specific slopes and intercepts",
    "text": "Unit specific slopes and intercepts\nTaking this one more step, removing the between variation may reveal different unit slopes and intercepts.\n\n\ncode\nX &lt;- X %&gt;% mutate(ix = ind*x, y = 1+ 13*ind + 2*x + 1.5*ix  + e)\nslopes &lt;- lm(y ~ x + ix + as.factor(ind) , data = X) \n#summary(slopes)\nnewpool &lt;- lm(y ~ x , data = X)\nnewpoolfit &lt;- predict(newpool, X)\npredictions &lt;- data.frame(X, newpoolfit, predict(slopes, interval = \"confidence\"))\n\nggplot(predictions, aes(x = x, y = newpoolfit)) +\n  geom_line(color=\"blue\") + geom_point(aes(x=x, y=fit, group=ind)) + theme_minimal() + labs(title = \"Unit Specific Slopes and Intercepts\", subtitle = \"Different intercepts, different slopes\", x=\"x\", y=\"y\")"
  },
  {
    "objectID": "panel24.html#dummy-variables-interactions",
    "href": "panel24.html#dummy-variables-interactions",
    "title": "Panel Data",
    "section": "",
    "text": "If this makes you think of our work on dummy variables and interactions, you’re doing well (if not, please review).\n\ndummy variables allow groups to have different intercepts.\ninteractions allow groups to have different slopes.\n\nExtend this logic to every panel; so each unit can have a different intercept, different slope, or both."
  },
  {
    "objectID": "panel24.html#fixedrandom-effects",
    "href": "panel24.html#fixedrandom-effects",
    "title": "Panel Data",
    "section": "",
    "text": "fixed and random effects models both seek to account for heterogeneity in the panel data, specifically for \\(i\\) or \\(t\\) heterogeneity.\nboth techniques contribute to model fit when we have essentially exhausted the theoretically derived sources of \\(y\\) we can include in the model.\nwhile these techniques can improve the specification of the model with respect to cross-sectional heterogeneity, they do not directly address either the heteroskedastic or autocorrelation problems, particularly the latter.\nwhile both of these issues can (and often do) arise from model misspecification, including these cross-sectional effects does not necessarily solve either problem.\n\nThese slides only address fixed effects."
  },
  {
    "objectID": "panel24.html#modeling-unit-heterogeneity",
    "href": "panel24.html#modeling-unit-heterogeneity",
    "title": "Panel Data",
    "section": "Modeling unit heterogeneity",
    "text": "Modeling unit heterogeneity\nThe first component of the error term, \\(\\alpha_i\\),\n\\[\\epsilon_{i,t} = \\alpha_{i} + \\eta_{i,t}\\]\nis what we’d like to model, thereby removing it from the error term. These are the individual or unit specific intercepts that capture unit heterogeneity. Estimating those intercepts removes \\(\\alpha_i\\) from the error term."
  },
  {
    "objectID": "panel24.html#fixed-effects---de-meaning",
    "href": "panel24.html#fixed-effects---de-meaning",
    "title": "Panel Data",
    "section": "Fixed Effects - de-meaning",
    "text": "Fixed Effects - de-meaning\nDe-meaning the data is a common approach to fixed effects estimation - this is what Stata’s ``xt’’ commands do. \\[\\begin{eqnarray}\ny_{i}= \\mathbf{X_{i}}\\mathbf{\\beta}+ \\mathbf{Z_i}\\delta+ \\epsilon_{i} \\nonumber \\\\ \\nonumber\n\\end{eqnarray}\\]\nIn this regression, \\(\\mathbf{X_{it}}\\) are the independent variables that vary across individuals and across time, and \\(\\mathbf{Z_i}\\) are independent variables that only vary across individuals, not across time.\nDe-Meaning\nRecall in simple linear model we have already shown this:\n\n\ncode\nm1 &lt;- lm(y ~ x, data = X)\nX &lt;- X %&gt;% mutate(ydemean = y - mean(y), xdemean = x - mean(x))\nm2 &lt;- lm(y ~ xdemean, data = X)\nm3 &lt;- lm(ydemean ~ xdemean, data = X)\nmodelsummary(list(m1, m2, m3))\n\n\n \n\n  \n    \n    \n    tinytable_w3diswqxtpp1hfhfv1aq\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  217.973 \n                  -960.351\n                  0.000   \n                \n                \n                             \n                  (63.753)\n                  (30.688)\n                  (30.688)\n                \n                \n                  x          \n                  -112.221\n                          \n                          \n                \n                \n                             \n                  (5.322) \n                          \n                          \n                \n                \n                  xdemean    \n                          \n                  -112.221\n                  -112.221\n                \n                \n                             \n                          \n                  (5.322) \n                  (5.322) \n                \n                \n                  Num.Obs.   \n                  20      \n                  20      \n                  20      \n                \n                \n                  R2         \n                  0.961   \n                  0.961   \n                  0.961   \n                \n                \n                  R2 Adj.    \n                  0.959   \n                  0.959   \n                  0.959   \n                \n                \n                  AIC        \n                  257.5   \n                  257.5   \n                  257.5   \n                \n                \n                  BIC        \n                  260.5   \n                  260.5   \n                  260.5   \n                \n                \n                  Log.Lik.   \n                  -125.760\n                  -125.760\n                  -125.760\n                \n                \n                  F          \n                  444.633 \n                  444.633 \n                  444.633 \n                \n                \n                  RMSE       \n                  130.20  \n                  130.20  \n                  130.20  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nIn panel data, suppose we transform all the variables in the model by the panel means - that is, we subtract the panel mean of each variable from each observation in the panel. This is called de-meaning the data."
  },
  {
    "objectID": "panel24.html#de-meaning---state-data",
    "href": "panel24.html#de-meaning---state-data",
    "title": "Panel Data",
    "section": "De-meaning - state data",
    "text": "De-meaning - state data\nTransform every variable in the regression by subtracting the unit-mean from each observation. Notice since \\(\\mathbf{Z_i}\\) is constant within individual, the mean of \\(\\mathbf{Z_i}\\) is equal to \\(\\mathbf{Z_i}\\), so these variables drop out of the regression.\n\\[y_{i}= \\mathbf{X_{i}}\\mathbf{\\beta}+ \\mathbf{Z_i}\\delta+ \\epsilon_{i} \\nonumber \\\\ \\nonumber \\\\\n\\bar{y_{i}}= \\bar{\\mathbf{X_{i}}}\\mathbf{\\beta}+ \\bar{\\mathbf{Z_i}\\delta}+ \\bar{\\epsilon_{i}} \\nonumber\\\\ \\nonumber \\\\\ny_{i}-\\bar{y_{i}}= (\\mathbf{X_{i}}-\\bar{\\mathbf{X_{i}}})\\mathbf{\\beta}+ (\\mathbf{Z_i}-\\bar{\\mathbf{Z_i}})\\delta+ ( \\epsilon_{i} -\\bar{\\epsilon_{i}})\\nonumber \\\\ \\nonumber \\\\\n  \\dot{y} = \\Delta   \\dot{\\mathbf{X}}\\beta +   \\Delta  \\dot{\\epsilon} \\]"
  },
  {
    "objectID": "panel24.html#how-de-meaning-works",
    "href": "panel24.html#how-de-meaning-works",
    "title": "Panel Data",
    "section": "How de-meaning works",
    "text": "How de-meaning works\nDe-meaning the data transforms the variables in the model by subtracting the panel means from each observation. The transformed variables are de-meaned such that the means of the transformed variables are zero.\nThe transformed variables are de-meaned such that the means of the transformed variables are zero.\n\\[\\dot{y} = \\Delta   \\dot{\\mathbf{X}}\\beta +   \\Delta  \\dot{\\epsilon} \\]\n\nDe-meaning regularizes the time-wise variation in cross-sections by measuring them relative to the panel means.\nWe’re controlling for the effects of individual cross-sections by using the cross-sections themselves as controls.\nWe’ve remeasured time-wise variation within each individual in a way that removes individual specific differences."
  },
  {
    "objectID": "panel24.html#lsdv",
    "href": "panel24.html#lsdv",
    "title": "Panel Data",
    "section": "LSDV",
    "text": "LSDV\nAn equivalent and common way to implement fixed effects is by LSDV - Least Squares with Dummy Variables - accomplished by including dummy variables for each individual. The coefficients in the dummy variables capture changes in the intercept for each individual.\n\nThe dummy variables are \\(\\mathbf{Z_i}\\) in the math above - but we don’t de-mean them out; we actually measure a coefficient for each \\(i\\).\nIn LSDV, we are explicitly measuring the individual heterogeneity, \\(\\alpha_i\\), thereby removing it from the error."
  },
  {
    "objectID": "panel24.html#lsdv-1",
    "href": "panel24.html#lsdv-1",
    "title": "Panel Data",
    "section": "LSDV",
    "text": "LSDV\nAn equivalent and common way to implement fixed effects is by LSDV - Least Squares with Dummy Variables - accomplished by including dummy variables for each individual. The coefficients in the dummy variables capture changes in the intercept for each individual.\n\nThe dummy variables are \\(\\mathbf{Z_i}\\) in the math above - but we don’t de-mean them out; we actually measure a coefficient for each \\(i\\).\nIn LSDV, we are explicitly measuring the individual heterogeneity, \\(\\alpha_i\\), thereby removing it from the error."
  },
  {
    "objectID": "panel24.html#fixed-effects",
    "href": "panel24.html#fixed-effects",
    "title": "Panel Data",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nLSDV and de-meaning are equivalent methods.\n\nLSDV uses lots of degrees of freedom, will sometimes produce perfect collinearity among unit intercepts. However, LSDV gives estimates of every unit intercept.\nDe-meaning (Stata’s ``xt’’ suite) is efficient, but doesn’t give these intercepts.\nAnother equivalent method is the absorbing regression where the variables are de-meaned as above, then the overall mean of each variable is added back. The unit effects are said to be absorbed."
  },
  {
    "objectID": "panel24.html#cautionary-note",
    "href": "panel24.html#cautionary-note",
    "title": "Panel Data",
    "section": "Cautionary Note",
    "text": "Cautionary Note\nPeople love fixed effects. Too much. Like anything else, these should be used in an informed and cautious way, not merely included in every panel regression. Issues to consider are:\n\nCan I measure unit/time differences better than with dummy variables?\nDo I have other variables in the model that only vary by \\(i\\) or by \\(t\\)? Collinearity?\nIn logit etc. models with panels, any panel where \\(y\\) is all zero will drop out of the regression. Green, Kim, and Yoon (2001) argue for (almost always) using fixed effects in conflict studies (panel data, binary \\(y\\) variable). Their advice is suspect at least because the models non-randomly lose all pairs of states that never fight."
  },
  {
    "objectID": "panel24.html#fixed-effects-and-endogeneity",
    "href": "panel24.html#fixed-effects-and-endogeneity",
    "title": "Panel Data",
    "section": "Fixed Effects and Endogeneity",
    "text": "Fixed Effects and Endogeneity\nIf \\(cov(X, \\epsilon) \\neq 0\\), we have an endogeneity problem. Endogeneity arises for different reasons - for our purposes here, the main thing is that the correlation of \\(X\\) and \\(\\epsilon\\) indicates endogeneity, and we already know some of the consequences.\nFixed effects are appropriate for the case where we think the individual errors are correlated with the \\(X\\) variables; modeling those individual errors, \\(\\mu_i\\) in any of the ways described above removes those effects from the error term, thus returning us to the blissful case where \\(cov(X, \\epsilon)=0\\).\nIn other words, fixed effects can serve as a (partial) solution to endogeneity."
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "CV",
    "section": "",
    "text": "Employment\nBinghamton University\nAssociate Dean for Undergraduate Studies, Harpur College of Arts and Sciences, June 2020 – present\nChair, Department of Political Science, 2006 – 2009; 2010 – 2013; 2016 – 2019.\nProfessor of Political Science, September 2016 – present.\nDirector of Graduate Studies, June 2004 – August 2006, Binghamton University.\nAssociate Professor of Political Science, Sept. 2004 – Aug. 2016, Binghamton University.\nAssistant Professor of Political Science, Sept. 2000 – Aug. 2004, Binghamton University.\nCornell University\nVisiting Scholar, Cornell University Department of Government, 2009 – 2010.\nUniversity of Iowa\nVisiting Assistant Professor of Political Science, 1999 – 2000\n\n\nEducation\nPh.D., 1999, Political Science Florida State University (1994–99)\nM.S., 1996, Political Science Florida State University\nB.A., 1990, Political Science The University of the South, Sewanee, TN\n\n\nJournal Articles\n\n\n\n\n\nData Sets\nMass Mobilization Data Project (sponsored by the Political Instability Task Force, CIA): data on protests against governments, global coverage, 1990-2019.\n\nMass Mobilization Project Website - \nMass Mobilization Dataverse - \n\nInstitutions and Elections Project (sponsored by the Political Instability Task Force, CIA): data on political institutions, practices, and election events, global coverage, 1972-2005.\nIAEP Website - \n\n\nGrants & Contracts\n\nPolitical Instability Task Force (Leidos/CIA), “Mass Mobilization Project” $58,000 2017-2020.\nPolitical Instability Task Force (Leidos/CIA), “Mass Mobilization Project” $183,000 2014-2016.\nPolitical Instability Task Force (Leidos/CIA), “Mass Mobilization Project” $37,000 Winter 2014.\nNational Science Foundation (Social and Behavioral Dimensions of National Security, Conflict, and Cooperation) “Collaborative Research: Experimental Analysis of Alternative Models of Conflict Bargaining.” with Charles Holt (UVA), Timothy Nordstrom (Ole Miss), William Reed (Maryland), and Katri Sieberg (Tampere University, Finland); $487,779, August 2009 - August 2012. [BCS #0904946]\nNational Science Foundation Doctoral Dissertation Research in Political Science: “Know Then Thyself, Domestic Delegation, Information, Leadership Uncertainty and Decisions during Crisis Bargaining.” (advisor for Aparna Kher) $10,524, September 2009 - August 2010. [SES #0921149]\nPolitical Instability Task Force (Science Applications International Corporation), “Institutions and Elections Project” with Pat Regan $165,000 September 2006-August 2007\nPolitical Instability Task Force (Science Applications International Corporation), “Institutions and Elections Project” with Pat Regan $133,775 January-August 2006\n\n\n\nEdited Volumes\nEmerging Methodology in the Quantitative Study of Conflict, Special Issue of the , 47(1), February 2003; coeditor with Patrick Regan.\n\n\nMedia\nThe Washington Post “Biden and other Democrats forced to adapt to pro-Palestinian protests” March 18, 2024. \nNewsweek “The Cost-of-Living Crisis Reaches Breaking Point” April 5, 2023. \nThe World (PRX, NPR) ``The profile of worldwide protests.’’ July 18, 2022. \nARD Radio, Germany ``Studie über Massenmobilisierung: Protest wird professioneller.’’ January 21, 2022. \nBBC News ``¿‘En qué se parecen (y en qué no) las protestas de Colombia a las que ocurrieron en Chile?’’ May 10, 2021. \nNational Geographic ``This expert has tracked 30 years of global protests. Here’s what he’s learned.’’ January 11, 2021. \n\n\nTechnical Reports & Presentations\n“Democratic Fragility, Democratic Resistance.” August 2023. PITF meeting, McLean, VA.\n“Instability and Mass Mobilization in 2021.” January 2021, Binghamton University. Report for the Political Instability Task Force.\n“Protest and Repression in the COVID-19 Era.” May 2020, Binghamton University. Report for the Political Instability Task Force.\n“Protest Dynamics in 2020.” January 2020, Binghamton University. Report for the Political Instability Task Force.\n\n\n\n\n\nReferences\nAvailable upon request."
  },
  {
    "objectID": "panel24.html#fixed-effects-within-model",
    "href": "panel24.html#fixed-effects-within-model",
    "title": "Panel Data",
    "section": "Fixed effects (within model)",
    "text": "Fixed effects (within model)\nThere are two general ways to estimate fixed effects models:\n\nLSDV (Least Squares with Dummy Variables)\nDe-meaning the data\n\n\nLSDV\nIf the unit specific error, \\(\\mu_i\\), is correlated with the \\(X\\) variables, a simple way to purge that unit heterogeneity from the error term is to estimate unit specific intercepts to measure the (unmodeled) differences among units. Do this by including a dummy variable for each unit - this is called the “least squares with dummy variables” (LSDV) model. It’s easy to implement and has the virtue of providing measures of cross sectional differences (in the unit intercepts), though it can be inefficient to estimate so many additional parameters.\nUsing the state data on murder rates, here are coefficients from the LSDV model regressing murder rate on state unemployment, per capita income, and dummy variables for each state (excluding the intercept so we get the whole set of state intercepts):\n\nlsdv &lt;- lm(murder ~ unemp + prcapinc +  factor(statename) -1 , data=states)\n\n\n\ncode\n#fixed effects, state data\n\nlsdv &lt;- lm(murder ~ unemp + prcapinc +  factor(statename) -1 , data=states)\npool &lt;- lm(murder ~ unemp +prcapinc , data=states)\n\n# coefficients as data frame for plotting\ncoefs&lt;- data.frame(coef(summary(lsdv)))\ncoefs$state &lt;- rownames(coefs)\ncoefs$state[coefs$state==\"unemp\"] &lt;- \".................Unemployment\"\ncoefs$state[coefs$state==\"prcapinc\"] &lt;- \".................Per capita income\"\ncoefs$state &lt;- substr(coefs$state, 18, nchar(coefs$state))\ncoefs$model &lt;- \"lsdv\"\n#coefs$sig &lt;-ifelse(coefs$`Pr...t..`&lt;.05, 1,0)\n\n#pooled coefficients \npcoefs&lt;- data.frame(coef(summary(pool)))\npcoefs$state &lt;- rownames(pcoefs)\npcoefs$state[pcoefs$state==\"unemp\"] &lt;- \"Unemployment\"\npcoefs$state[pcoefs$state==\"prcapinc\"] &lt;- \"Per capita income\"\npcoefs$state[pcoefs$state==\"(Intercept)\"] &lt;- \"Intercept\"\npcoefs$model &lt;- \"pool\"\n\n\ncoefs &lt;- rbind(coefs, pcoefs)\ncoefs &lt;- coefs %&gt;% mutate(var= ifelse((state==\"Unemployment\"|state==\"Per capita income\"|state==\"Intercept\"),1, 0))\n\n## plot fixed effects ----\n\np &lt;- ggplot(coefs, aes(x = Estimate, y = state, label = state, color=factor(model))) +\n  geom_point(size = 1) +\n  geom_errorbarh(aes(xmin = Estimate - 1.96 * Std..Error, xmax = Estimate + 1.96 * Std..Error)) +\n  geom_text_repel(data=coefs,  size=2.5, force=5) + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs ( colour = NULL, y = \"\", x =  \"Coefficients (Murder Rate)\" ) +\n  ggtitle(\"Fixed Effect Estimates\", subtitle = \"State Intercepts\") \n\np + scale_color_manual(values = c(\"red\", \"black\")) +\n  guides(color=\"none\")\n\n\n\n\n\n\n\n\n\nLSDV is inefficient, requiring that we estimate a lot of additional parameters, potentially inducing collinearity.\n\n\nDe-meaning\nAn equivalent method is to de-mean the data, removing the unit means from each unit. This is the approach most software packages take to fixed effects estimation.\n\\[y_{i}= \\mathbf{X_{i}}\\mathbf{\\beta}+ \\mathbf{Z_i}\\delta+ \\epsilon_{i} \\]\nWhen we transform every variable in the regression by subtracting the unit-mean from each observation, you’ll notice since \\(\\mathbf{Z_i}\\) is constant within individual, the mean of \\(\\mathbf{Z_i}\\) is equal to \\(\\mathbf{Z_i}\\), so these variables drop out of the regression.\n\\[y_{i}= \\mathbf{X_{i}}\\mathbf{\\beta}+ \\mathbf{Z_i}\\delta+ \\epsilon_{i}\\] \\[\\bar{y_{i}}= \\bar{\\mathbf{X_{i}}}\\mathbf{\\beta}+ \\bar{\\mathbf{Z_i}\\delta}+ \\bar{\\epsilon_{i}} \\]\n\\[y_{i}-\\bar{y_{i}}= (\\mathbf{X_{i}}-\\bar{\\mathbf{X_{i}}})\\mathbf{\\beta}+ (\\mathbf{Z_i}-\\bar{\\mathbf{Z_i}})\\delta+ ( \\epsilon_{i} -\\bar{\\epsilon_{i}})\\]\n\\[  \\dot{y} = \\Delta   \\dot{\\mathbf{X}}\\beta +   \\Delta  \\dot{\\epsilon} \\]\nIn this regression, \\(\\mathbf{X_{it}}\\) are the independent variables that vary across individuals and across time, and \\(\\mathbf{Z_i}\\) are independent variables that only vary across individuals, not across time. Looking at the US state data again:\n\n\ncode\nest &lt;- states %&gt;% dplyr::select(statename, year, murder, unemp, prcapinc, south, hsdip)  %&gt;%\n  rename('murder rate' = murder,\n         'unemployment rate' = unemp,\n         'p/c income' = prcapinc,\n         'hs diploma %' = hsdip) %&gt;%\n  slice(255:275) \n  knitr::kable(est) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatename\nyear\nmurder rate\nunemployment rate\np/c income\nsouth\nhs diploma %\n\n\n\n\nIndiana\n1982\n6.5\n11.9\n9947.301\n0\n75.6\n\n\nIndiana\n1983\n5.2\n11.1\n9943.196\n0\n75.6\n\n\nIndiana\n1984\n5.5\n8.6\n11823.890\n0\n75.6\n\n\nIndiana\n1985\n5.8\n7.9\n12408.550\n0\n75.6\n\n\nIndiana\n1986\n6.0\n6.7\n13158.820\n0\n75.6\n\n\nIndiana\n1987\n5.6\n6.4\n13964.920\n0\n75.6\n\n\nIndiana\n1988\n6.4\n6.8\n15066.420\n0\n75.6\n\n\nIndiana\n1989\n6.3\n6.0\n16598.301\n0\n75.6\n\n\nIndiana\n1990\n6.2\n5.3\n16827.561\n0\n75.6\n\n\nIndiana\n1991\n7.5\n5.9\n17281.820\n0\n75.6\n\n\nIndiana\n1992\n8.2\n6.5\n18478.590\n0\n75.6\n\n\nIndiana\n1993\n7.5\n5.3\n19222.221\n0\n75.6\n\n\nIowa\n1975\n2.5\n4.2\n5978.479\n0\n80.1\n\n\nIowa\n1976\n2.3\n4.0\n6135.675\n0\n80.1\n\n\nIowa\n1977\n2.3\n4.0\n7025.738\n0\n80.1\n\n\nIowa\n1978\n2.6\n4.0\n7713.258\n0\n80.1\n\n\nIowa\n1979\n2.2\n4.1\n8821.049\n0\n80.1\n\n\nIowa\n1980\n2.2\n5.7\n9521.620\n0\n80.1\n\n\nIowa\n1981\n2.6\n6.9\n10592.870\n0\n80.1\n\n\nIowa\n1982\n2.3\n8.5\n10899.480\n0\n80.1\n\n\nIowa\n1983\n2.3\n8.1\n10738.530\n0\n80.1\n\n\n\n\n\nNotice some variables vary over unit and time, and others only over unit; the latter drop out since they are constant within unit.\n\n\nDe-meaning\nRecall in the simple linear model we have already shown that demeaning \\(x\\) (model 2), demeaning \\(x\\) and \\(y\\) (model 3), both produce the same estimate for \\(\\beta_x\\) as the original model (model 1).\n\n\ncode\nm1 &lt;- lm(y ~ x, data = X)\nX &lt;- X %&gt;% mutate(ydemean = y - mean(y), xdemean = x - mean(x))\nm2 &lt;- lm(y ~ xdemean, data = X)\nm3 &lt;- lm(ydemean ~ xdemean, data = X)\nmodelsummary(list(m1, m2, m3))\n\n\n \n\n  \n    \n    \n    tinytable_08gpukg40cinmpvalump\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  217.973 \n                  -960.351\n                  0.000   \n                \n                \n                             \n                  (63.753)\n                  (30.688)\n                  (30.688)\n                \n                \n                  x          \n                  -112.221\n                          \n                          \n                \n                \n                             \n                  (5.322) \n                          \n                          \n                \n                \n                  xdemean    \n                          \n                  -112.221\n                  -112.221\n                \n                \n                             \n                          \n                  (5.322) \n                  (5.322) \n                \n                \n                  Num.Obs.   \n                  20      \n                  20      \n                  20      \n                \n                \n                  R2         \n                  0.961   \n                  0.961   \n                  0.961   \n                \n                \n                  R2 Adj.    \n                  0.959   \n                  0.959   \n                  0.959   \n                \n                \n                  AIC        \n                  257.5   \n                  257.5   \n                  257.5   \n                \n                \n                  BIC        \n                  260.5   \n                  260.5   \n                  260.5   \n                \n                \n                  Log.Lik.   \n                  -125.760\n                  -125.760\n                  -125.760\n                \n                \n                  F          \n                  444.633 \n                  444.633 \n                  444.633 \n                \n                \n                  RMSE       \n                  130.20  \n                  130.20  \n                  130.20  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\nDe-meaning - US state data\nIn panel data, suppose we transform all the variables in the model by the panel means - that is, we subtract the panel mean of each variable from each observation in the panel. This is called de-meaning the data. Below, we do this for all the variables in the state data model, and estimate by OLS. The table below compares:\n\nthe LSDV model, which includes dummy variables for each state.\nthe de-meaned model (computed by hand); subtract the unit (state) means from each observation.\nthe de-meaned model (within model) estimated by the plm package in R.\n\nYou’ll see the estimates are the same across the three models.\n\n\ncode\nstates &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L8_panel/code/USstatedata.dta\")\n\nest &lt;- states %&gt;% dplyr::select(id, year, murder, unemp, prcapinc, south, hsdip) \n\nest &lt;- est %&gt;% group_by(id) %&gt;% mutate(dmurder = murder-mean(murder, na.rm=TRUE), dunemp=unemp-mean(unemp, na.rm=TRUE), dprcapinc=prcapinc-mean(prcapinc, na.rm=TRUE), dhsdip=hsdip-mean(hsdip, na.rm=TRUE), dsouth = south-mean(south, na.rm=TRUE))\n\n#lsdv &lt;- lm(murder ~ unemp + prcapinc +  factor(id) -1 , data=est)\nde &lt;- lm(dmurder ~ dunemp + dprcapinc , data= est)\n\n# index=c(i,t)\nplmI &lt;- plm::plm(murder ~ unemp + prcapinc, model=\"within\" ,index=c(\"id\", \"year\"), data=states)\n\nmodels&lt;-list(\n  \"LSDV\" = lsdv,\n  \"De-meaned (by hand)\" = de,\n  \"De-meaned (plm)\" = plmI\n)\n\nmodelsummary(models, coef_omit = 3:52, gof_map = NA, fmt=4,\n             notes = list('(State intercepts omitted.)'))\n\n\n \n\n  \n    \n    \n    tinytable_w8td0zxe75hhagi0jp3w\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                LSDV\n                De-meaned (by hand)\n                De-meaned (plm)\n              \n        \n        (State intercepts omitted.)\n        \n                \n                  unemp      \n                  -0.1422 \n                          \n                  -0.1422 \n                \n                \n                             \n                  (0.0261)\n                          \n                  (0.0261)\n                \n                \n                  prcapinc   \n                  -0.0001 \n                          \n                  -0.0001 \n                \n                \n                             \n                  (0.0000)\n                          \n                  (0.0000)\n                \n                \n                  (Intercept)\n                          \n                  0.0000  \n                          \n                \n                \n                             \n                          \n                  (0.0414)\n                          \n                \n                \n                  dunemp     \n                          \n                  -0.1422 \n                          \n                \n                \n                             \n                          \n                  (0.0254)\n                          \n                \n                \n                  dprcapinc  \n                          \n                  -0.0001 \n                          \n                \n                \n                             \n                          \n                  (0.0000)\n                          \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nYour data is a mystery to your software\n\n\n\nRemember, estimation software knows nothing about your data. R does not know if your data are cross-sectional, time-series, panel, etc. The analyst has to identify the data structure in order to address any of these issues.\nFor example - the excellent plm library assumes the first two columns of your data are unit and time, respectively. If they’re not, R has no way of knowing. It’s essential to tell R what the panel structure is - in plm by writing index=c(“unit”, “time”).\n\n\n\n\nWhat fixed effects do\nFixed effects (whether by LSDV or demeaning) remove the unit specific effects from the error term, so that the error term is uncorrelated with the \\(X\\) variables. \\(\\mu_i\\) is now modeled, so no longer in the error; the error now only contains $_{i,t}. Fixed effects remove the between unit variation, leaving only the within unit variation.\n\n\nLSDV and de-meaning are equivalent methods.\n\nLSDV uses lots of degrees of freedom, will sometimes produce perfect collinearity among unit intercepts. However, LSDV gives estimates of every unit intercept.\nDe-meaning (the method used in R’s plm, feols, and others; Stata’s “xt” suite) is efficient, doesn’t estimate the intercepts.\nAnother equivalent method is the absorbing regression where the variables are de-meaned as above, then the overall mean of each variable is added back. The unit effects are said to be absorbed."
  },
  {
    "objectID": "panel24.html#de-meaning",
    "href": "panel24.html#de-meaning",
    "title": "Panel Data",
    "section": "De-meaning",
    "text": "De-meaning\nAn equivalent method is to de-mean the data, removing the unit means from each unit. This is the approach most software packages take to fixed effects estimation.\n\\[y_{i}= \\mathbf{X_{i}}\\mathbf{\\beta}+ \\mathbf{Z_i}\\delta+ \\epsilon_{i} \\]\nIn this regression, \\(\\mathbf{X_{it}}\\) are the independent variables that vary across individuals and across time, and \\(\\mathbf{Z_i}\\) are independent variables that only vary across individuals, not across time. Looking at the US state data again:\n\n\ncode\nest &lt;- states %&gt;% dplyr::select(statename, year, murder, unemp, prcapinc, south, hsdip)  %&gt;%\n  rename('murder rate' = murder,\n         'unemployment rate' = unemp,\n         'p/c income' = prcapinc,\n         'hs diploma %' = hsdip) %&gt;%\n  slice(255:275) \n  knitr::kable(est) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatename\nyear\nmurder rate\nunemployment rate\np/c income\nsouth\nhs diploma %\n\n\n\n\nIndiana\n1982\n6.5\n11.9\n9947.301\n0\n75.6\n\n\nIndiana\n1983\n5.2\n11.1\n9943.196\n0\n75.6\n\n\nIndiana\n1984\n5.5\n8.6\n11823.890\n0\n75.6\n\n\nIndiana\n1985\n5.8\n7.9\n12408.550\n0\n75.6\n\n\nIndiana\n1986\n6.0\n6.7\n13158.820\n0\n75.6\n\n\nIndiana\n1987\n5.6\n6.4\n13964.920\n0\n75.6\n\n\nIndiana\n1988\n6.4\n6.8\n15066.420\n0\n75.6\n\n\nIndiana\n1989\n6.3\n6.0\n16598.301\n0\n75.6\n\n\nIndiana\n1990\n6.2\n5.3\n16827.561\n0\n75.6\n\n\nIndiana\n1991\n7.5\n5.9\n17281.820\n0\n75.6\n\n\nIndiana\n1992\n8.2\n6.5\n18478.590\n0\n75.6\n\n\nIndiana\n1993\n7.5\n5.3\n19222.221\n0\n75.6\n\n\nIowa\n1975\n2.5\n4.2\n5978.479\n0\n80.1\n\n\nIowa\n1976\n2.3\n4.0\n6135.675\n0\n80.1\n\n\nIowa\n1977\n2.3\n4.0\n7025.738\n0\n80.1\n\n\nIowa\n1978\n2.6\n4.0\n7713.258\n0\n80.1\n\n\nIowa\n1979\n2.2\n4.1\n8821.049\n0\n80.1\n\n\nIowa\n1980\n2.2\n5.7\n9521.620\n0\n80.1\n\n\nIowa\n1981\n2.6\n6.9\n10592.870\n0\n80.1\n\n\nIowa\n1982\n2.3\n8.5\n10899.480\n0\n80.1\n\n\nIowa\n1983\n2.3\n8.1\n10738.530\n0\n80.1\n\n\n\n\n\nNotice some variables vary over unit and time, and others only over unit."
  },
  {
    "objectID": "panel24.html#lsdv-2",
    "href": "panel24.html#lsdv-2",
    "title": "Panel Data",
    "section": "LSDV",
    "text": "LSDV\nThe inclusion of these dummy variables for fixed effects is an admission of ignorance about the differences between individuals and simply an attempt to soak up individual-specific variation we’re not able to model from a more theoretically oriented vantage point.\n\ndemeaning works by removing the cross sectional differences that don’t vary over time.\nLSDV works by measuring the cross sectional differences that don’t vary over time.\nthese are equivalent."
  },
  {
    "objectID": "panel24.html#varying-on-space-and-time",
    "href": "panel24.html#varying-on-space-and-time",
    "title": "Panel Data",
    "section": "",
    "text": "Models using pooled or panel data must conform to the same i.i.d. assumptions as cross sectional or time series data, but now in two dimensions. So we might have serial correlation in each of \\(j\\) cross sections. Moreover, the cross sections may vary in unknown ways.\nIn this lecture, I want to introduce the structure of panel data, point to the problems that arise in it, and focus on some solutions."
  },
  {
    "objectID": "panel24.html#what-fixed-effects-do",
    "href": "panel24.html#what-fixed-effects-do",
    "title": "Panel Data",
    "section": "What fixed effects do",
    "text": "What fixed effects do\nFixed effects (whether by LSDV or demeaning) remove the unit specific effects from the error term, so that the error term is uncorrelated with the \\(X\\) variables. They remove the between unit variation, leaving only the within unit variation."
  },
  {
    "objectID": "panel24.html#lsdv-and-de-meaning-are-equivalent-methods.",
    "href": "panel24.html#lsdv-and-de-meaning-are-equivalent-methods.",
    "title": "Panel Data",
    "section": "LSDV and de-meaning are equivalent methods.",
    "text": "LSDV and de-meaning are equivalent methods.\n\nLSDV uses lots of degrees of freedom, will sometimes produce perfect collinearity among unit intercepts. However, LSDV gives estimates of every unit intercept.\nDe-meaning (the method used in R’s plm, feols, and others; Stata’s “xt” suite) is efficient, doesn’t estimate the intercepts.\nAnother equivalent method is the absorbing regression where the variables are de-meaned as above, then the overall mean of each variable is added back. The unit effects are said to be absorbed."
  },
  {
    "objectID": "panel24.html#real-data",
    "href": "panel24.html#real-data",
    "title": "Panel Data",
    "section": "",
    "text": "Here is an example of panel data, the US states over the years 1975-1993. The data contain measures of various crime rates, unemployment, per capita income, and other characteristics of the US states.\n\ncode\nstates &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L8_panel/code/USstatedata.dta\")\n\nest &lt;- states %&gt;% dplyr::select(statename, year, murder, unemp, prcapinc, south, hsdip)  %&gt;%\n  rename('murder rate' = murder,\n         'unemployment rate' = unemp,\n         'p/c income' = prcapinc,\n         'hs diploma %' = hsdip) %&gt;%\n  slice(10:25) \n  knitr::kable(est) \n\n\n\n\n\n\n\n\n\n\n\n\n\nstatename\nyear\nmurder rate\nunemployment rate\np/c income\nsouth\nhs diploma %\n\n\n\n\nAlabama\n1984\n9.4\n11.1\n10130.540\n1\n66.9\n\n\nAlabama\n1985\n9.8\n8.9\n10505.100\n1\n66.9\n\n\nAlabama\n1986\n10.1\n9.8\n11405.080\n1\n66.9\n\n\nAlabama\n1987\n9.3\n7.8\n12100.640\n1\n66.9\n\n\nAlabama\n1988\n9.9\n7.2\n12983.180\n1\n66.9\n\n\nAlabama\n1989\n10.2\n7.0\n14252.850\n1\n66.9\n\n\nAlabama\n1990\n11.6\n6.8\n14867.610\n1\n66.9\n\n\nAlabama\n1991\n11.5\n7.2\n15667.160\n1\n66.9\n\n\nAlabama\n1992\n11.0\n7.3\n16610.750\n1\n66.9\n\n\nAlabama\n1993\n11.6\n7.5\n17211.670\n1\n66.9\n\n\nAlaska\n1975\n12.2\n6.7\n9813.514\n0\n86.6\n\n\nAlaska\n1976\n11.3\n8.0\n10665.820\n0\n86.6\n\n\nAlaska\n1977\n10.8\n9.4\n9674.242\n0\n86.6\n\n\nAlaska\n1978\n12.9\n11.2\n10947.630\n0\n86.6\n\n\nAlaska\n1979\n13.3\n9.2\n11422.890\n0\n86.6\n\n\nAlaska\n1980\n9.7\n9.5\n12565.000\n0\n86.6"
  },
  {
    "objectID": "panel24.html#de-meaning-1",
    "href": "panel24.html#de-meaning-1",
    "title": "Panel Data",
    "section": "De-meaning",
    "text": "De-meaning\nRecall in the simple linear model we have already shown that demeaning \\(x\\) (model 2), demeaning \\(x\\) and \\(y\\) (model 3), both produce the same estimate for \\(\\beta_x\\) as the original model (model 1).\n\n\ncode\nm1 &lt;- lm(y ~ x, data = X)\nX &lt;- X %&gt;% mutate(ydemean = y - mean(y), xdemean = x - mean(x))\nm2 &lt;- lm(y ~ xdemean, data = X)\nm3 &lt;- lm(ydemean ~ xdemean, data = X)\nmodelsummary(list(m1, m2, m3))\n\n\n \n\n  \n    \n    \n    tinytable_08gpukg40cinmpvalump\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  217.973 \n                  -960.351\n                  0.000   \n                \n                \n                             \n                  (63.753)\n                  (30.688)\n                  (30.688)\n                \n                \n                  x          \n                  -112.221\n                          \n                          \n                \n                \n                             \n                  (5.322) \n                          \n                          \n                \n                \n                  xdemean    \n                          \n                  -112.221\n                  -112.221\n                \n                \n                             \n                          \n                  (5.322) \n                  (5.322) \n                \n                \n                  Num.Obs.   \n                  20      \n                  20      \n                  20      \n                \n                \n                  R2         \n                  0.961   \n                  0.961   \n                  0.961   \n                \n                \n                  R2 Adj.    \n                  0.959   \n                  0.959   \n                  0.959   \n                \n                \n                  AIC        \n                  257.5   \n                  257.5   \n                  257.5   \n                \n                \n                  BIC        \n                  260.5   \n                  260.5   \n                  260.5   \n                \n                \n                  Log.Lik.   \n                  -125.760\n                  -125.760\n                  -125.760\n                \n                \n                  F          \n                  444.633 \n                  444.633 \n                  444.633 \n                \n                \n                  RMSE       \n                  130.20  \n                  130.20  \n                  130.20  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\nDe-meaning - state data\nIn panel data, suppose we transform all the variables in the model by the panel means - that is, we subtract the panel mean of each variable from each observation in the panel. This is called de-meaning the data. Below, we do this for all the variables in the state data model, and estimate by OLS. The table below compares:\n\nthe LSDV model, which includes dummy variables for each state.\nthe de-meaned model (computed by hand), which subtracts the state means from each observation.\nthe de-meaned model (within model) estimated by the plm package in R.\n\nYou’ll see the estimates are the same across the three models.\n\n\ncode\nstates &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L8_panel/code/USstatedata.dta\")\n\nest &lt;- states %&gt;% dplyr::select(id, year, murder, unemp, prcapinc, south, hsdip) \n\nest &lt;- est %&gt;% group_by(id) %&gt;% mutate(dmurder = murder-mean(murder, na.rm=TRUE), dunemp=unemp-mean(unemp, na.rm=TRUE), dprcapinc=prcapinc-mean(prcapinc, na.rm=TRUE), dhsdip=hsdip-mean(hsdip, na.rm=TRUE), dsouth = south-mean(south, na.rm=TRUE))\n\n#pool &lt;- lm(murder ~ unemp + prcapinc , data=est)\nlsdv &lt;- lm(murder ~ unemp + prcapinc +  factor(id) -1 , data=est)\nde &lt;- lm(dmurder ~ dunemp + dprcapinc , data= est)\n\n# index=c(i,t)\nplmP &lt;- plm::plm(murder ~ unemp + prcapinc, model=\"pooling\" , data=est)\nplmI &lt;- plm::plm(murder ~ unemp + prcapinc, model=\"within\" ,index=c(\"id\", \"year\"), data=est)\nplmT &lt;- plm::plm(murder ~ unemp + prcapinc, model=\"between\" , data=est)\n\nmodels&lt;-list(\n  \"LSDV\" = lsdv,\n  \"De-meaned (by hand)\" = de,\n  \"De-meaned (plm)\" = plmI\n)\n\nmodelsummary(models, coef_omit = 3:52, gof_map = NA, fmt=4,\n             notes = list('(State intercepts omitted.)'))\n\n\n \n\n  \n    \n    \n    tinytable_w8td0zxe75hhagi0jp3w\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                LSDV\n                De-meaned (by hand)\n                De-meaned (plm)\n              \n        \n        (State intercepts omitted.)\n        \n                \n                  unemp      \n                  -0.1422 \n                          \n                  -0.1422 \n                \n                \n                             \n                  (0.0261)\n                          \n                  (0.0261)\n                \n                \n                  prcapinc   \n                  -0.0001 \n                          \n                  -0.0001 \n                \n                \n                             \n                  (0.0000)\n                          \n                  (0.0000)\n                \n                \n                  (Intercept)\n                          \n                  0.0000  \n                          \n                \n                \n                             \n                          \n                  (0.0414)\n                          \n                \n                \n                  dunemp     \n                          \n                  -0.1422 \n                          \n                \n                \n                             \n                          \n                  (0.0254)\n                          \n                \n                \n                  dprcapinc  \n                          \n                  -0.0001 \n                          \n                \n                \n                             \n                          \n                  (0.0000)\n                          \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWhen we transform every variable in the regression by subtracting the unit-mean from each observation, you’ll notice since \\(\\mathbf{Z_i}\\) is constant within individual, the mean of \\(\\mathbf{Z_i}\\) is equal to \\(\\mathbf{Z_i}\\), so these variables drop out of the regression.\n\\[y_{i}= \\mathbf{X_{i}}\\mathbf{\\beta}+ \\mathbf{Z_i}\\delta+ \\epsilon_{i}\\] \\[\\bar{y_{i}}= \\bar{\\mathbf{X_{i}}}\\mathbf{\\beta}+ \\bar{\\mathbf{Z_i}\\delta}+ \\bar{\\epsilon_{i}} \\]\n\\[y_{i}-\\bar{y_{i}}= (\\mathbf{X_{i}}-\\bar{\\mathbf{X_{i}}})\\mathbf{\\beta}+ (\\mathbf{Z_i}-\\bar{\\mathbf{Z_i}})\\delta+ ( \\epsilon_{i} -\\bar{\\epsilon_{i}})\\]\n\\[  \\dot{y} = \\Delta   \\dot{\\mathbf{X}}\\beta +   \\Delta  \\dot{\\epsilon} \\]\n\n\n\n\n\n\nYour data is a mystery to your software\n\n\n\nRemember, estimation software knows nothing about your data. R does not know if your data are cross-sectional, time-series, panel, etc. The analyst has to identify the data structure in order to address any of these issues."
  },
  {
    "objectID": "panel24.html#unit-specific-intercepts",
    "href": "panel24.html#unit-specific-intercepts",
    "title": "Panel Data",
    "section": "Unit specific intercepts",
    "text": "Unit specific intercepts\nNow, relax the assumption the intercepts are the same; this is like the within model insofar as it removes the between variation.\n\n\ncode\n#different intercepts \nggplot(predictions, aes(x = x, y = poolfit)) +\n  geom_line(color=\"blue\") + geom_point(aes(x=x, y=fit, group=ind)) + theme_minimal() + labs(title = \"Pooled Data\", subtitle = \"Different intercepts\", x=\"x\", y=\"y\")"
  },
  {
    "objectID": "panel24.html#unit-heterogeneity-in-panel-data",
    "href": "panel24.html#unit-heterogeneity-in-panel-data",
    "title": "Panel Data",
    "section": "Unit Heterogeneity in Panel Data",
    "text": "Unit Heterogeneity in Panel Data\nSuppose the basic regression model,\n\\[y_{it}= \\widehat{\\beta_0} + \\widehat{\\beta_1} + u_{it}\\]\nSuppose the residual, \\(u\\) is comprised of two components,\n\\[u_{it} = \\mu_{i} + \\epsilon_{it}\\]\n\\(\\epsilon\\) is the random disturbance, and say it meets the Gauss-Markov assumptions; it is also uncorrelated with the \\(X\\) variables and with the unit errors, \\(\\mu_i\\).\nThe unit errors, \\(\\mu_i\\), are the individual effects resulting from what we fail to account for in the model for each individual, \\(i\\). This part of the disturbance varies between units or individuals but not within unit (so not over time - note the subscript).\nIf \\(\\mu_i\\) is correlated with the \\(X\\) variables, \\(\\widehat{\\beta}\\) will be biased - this is the fixed effects model. \n\n\n\n\n\n\nFixed effects addresses omitted variable bias\n\n\n\nIf \\(\\mu_i\\) is correlated with the \\(X\\) variables, we have the fixed effects model. The problem we’re addressing is omitted variable bias. The unobserved heterogeneity among panels appears in the error term, \\(\\mu_i\\). This suggests the problem is really about measurement and model specification - if we can measure the unit heterogeneity and include it in the model, we purge it from the error term.\n\n\nThis is the motivation for the fixed effects model - this is the “within model” illustrated above where we remove (or control for) the between variation."
  },
  {
    "objectID": "panel24.html#random-effects",
    "href": "panel24.html#random-effects",
    "title": "Panel Data",
    "section": "Random Effects",
    "text": "Random Effects\nRecall we partitioned the residual in the panel data model into two parts:\n\\[u_{it} = \\mu_{i} + \\epsilon_{it}\\]\nand we motivated the fixed effects model by saying \\(\\mu_i\\) is correlated with the \\(X\\) variables, so amounting to an ommited variable problem. If \\(\\mu_i\\) is correlated with the \\(X\\) variables, estimates of \\(\\widehat{\\beta}\\) will be biased.\nThe random effects model assumes \\(\\mu_i\\) is uncorrelated with the \\(X\\) variables, so is itself a random variable in the error term - in this case, the \\(\\widehat{\\beta}s\\) are not biased, but the standard errors are inefficient. Unlike fixed effects, random effects do not model \\(\\mu_i\\), but account for it in computing the standard errors, thereby addressing the inefficiency."
  },
  {
    "objectID": "panel24.html#which-should-i-use",
    "href": "panel24.html#which-should-i-use",
    "title": "Panel Data",
    "section": "Which should I use?",
    "text": "Which should I use?\nSome authors (e.g. Bailey (2016)) pretty strongly prefer fixed effects, in part because it’s difficult to conceive of when/why \\(\\mu_i\\) would be uncorrelated with the \\(X\\) variables, and in part because fixed effects help address endogenity issues - i.e., the case where \\(cov(X, \\mu_i) \\neq 0\\).\nWooldridge (2013) suggests that the random effects model can be useful where some (large) proportion of the \\(x\\) variables are constant within unit, but vary across units - that is, they’re time-invariant. These, of course, are the variables that will drop out of the fixed effects model.\n\nHausman test\nThe Hausman test is a test the equivalence of the fixed and random effects coefficient vectors. If the two are the same (we fail to reject the null), we prefer random effects; if we do reject the null (the coefficient vectors are different), we prefer fixed effects.\n\n\ncode\n# fixed vs random effects\n\nFE &lt;- plm::plm(murder ~ unemp + prcapinc, model=\"within\" ,index=c(\"id\", \"year\"), data=states)\nRE &lt;- plm::plm(murder ~ unemp + prcapinc, model=\"random\" ,index=c(\"id\", \"year\"), data=states)\n\nphtest(FE,RE)\n\n\n\n    Hausman Test\n\ndata:  murder ~ unemp + prcapinc\nchisq = 8.7476, df = 2, p-value = 0.0126\nalternative hypothesis: one model is inconsistent\n\n\nIn this case, we’d prefer the fixed effects model. Rejecting the null suggests the random effect assumption (that the error and \\(X\\) are uncorrelated) is false. Failure to reject the null could indicate (per Wooldridge (2013)) there’s not enough information in the data to produce precise estimates - note this is not much of an endorsement of random effects so much as a question about how much we’re asking of the data.\nIn the end, Wooldridge (2013) writes, “FE is almost always much more convincing that RE for policy analysis using aggregated data.” That said, we should be careful to evaluate whether the model sufficiently captures “between” effects without FE to make FE/RE unnecessary. This is also straightforward to do via F-test between the FE and pooled models."
  },
  {
    "objectID": "panel24.html#references",
    "href": "panel24.html#references",
    "title": "Panel Data",
    "section": "References",
    "text": "References\n\n\nBailey, Michael A. 2016. Real Stats: Using Econometrics for Political Science and Public Policy. Oxford University Press.\n\n\nGreen, Donald P, Soo Yeon Kim, and David H Yoon. 2001. “Dirty Pool.” International Organization 55 (2): 441–68.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  }
]