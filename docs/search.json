[
  {
    "objectID": "probability24.html",
    "href": "probability24.html",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns.\n\n\n\n\nProbability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value).\n\n\n\n\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita.\n\n\n\n\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely.\n\n\n\n\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war.\n\n\n\n\nThese four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information.\n\n\n\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24.html#why-probability-distributions",
    "href": "probability24.html#why-probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns."
  },
  {
    "objectID": "probability24.html#probability-distributions",
    "href": "probability24.html#probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Probability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?"
  },
  {
    "objectID": "probability24.html#things-we-want-to-know-about-x",
    "href": "probability24.html#things-we-want-to-know-about-x",
    "title": "Basic probability",
    "section": "",
    "text": "\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value)."
  },
  {
    "objectID": "probability24.html#types-of-variables",
    "href": "probability24.html#types-of-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Variables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "probability24.html#discrete-variables",
    "href": "probability24.html#discrete-variables",
    "title": "Basic probability",
    "section": "",
    "text": "May be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "probability24.html#continuous-variables",
    "href": "probability24.html#continuous-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Can be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "probability24.html#levels-of-measurement",
    "href": "probability24.html#levels-of-measurement",
    "title": "Basic probability",
    "section": "",
    "text": "These four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "probability24.html#levels-of-measurement-and-models",
    "href": "probability24.html#levels-of-measurement-and-models",
    "title": "Basic probability",
    "section": "",
    "text": "In general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24.html#pdf-and-cdf",
    "href": "probability24.html#pdf-and-cdf",
    "title": "Basic probability",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nProbability distributions can be described by\n\nProbability Density Function (PDF) which maps \\(X\\) onto the probability space describing the probabilities of every value of \\(X\\), \\(x\\).\nCumulative Distribution Function (CDF) which maps \\(X\\) onto the probability space describing the probability \\(X\\) is less than some value, \\(x\\)."
  },
  {
    "objectID": "probability24.html#pdf-density",
    "href": "probability24.html#pdf-density",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nDefinition\n\n\n\nThe Probability Density Function or PDF describes the probability a random variable takes on a particular value, and it does so for all values of that random variable."
  },
  {
    "objectID": "probability24.html#pdf-density-1",
    "href": "probability24.html#pdf-density-1",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nExample\n\n\n\nSuppose we toss a fair coin 1 time and want to describe the probability distribution of the outcome, \\(Y\\) which is a random variable. The possible outcomes are heads and tails, and the probability of each is .5. This is the PDF because it describes the probabilities associated with all possible outcomes of \\(Y\\)."
  },
  {
    "objectID": "probability24.html#pdf-density-2",
    "href": "probability24.html#pdf-density-2",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\nWhile establishing the probability of a value of a discrete variable is possible, establishing the probability of a particular value of a continuous variable is not.\nInstead, the continuous PDF describes the instantaneous rate of change in \\(Pr(X=x)\\) for every value of \\(x\\)."
  },
  {
    "objectID": "probability24.html#pdf-plots",
    "href": "probability24.html#pdf-plots",
    "title": "Basic probability",
    "section": "PDF plots",
    "text": "PDF plots"
  },
  {
    "objectID": "probability24.html#cdf",
    "href": "probability24.html#cdf",
    "title": "Basic probability",
    "section": "CDF",
    "text": "CDF\n\n\n\n\n\n\nDefinition\n\n\n\nFor a random variable, \\(Y\\), the probability \\(Y\\) is less than or equal to some particular value, \\(y\\) in the range of \\(Y\\) defines the cumulative density function.\nFor a continuous variable:\n\\[P(Y \\leq j) =\\int\\limits_{-\\infty}^{j} f(X)dX\\]\nFor a discrete variable:\n\\[P(Y \\leq j) = \\sum\\limits_{y\\leq j} P(Y=y)\n= 1- \\sum\\limits_{y&gt; j} P(Y=y)\\]"
  },
  {
    "objectID": "probability24.html#cdf-plots",
    "href": "probability24.html#cdf-plots",
    "title": "Basic probability",
    "section": "CDF plots",
    "text": "CDF plots"
  },
  {
    "objectID": "probability24.html#notation",
    "href": "probability24.html#notation",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\\(f(x)\\) denotes the PDF of \\(x\\).\n\\(F(x)\\) denotes the CDF of \\(x\\).\nSubstitute either the appropriate name, symbol, or function for \\(F\\) or \\(f\\) to indicate the distribution.\nLower case Greek letters denote PDFs: e.g. \\(f(x)= \\phi(x)\\) denotes the normal PDF.\nUpper case Greek letters denote CDFs: e.g. \\(F(x)=\\Phi(x)\\) denotes the normal CDF."
  },
  {
    "objectID": "probability24.html#notation-1",
    "href": "probability24.html#notation-1",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nExample\n\n\n\n\\(x\\) is distributed Normal, \\(N(\\mu, \\sigma^{2})\\):\n\\[F(x) = \\Phi_{\\mu, \\sigma^{2}}(x)  =    \\int  \\phi_{\\mu, \\sigma^{2}} (x) f(x)d(x) \\]\n\\[ f(x) = \\phi_{\\mu, \\sigma^{2}}(x) =\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{(x-\\mu)^{2}}{2 \\sigma^{2}} \\right) \\]\nNote the first derivative of the CDF is the PDF; the integral of the PDF is the CDF."
  },
  {
    "objectID": "probability24.html#bernoulli",
    "href": "probability24.html#bernoulli",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nSuppose a binary variable \\(x\\) that takes on only the values of zero and one (\\(x \\in {0, 1}\\)):\n\\[\nPr(X=1)=\\pi\\nonumber \\\\\nPr(x=0)= 1-Pr(x=1) \\\\\n= 1-\\pi   \n\\]"
  },
  {
    "objectID": "probability24.html#bernoulli-1",
    "href": "probability24.html#bernoulli-1",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe PDF is:\n\\[ f(x) = \\\\\n          \\pi    ~~~~~~~~ ~ ~ ~~~~ \\text{if } x=1\\\\\n         1-\\pi   ~ ~ ~ ~~~~\\text{if } x=0\n     \\]\nor\n\\[f(x) = \\pi^{x}(1-\\pi)^{1-x} \\]"
  },
  {
    "objectID": "probability24.html#bernoulli-2",
    "href": "probability24.html#bernoulli-2",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe CDF is:\n\\[F(x) =\\sum_{x} f(x) \\]\nand the expected value is\n\\[\nE(x) =\\sum_{x}x f(x)    \\\\\n= (1)(\\pi)+(0)(1-\\pi)   \\\\\n= \\pi\n\\]"
  },
  {
    "objectID": "probability24.html#binomial",
    "href": "probability24.html#binomial",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nBernoulli is important because it is the foundation for a lot of other distributions, including the binomial distribution. The binomial describes the success probability function (where \\(x=1\\) is a “success”) from a set of \\(n\\) independent Bernoulli trials. So, the binomial is the probability of successes (“ones”) in \\(n\\) independent Bernoulli trials with identical probabilities, \\(\\pi\\).\n\\(~\\)\nThere are two essential parts to the binomial PDF - the probability of success, and the number of ways a success can occur."
  },
  {
    "objectID": "probability24.html#binomial-1",
    "href": "probability24.html#binomial-1",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe probability of success is the Bernoulli probability:\n\\[\nf(x) = \\pi^{x}(1-\\pi)^{1-x}\n\\]\nand the number of ways success can occur (called “n-tuples”) is\n\\[  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} = \\frac{n!}{x!(n-x)!}\n\\]\nNotice the notation for the n-tuple."
  },
  {
    "objectID": "probability24.html#binomial-2",
    "href": "probability24.html#binomial-2",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe PDF combines these:\n\\[\nf(x)=\n  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} \\pi^{x}(1-\\pi)^{n-x}\n\\]\nThere are \\(n\\) trials, \\(x\\) is the number of successes, and \\(n-x\\) is the number of failures. Each event in the n-tuple arises with the Bernoulli probability."
  },
  {
    "objectID": "probability24.html#binomial-3",
    "href": "probability24.html#binomial-3",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are going to toss a fair coin 4 times and want to know how many ways we can have heads come up twice.\n\\[\n\\frac{4!}{2!(4-2)!} = 6\n\\] Now, suppose we want to know the probability exactly 2 of those 4 tosses is heads.\n\\[\nPr(x=2) =\\frac{4!}{2!(4-2)!} (.5)^{2}(.5)^{2} \\\\\n= 6(0.0625) \\\\\n= 0.375\n\\]"
  },
  {
    "objectID": "probability24.html#binomial-4",
    "href": "probability24.html#binomial-4",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nHere’s another example: suppose we flip 5 fair coins once each; what is the PDF of the number of heads we expect? In other words, what is the probability associated with each possible number of successes (heads), from 0 to 5?\n\\[\nP(x=0)  =  \\begin{pmatrix}\n    5  \\\\\n    0\n  \\end{pmatrix} \\cdot  (.5)^{0} \\cdot (.5) ^{5} = 1 \\cdot 1 \\cdot 0.03125=0.03125\\\\\n  \\] \\[\nP(x=1) =  \\begin{pmatrix}\n    5  \\\\\n    1\n  \\end{pmatrix} \\cdot (.5)^{1}\\cdot (.5)^{4} = 5 \\cdot .5 \\cdot 0.0625=0.15625 \\\\\n  \\] \\[P(x=2) =  \\begin{pmatrix}\n    5  \\\\\n    2\n  \\end{pmatrix} \\cdot (.5)^{2}\\cdot (.5)^{3} = 10 \\cdot .25 \\cdot 0.125=0.3125 \\\\\n\\] \\[\n\\vdots \\vdots \\vdots\n\\]"
  },
  {
    "objectID": "probability24.html#binomial-family-distributions",
    "href": "probability24.html#binomial-family-distributions",
    "title": "Basic probability",
    "section": "Binomial family distributions",
    "text": "Binomial family distributions\n\nthe geometric distribution describes repeated Bernoulli trials with probability of success \\(\\pi\\), until the first success.\nthe negative binomial describes the number of Bernoulli failures prior to the first success - it can be thought of as a counting process up to the first success.\nthe poisson describes Bernoulli trials where \\(\\pi\\) for any particular trial is very small."
  },
  {
    "objectID": "probability24.html#the-normal-distribution",
    "href": "probability24.html#the-normal-distribution",
    "title": "Basic probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe Normal distribution is the most widely used distribution in the social sciences.\n\nThe normal seems to “fit” a lot of the variables social scientists measure and use in models.\nEstimation techniques we often employ assume the disturbance term is normally distributed; one result is the coefficients are either normally distributed or t-distributed."
  },
  {
    "objectID": "probability24.html#normal-pdf",
    "href": "probability24.html#normal-pdf",
    "title": "Basic probability",
    "section": "Normal PDF",
    "text": "Normal PDF\nThe normal PDF: \\[\nPr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]\n\\]\nwhere two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\) describe the location and shape of the distribution, the mean and variance respectively; we indicate a normally distributed variable and its parameters as\n\\[\nY \\sim \\text{Normal}(\\mu,\\sigma^{2}) \\nonumber\n\\]\n\n\nDescribe these dataNormal?\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "probability24.html#standard-normal",
    "href": "probability24.html#standard-normal",
    "title": "Basic probability",
    "section": "Standard Normal",
    "text": "Standard Normal\nA useful special case of the Normal is the standard normal, \\(z \\sim \\text{Normal}(0,1)\\). The PDF for the standard normal is given by\n\\[\n\\phi(z)=\\frac{1}{\\sqrt{2 \\pi }} exp \\left[\\frac{-z^{2}}{2}\\right] \\nonumber\n\\]\nwhere the parameters themselves drop out since we’d be subtracting a mean of zero and dividing/multiplying by a variance of 1. The standard normal CDF is denoted \\(\\Phi(z)\\); the standard normal PDF is denoted \\(\\phi(z)\\)."
  },
  {
    "objectID": "probability24.html#normal-pdfs-different-moments",
    "href": "probability24.html#normal-pdfs-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs different moments",
    "text": "Normal PDFs different moments"
  },
  {
    "objectID": "probability24.html#models",
    "href": "probability24.html#models",
    "title": "Basic probability",
    "section": "Models",
    "text": "Models\n\nProbability models include unobserved disturbances; we make assumptions about the distributions of those errors, \\(\\epsilon\\).\nWhat we assume about the unobservables is always informed by what we know about the observables, mainly \\(y\\).\nA useful way to describe or summarize \\(y\\) is to characterize its observed distribution.\nThe distribution of \\(y\\) informs our assumption about the distribution of \\(\\epsilon\\)."
  },
  {
    "objectID": "probability24.html#why-this-matters-to-ols",
    "href": "probability24.html#why-this-matters-to-ols",
    "title": "Basic probability",
    "section": "Why this matters to OLS",
    "text": "Why this matters to OLS\nLinear regression is such an inferential model; we have a number of sources of uncertainty. We represent that uncertainty in the model via the disturbance term, \\(\\epsilon\\).\n\\(~\\)\nIn order to know things about \\(\\epsilon\\) and other parts of the model, we assume that the disturbances are normally distributed; \\(y\\) should be normal too."
  },
  {
    "objectID": "probability24.html#normality-and-centrality",
    "href": "probability24.html#normality-and-centrality",
    "title": "Basic probability",
    "section": "Normality and Centrality",
    "text": "Normality and Centrality\nWe rely on stuff being normally distributed, and central tendency being meaningful. The Central Limit Theorem facilitates this."
  },
  {
    "objectID": "probability24.html#central-limit-theorem",
    "href": "probability24.html#central-limit-theorem",
    "title": "Basic probability",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ {\\sum\\limits_{n \\rightarrow \\infty} (\\bar{X_i})} / {n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "probability24.html#clt---why-is-this-valuable",
    "href": "probability24.html#clt---why-is-this-valuable",
    "title": "Basic probability",
    "section": "CLT - Why is this valuable?",
    "text": "CLT - Why is this valuable?\n\nIf we assume repeated sampling and/or infinitely large samples, we can assume normality.\nWe know the properties of the Normal intimately well. We can use this knowledge to evaluate where some value of \\(x\\) lies on the Normal CDF, what the probability less than that value is - we can figure out what the probability of observing that value is (on the PDF).\nAs we’ll see, \\(\\widehat{\\beta}\\) is normally distributed in the OLS model (it is in MLE as well). The CLT and what we know about normality facilitate inference."
  },
  {
    "objectID": "probability24.html#inference",
    "href": "probability24.html#inference",
    "title": "Basic probability",
    "section": "Inference",
    "text": "Inference\nInference is our effort to measure and characterize our uncertainty about the model and its parameters.\n\nuncertainty is the most important thing we estimate in inferential models.\ncharacterizing uncertainty relies on probability theory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PLSC 501 - Spring 2024",
    "section": "",
    "text": "This is the course website for PLSC 501 - it is a data science course focused on data management, coding in R, and learning basic regression techniques in OLS and MLE.\n\nSyllabus\nSlides\nCode"
  },
  {
    "objectID": "matrix24.html",
    "href": "matrix24.html",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model.\n\n\n\n\nA single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A.\n\n\n\nThe dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar.\n\n\n\n\nA is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]\n\n\n\n\n\n\nA rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3).\n\n\n\nA vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24.html#matrix-notation",
    "href": "matrix24.html#matrix-notation",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model."
  },
  {
    "objectID": "matrix24.html#matrices",
    "href": "matrix24.html#matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A."
  },
  {
    "objectID": "matrix24.html#dimensions",
    "href": "matrix24.html#dimensions",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "The dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar."
  },
  {
    "objectID": "matrix24.html#symmetric-matrices",
    "href": "matrix24.html#symmetric-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#rectangular-matrices",
    "href": "matrix24.html#rectangular-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3)."
  },
  {
    "objectID": "matrix24.html#vectors",
    "href": "matrix24.html#vectors",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24.html#transposition",
    "href": "matrix24.html#transposition",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nThe transpose of a matrix A is the matrix flipped on its side such that its rows become columns and columns become rows; the transpose of A is denoted \\({\\mathbf A'}\\) and is referred to as “\\({\\mathbf A}\\) transpose.”\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right]\n~~~~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5 & 2\\\\\n\\end{array} \\right] \\]\nso \\({\\mathbf X}\\), a (3,2) matrix, becomes \\({\\mathbf X'}\\), a (2,3) matrix."
  },
  {
    "objectID": "matrix24.html#transposition-1",
    "href": "matrix24.html#transposition-1",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nIf \\({\\mathbf A}\\) is symmetric, then \\({\\mathbf A}={\\mathbf A'}\\) – take a look at the following symmetric matrix and you’ll see why:\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#transposition-2",
    "href": "matrix24.html#transposition-2",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nAlso note that the transpose of a transposed matrix results in the original matrix: \\((\\mathbf{X}')'=\\mathbf{X}\\).\nTransposing a row vector results in a column vector and vice versa:\n\\[\\mathbf{x} =  \\left[\n\\begin{array}{c}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n\\end{array} \\right]\n~~~~~~~~~\n\\mathbf{x'} =  \\left[\n\\begin{array}{cccc}\n-1 & -9 &4 &16\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#trace-of-a-matrix",
    "href": "matrix24.html#trace-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nThe trace of a matrix is the sum of the diagonal elements of a square matrix, and is denoted \\(tr(\\mathbf{A})=a_{11}+a_{22}+a_{33}\\ldots+a_{nn} = \\sum\\limits_{i=1}^{n}a_{ii}\\)\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nsuch that\n\\(tr(\\mathbf{A})=2+5+2=9\\)"
  },
  {
    "objectID": "matrix24.html#addition-subtraction",
    "href": "matrix24.html#addition-subtraction",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\n\nAddition and subtraction of matrices is analogous to the same scalar operations, but depend on the conformatibility of the matrices to be added or subtracted.\nTwo matrices are conformable for addition or subtraction iff they are of the same order.\nNote that conformability means something different for addition/subtraction than for multiplication."
  },
  {
    "objectID": "matrix24.html#addition-subtraction-1",
    "href": "matrix24.html#addition-subtraction-1",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider the following (2,2) matrices and the problem \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{C}\\):\n\\[ \\left[\n\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21}& a_{22} \\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{cc}\nb_{11} & b_{12}\\\\\nb_{21}& b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21}& a_{22}+b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\nc_{11} & c_{12}\\\\\nc_{21}& c_{22} \\\\\n\\end{array} \\right] \\]\nAddition and subtraction are only possible among matrices that share the same order; otherwise, they are nonconformable."
  },
  {
    "objectID": "matrix24.html#addition-subtraction-2",
    "href": "matrix24.html#addition-subtraction-2",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider a second example of addition, and note that subtraction would follow directly:\n\\[ \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{ccc}\n1 &5 &7\\\\\n3& 7& 1\\\\\n2& 4& 9\\\\\n\\end{array} \\right]\n=\n  \\left[\n\\begin{array}{ccc}\n3 &9 &15\\\\\n7& 12& 4\\\\\n10& 7& 11\\\\\n\\end{array} \\right] \\]\nMatrix addition adheres to the commutative and associative properties such that:\n\\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) (Commutative), and \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}= \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) (Associative)."
  },
  {
    "objectID": "matrix24.html#multiplication",
    "href": "matrix24.html#multiplication",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\nLet’s begin by multiplying a scalar and a matrix - the product is a matrix whose elements are the scalar multiplied by each element of the original matrix, so:\n\\[ \\beta\n\\left[\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\n\\beta x_{11} & \\beta x_{12}\\\\\n\\beta x_{21}&  \\beta x_{22} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#multiplication-1",
    "href": "matrix24.html#multiplication-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\n\nIn order to multiply two matrices (or vectors), they must be conformable for multiplication.\nTwo matrices are conformable for multiplication iff the number of columns in the first matrix is equal to the number of rows in the second matrix. That is, \\(\\mathbf{A_{i,j}}\\) and \\(\\mathbf{B_{j,k}}\\).\nAn easy way to think of this is to write the dimensions of the two matrices, (i,j),(j,k) - the inner dimensions are the same, so the matrix is conformable for multiplication - moreover, the outer dimensions (i,k) give the dimension of the product matrix."
  },
  {
    "objectID": "matrix24.html#multiplication---inner-product",
    "href": "matrix24.html#multiplication---inner-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\n\nTo illustrate multiplication, let’s start with a column vector, \\(\\mathbf{e}\\) whose order is (N,1) and take its inner product.\nThe inner product of a column vector is the transpose of the column vector (thus, a row vector) post-multiplied by the column vector, so \\({\\mathbf e'\\mathbf e}\\). The inner product is a scalar."
  },
  {
    "objectID": "matrix24.html#multiplication---inner-product-1",
    "href": "matrix24.html#multiplication---inner-product-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\nWhen we transpose the column vector \\({\\mathbf e}\\) of (N,1) order, we get a row vector \\({\\mathbf e'}\\) of (1,N) order. Inner dimensions match, so they are conformable.\n\\[\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n~~~~~\n\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right] \\] \\[=\n(-1 \\cdot -1)+(-9 \\cdot -9)+(4 \\cdot 4)+ (16 \\cdot 16)+ (-10 \\cdot -10) = 454\\]"
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing",
    "href": "matrix24.html#least-squares-foreshadowing",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nNote the inner product of a column vector is a scalar; the inner product of \\(\\mathbf{e}\\), is the sum of the squares of \\(\\mathbf{e}\\).\n\\[\\mathbf{e'e}= e_{1}e_{1}+e_{2}e_{2}+\\ldots e_{N}e_{N} = \\sum\\limits_{i=1}^{N}e_{i}^{2}\\]"
  },
  {
    "objectID": "matrix24.html#multiplication---outer-product",
    "href": "matrix24.html#multiplication---outer-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - outer product",
    "text": "Multiplication - outer product\nThe outer product of a column vector is the transpose of the column vector (thus, a row vector) pre-multiplied by the column vector, so \\({\\mathbf e\\mathbf e'}\\). The outer product is an (N,N) matrix.\n\\[\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right]\n\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n\\]\nLet’s multiply \\({\\mathbf e\\mathbf e'}\\), a column vector of (5,1) by a row vector of (1,5); we’ll obtain a square matrix of (5,5)."
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing-1",
    "href": "matrix24.html#least-squares-foreshadowing-1",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nLet \\(\\mathbf{e}\\) represent the residuals or errors in our regression. The outer product\n\\[{\\mathbf e\\mathbf e'} =  \\left[\n\\begin{array}{rrrrr}\n1 & 9 &-4 &-16 &10\\\\\n9 & 81 &-36 &-144 &90\\\\\n-4 & -36 &16 &64 &-40\\\\\n-16 & -144 &64 &256 &-160\\\\\n10 & 90 &-40 &-160 &100\\\\\n\\end{array} \\right] \\]\nis the variance-covariance matrix of \\(\\mathbf{e}\\). The squares on the main diagonal (which sums to the sum of squares) and the symmetry of the off-diagonal elements."
  },
  {
    "objectID": "matrix24.html#inverting-matrices-1",
    "href": "matrix24.html#inverting-matrices-1",
    "title": "Matrix algebra basics",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\n\nYou’ll have noticed that division has been conspicuously absent so far. In scalar algebra, we sometimes represent division in fractions, \\(\\frac{1}{2}\\).\nAnother way to represent the same quantity is \\(2^{-1}\\), or the inverse of 2. Of course, in scalar algebra, a number multiplied by its inverse equals 1, e.g., \\(2 \\cdot 2^{-1}=1\\) or \\(2 \\cdot \\frac{1}{2}=1\\). So, if we are given \\(4x=1\\) and want to solve for \\(x\\) what we really want to know is “what is the inverse of 4 such that \\(4 \\cdot 4^{-1}=1\\)?” We consider matrices in a similar manner."
  },
  {
    "objectID": "matrix24.html#inverting-square-matrices",
    "href": "matrix24.html#inverting-square-matrices",
    "title": "Matrix algebra basics",
    "section": "Inverting Square Matrices",
    "text": "Inverting Square Matrices\nImagine a square matrix, \\(\\mathbf{X}\\) and its inverse, denoted \\({\\mathbf{X^{-1}}}\\) (read as “X inverse”). Analogous to scalar algebra, a matrix multiplied by its inverse is equal to the identity matrix, \\(\\mathbf{I}\\).\nSo in order to find \\({\\mathbf{X^{-1}}}\\), we must find the matrix that, when multiplied by \\(\\mathbf{X}\\), produces \\(\\mathbf{I}\\). Thus, for a square matrix, its inverse (if it exists) is such that\n\\[\\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X^{-1}} \\mathbf{X}=\\mathbf{I}\\]\nNotice the commutative property at work here - this is because \\(\\mathbf{X}\\) is square, such that \\(n=m\\), so \\(\\mathbf{X}\\) and \\({\\mathbf{X^{-1}}}\\) have the same dimensions."
  },
  {
    "objectID": "matrix24.html#determinant",
    "href": "matrix24.html#determinant",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nAn important characteristic of square matrices is the determinant. The determinant, denoted \\(|X|\\), is a scalar; every square matrix has one. We evaluate the determinant by examining the cross-products of the matrice’s elements. This is simple in the 2x2 matrix, less so in larger matrices.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right| = x_{11}x_{22}-x_{12}x_{21}\\]"
  },
  {
    "objectID": "matrix24.html#determinant-1",
    "href": "matrix24.html#determinant-1",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nIn the 2x2 matrix, the determinant is the cross-product of the main diagonals minus the cross-product of the off-diagonals.\n\\[ |\\mathbf{X_{1}}|=\\left|\n\\begin{array}{cc}\n2 & 4\\\\\n6& 3 \\\\\n\\end{array} \\right| = 2 \\cdot 3-4 \\cdot 6 = -18\n\\]\nThe determinant is -18; \\(|\\mathbf{X_{1}}|=-18\\)."
  },
  {
    "objectID": "matrix24.html#determinant-2",
    "href": "matrix24.html#determinant-2",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nThis matrix has a determinant of zero - this is an important case.\n\\[ |\\mathbf{X_{2}}|=\\left|\n\\begin{array}{cc}\n3 & 6\\\\\n2& 4 \\\\\n\\end{array} \\right| = 3 \\cdot 4-6 \\cdot 2 =0\n\\] A matrix with determinant zero is singular. A singluar matrix has no inverse."
  },
  {
    "objectID": "matrix24.html#singular-matrices",
    "href": "matrix24.html#singular-matrices",
    "title": "Matrix algebra basics",
    "section": "Singular matrices",
    "text": "Singular matrices\nThere are some important conditions that will produce a determinant of zero and thus a singular matrix:\n\nIf all the elements of any row or column of a matrix are equal to zero, then the determinant is zero.\nIf two rows or columns of a matrix are identical, the determinant is zero.\nIf a row or column of a matrix is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing-2",
    "href": "matrix24.html#least-squares-foreshadowing-2",
    "title": "Matrix algebra basics",
    "section": "Least Squares Foreshadowing",
    "text": "Least Squares Foreshadowing\nFor the linear model in least squares, this is important because computing estimates of \\(\\beta\\) requires inverting a matrix. Let’s derive \\(\\beta\\) in matrix notation to see how:\nThe estimated model is:\n\\[\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}}\\] Minimizing \\(\\widehat{\\mathbf{\\epsilon}}'\\widehat{\\mathbf{\\epsilon}}\\) (skipping the math for now), we get\n\\[(\\mathbf{X'X}) \\widehat{\\beta}=\\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares",
    "href": "matrix24.html#foreshadowing-least-squares",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThe matrix \\((\\mathbf{X'X})\\) gives the sums of squares (on the main diagonal) and the sums of cross products (in the off-diagonals) of all the \\(\\mathbf{X}\\) variables; the matrix is symmetric. Since \\(\\beta\\) is the unknown vector in this equation, solve for \\(\\beta\\) by dividing both sides by \\((\\mathbf{X'X})\\) - in matrix terms, we premultiply each side by \\((\\mathbf{X'X)^{-1}}\\):\n\\[(\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) \\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares-1",
    "href": "matrix24.html#foreshadowing-least-squares-1",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nAs we know, \\((\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) = \\mathbf{I}\\). So,\n\\[\\mathbf{I}\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\] or \\[\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]\nEstimating \\(\\widehat{\\beta}\\) requires inverting \\(\\mathbf{(X'X)}\\) If the matrix \\(\\mathbf{(X'X)}\\) is singular, then \\(\\mathbf{(X'X)^{-1}}\\) does not exist, and we cannot estimate \\(\\widehat{\\beta}\\). Least Squares fails."
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares-2",
    "href": "matrix24.html#foreshadowing-least-squares-2",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThinking specifically of the \\(\\mathbf{X}\\) variables in the model, any of these conditions produce perfect collinearity - estimation fails because the matrix can’t invert.\n\nIf all the elements of any row or column of \\(\\mathbf{X}\\) are equal to zero, then the determinant is zero.\nIf two rows or columns of \\(\\mathbf{X}\\) are identical, the determinant is zero.\nIf a row or column of \\(\\mathbf{X}\\) is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24.html#examples",
    "href": "matrix24.html#examples",
    "title": "Matrix algebra basics",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2Example 3\n\n\n\\({\\mathbf X}\\) below has a row of zeros; compute the determinant using the difference of cross-products.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n0 & 0\\\\\n19& 12 \\\\\n\\end{array} \\right| = 0 \\cdot 12-0 \\cdot 19 = 0 \\]\nYou can see in \\({\\mathbf X}\\) that because all the cross-products involve multiplying by zero, the determinant will always be zero; this applies to larger matrices as well.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n5 & 7\\\\\n5& 7 \\\\\n\\end{array} \\right| = 5 \\cdot 7-5 \\cdot 7 =0 \\]\nIn \\({\\mathbf X}\\) it’s easy to see that the cross-products will always be equal to one another if the rows or columns are identical, and the difference between identical values is zero.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n64 & 48\\\\\n16& 12 \\\\\n\\end{array} \\right| = 64 \\cdot 12-48 \\cdot 16 = 768-768=0\\]\nFinally \\({\\mathbf X}\\) shows that if one row or column is a linear combination of other rows or columns, the determinant will be zero; in that matrix, the top row is 4 times the bottom row."
  },
  {
    "objectID": "matrix24.html#inversion",
    "href": "matrix24.html#inversion",
    "title": "Matrix algebra basics",
    "section": "Inversion",
    "text": "Inversion\nThe matrix we need to invert, \\(\\mathbf{X'X}\\) is rectangular, so inversion is more complex. I’m going to illustrate the method of cofactor expansion - the purpose here is to give you an idea what software does every time you estimate an OLS model."
  },
  {
    "objectID": "matrix24.html#minor-of-a-matrix",
    "href": "matrix24.html#minor-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Minor of a matrix",
    "text": "Minor of a matrix\nThe minor of a matrix is the determinant of a submatrix created by deleting the \\(i\\)th row and the \\(j\\)th column of the full matrix, where the minor is denoted by the element where the deleted rows intersect.\n\\[ \\mathbf{A} =  \\left[\n\\begin{array}{ccc}\na_{11} & a_{12} &a_{13}\\\\\na_{21}& a_{22} &a_{23}\\\\\na_{31} &a_{32} & a_{33}\\\\\n\\end{array} \\right] \\] \\[\\text{so the minor of } a_{11} \\text{ is }\n|\\mathbf{M_{11}}| =  \\left|\n\\begin{array}{cc}\na_{22} &a_{23}\\\\\na_{32} & a_{33}\\\\\n\\end{array} \\right|  =  a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\]"
  },
  {
    "objectID": "matrix24.html#cofactor-matrix",
    "href": "matrix24.html#cofactor-matrix",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nThe cofactor of an element (\\(c_{ij}\\)) is the minor with a positive or negative sign depending on whether \\(i+j\\) is odd (negative) or even (positive). This is given by:\n\\[\\theta_{ij}=(-1)^{i+j}|\\mathbf{M_{ij}}|\\]\nso, from the example finding the minor of \\(a_{11}\\), \\(i\\)=1 and \\(j=1\\); their sum is 2 which is even, so the cofactor is \\(+a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\). If we find the cofactor for every element, \\(a_{ij}\\) in a matrix and replace each element with its cofactor, the new matrix is called the cofactor matrix of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24.html#cofactor-matrix-1",
    "href": "matrix24.html#cofactor-matrix-1",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nWhat we have so far is the signed matrix of minors:\n\\[\\mathbf{A} = \\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#cofactor-expansion",
    "href": "matrix24.html#cofactor-expansion",
    "title": "Matrix algebra basics",
    "section": "Cofactor Expansion",
    "text": "Cofactor Expansion\nFind the signed determinant of each 2x2 submatrix.\n\\[\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} = \\left[\n\\begin{array}{rrr}\n10&-10&0\\\\\n-12&0&6\\\\\n-2&5&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#find-the-determinant",
    "href": "matrix24.html#find-the-determinant",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nUsing the cofactor matrix, \\(\\mathbf{\\Theta_{A}}\\) we perform cofactor expansion in order to find the determinant, \\(|\\mathbf{A}|\\). Cofactor expansion is given by:\n\\[|\\mathbf{A}|=\\sum\\limits_{i,j=1}^{n}a_{ij}\\Theta_{ij}\\]\nwhich means that we take two corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) and sum the products of their elements - this gives us the determinant of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24.html#find-the-determinant-1",
    "href": "matrix24.html#find-the-determinant-1",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nSo taking the first row from \\(\\mathbf{A}\\) (1,4,2) and from \\(\\mathbf{\\Theta_{A}}\\) (10,-10,0) above, we compute\n\\[a_{11} \\cdot \\Theta_{A11}+a_{12} \\cdot \\Theta_{A12}+a_{13} \\cdot \\Theta_{A13}\\]\n\\[1 \\cdot 10+ 4 \\cdot -10 + 2 \\cdot 0 = -30\\] This is the determinant of \\(\\mathbf{A}\\) , \\(|\\mathbf{A}|\\). Note that any corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) will produce the same result."
  },
  {
    "objectID": "matrix24.html#adjoint-matrix",
    "href": "matrix24.html#adjoint-matrix",
    "title": "Matrix algebra basics",
    "section": "Adjoint Matrix",
    "text": "Adjoint Matrix\nIf we transpose the cofactor matrix, (cof \\(\\mathbf{A'}\\)), the new matrix is called the adjoint matrix, denoted adj\\(\\mathbf{A}\\)=(cof \\(\\mathbf{A'}\\)).\n\\[adj\\mathbf{A} = \\mathbf{\\Theta_{A}'} = \\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#inverting-the-matrix",
    "href": "matrix24.html#inverting-the-matrix",
    "title": "Matrix algebra basics",
    "section": "Inverting the matrix",
    "text": "Inverting the matrix\nBelieve it or not, this all leads us to inverting the matrix, provided it is nonsingular.\n\\[\\mathbf{A^{-1}}=\\frac{1}{|\\mathbf{A}|}(adj {\\mathbf A})\\] The inverse of A is equal to one over the determinant of A times the adjoint matrix of A. So we’re pre-multiplying \\(adj {\\mathbf A}\\) by the (scalar) determinant, -30."
  },
  {
    "objectID": "matrix24.html#inverse-of-matrix-a",
    "href": "matrix24.html#inverse-of-matrix-a",
    "title": "Matrix algebra basics",
    "section": "Inverse of Matrix A",
    "text": "Inverse of Matrix A\n\\[{\\mathbf A^{-1}}=-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n=\\left[\n\\begin{array}{rrr}\n-\\frac{1}{3}&\\frac{2}{5}&\\frac{1}{15}\\\\\n\\frac{1}{3}&0&-\\frac{1}{6}\\\\\n0&-\\frac{1}{5}&\\frac{3}{10}\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#checking-our-work",
    "href": "matrix24.html#checking-our-work",
    "title": "Matrix algebra basics",
    "section": "Checking our work",
    "text": "Checking our work\nWe can check our work to be sure we’ve inverted correctly by making sure that \\(\\mathbf{A^{-1}A}=\\mathbf{I_{3}}\\); so,\n\\[-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n\\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n=\n-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n-30&0&0\\\\\n0&-30&0\\\\\n0&0&-30\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols",
    "href": "matrix24.html#connecting-data-matrices-to-ols",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\nLet’s examine these matrices a little to get a feel for what the computation of \\(\\widehat{\\beta}\\) involves: \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\n\\[\n\\mathbf{X}= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \n\\end{matrix}  \\right]\n\\] The dimensions of \\(\\mathbf{X}\\) are \\(n x k\\); the sample size by the number of regressors (including the constant)."
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols-1",
    "href": "matrix24.html#connecting-data-matrices-to-ols-1",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \n\\end{matrix}  \\right] \\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols-2",
    "href": "matrix24.html#connecting-data-matrices-to-ols-2",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i}\n\\end{matrix}  \\right] \\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationships among \\(X_k\\)."
  },
  {
    "objectID": "overview24.html",
    "href": "overview24.html",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise\n\n\n\n\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding.\n\n\n\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling.\n\n\n\n\nIs not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal.\n\n\n\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here.\n\n\n\n\n\nis a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together.\n\n\n\n\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra\n\n\n\n\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\).\n\n\n\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?\n\n\n\n\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term.\n\n\n\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later.\n\n\n\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24.html#all-models-are-wrong-ldots",
    "href": "overview24.html#all-models-are-wrong-ldots",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise"
  },
  {
    "objectID": "overview24.html#the-regression-model",
    "href": "overview24.html#the-regression-model",
    "title": "Course overview",
    "section": "",
    "text": "This course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding."
  },
  {
    "objectID": "overview24.html#course-goals",
    "href": "overview24.html#course-goals",
    "title": "Course overview",
    "section": "",
    "text": "In May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling."
  },
  {
    "objectID": "overview24.html#this-class",
    "href": "overview24.html#this-class",
    "title": "Course overview",
    "section": "",
    "text": "Is not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal."
  },
  {
    "objectID": "overview24.html#this-class-1",
    "href": "overview24.html#this-class-1",
    "title": "Course overview",
    "section": "",
    "text": "is aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here."
  },
  {
    "objectID": "overview24.html#this-class-2",
    "href": "overview24.html#this-class-2",
    "title": "Course overview",
    "section": "",
    "text": "is a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together."
  },
  {
    "objectID": "overview24.html#what-do-you-need-to-know-for-501",
    "href": "overview24.html#what-do-you-need-to-know-for-501",
    "title": "Course overview",
    "section": "",
    "text": "What do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra"
  },
  {
    "objectID": "overview24.html#matrix-algebra",
    "href": "overview24.html#matrix-algebra",
    "title": "Course overview",
    "section": "",
    "text": "You should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\)."
  },
  {
    "objectID": "overview24.html#probability-theory",
    "href": "overview24.html#probability-theory",
    "title": "Course overview",
    "section": "",
    "text": "what are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?"
  },
  {
    "objectID": "overview24.html#understanding-regression-models",
    "href": "overview24.html#understanding-regression-models",
    "title": "Course overview",
    "section": "",
    "text": "The class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term."
  },
  {
    "objectID": "overview24.html#regression-models",
    "href": "overview24.html#regression-models",
    "title": "Course overview",
    "section": "",
    "text": "Posit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later."
  },
  {
    "objectID": "overview24.html#regression-models-1",
    "href": "overview24.html#regression-models-1",
    "title": "Course overview",
    "section": "",
    "text": "Can be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24.html#models-to-understand-politics",
    "href": "overview24.html#models-to-understand-politics",
    "title": "Course overview",
    "section": "Models to understand politics",
    "text": "Models to understand politics\n\\(~\\)\nThis a methods class, but the methods are only meaningful to us insofar as they help us understand politics. So let’s motivate the class with a question about politics."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Course overview\nMatrix algebra basics\nProbability basics\nThinking about data\nBivariate model\nMultivariate model\nPrediction methods\nNormality\nInference\nModel specification"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression."
  },
  {
    "objectID": "overview24.html#careful-thinking",
    "href": "overview24.html#careful-thinking",
    "title": "Course overview",
    "section": "Careful thinking",
    "text": "Careful thinking\nCorrelation does not imply causation.\n\n\n\nRepression during the COVID-19 Pandemic\nAmong the products of the pandemic is an opportunity for governments, under the guise of protecting public health, to repress citizens.\nThere is some evidence of democratic backsliding prior to the pandemic, but the global public health crisis gave governments a specific and universal opportunity to take advantage of this softened ground and to become more authoritarian.\nWe’re going to examine the covariates of violence against civilians during the COVID period, focusing on how states of emergency create changes in government violence.\n\n\n\nData\nLet’s look at ACLED event data, and data on states of emergency during the COVID period from Lundgren et al (2020).\nThe next figure (which should look familiar), plots protests and repression since mid-2019, the onset of the pandemic marked at March 15, 2020.\nThe blue lines are local regressions showing trends; while protests return to pre-pandemic levels (similar to the years prior), repression increases since the onset of COVID-19.\n\n\n\nProtests\n\n\ncode\nacled&lt;-read.csv(\"/Users/dave/Documents/2013/PITF/analysis/protests2021/data/2022analysis/acled2022.csv\")\n\n##Event_date variable has a \"Chr\" format. \n#Need to convert it into date format to calculate 7 day averages for Protests and Violence against Civilians\n\nacled$event_date&lt;-as.Date(acled$event_date,format=\"%d %B %Y\")\n#class(acled$event_date)\n\n##Compute number of events per day\n\nacled$perday&lt;-1\n\nprotests&lt;-filter(acled,event_type==\"Protests\" )\nstateviol&lt;-filter(acled, event_type==\"Violence against civilians\")\nprotests&lt;-aggregate(perday ~ event_date, data = protests, sum)\nstateviol&lt;-aggregate(perday ~ event_date, data = stateviol, sum)\n\n\n##Calculating a 7 day moving average\nprotests$rollmean&lt;-rollmean(protests$perday,k=7, fill=NA)\nstateviol$rollmean&lt;-rollmean(stateviol$perday,k=7, fill=NA)\n\nprotests &lt;- protests %&gt;% filter(protests$event_date &lt; \"2022-06-25\")\nstateviol &lt;- stateviol %&gt;% filter(stateviol$event_date &lt; \"2022-06-25\")\n\nprotestplot &lt;- ggplot(data=protests, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Protests\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; 'ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Protests During the Pandemic\") \n   \n\nprotestplot\n\n\n\n\n\n\n\n\n\n\n\n\nRepression\n\n\ncode\nrepressionplot &lt;- ggplot(data=stateviol, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Violence against civilians\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Violence Against Civilians During the Pandemic\") \n  \nrepressionplot\n\n\n\n\n\n\n\n\n\n\n\n\nStates of Emergency during the Pandemic\n\n\ncode\n####################\n#analysis data\n####################\n\n#make analysis data frame\nanalysisdata &lt;- left_join(perday, soe, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"event_date\"))\nanalysisdata &lt;- left_join(analysisdata, covid, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"date\"))\n\n#zeros for NA in soe_s\nanalysisdata &lt;- analysisdata %&gt;% mutate(across(soe_s, ~replace_na(., 0)))\n\n#fill soe series after onset\nanalysisdata &lt;- analysisdata %&gt;% group_by(ccode) %&gt;% mutate(cumSOE = cumsum(soe_s)) %&gt;% ungroup\n\n#plot cumulative states of emergency over time\ntotalsoe &lt;- analysisdata %&gt;% group_by(event_date) %&gt;%\n  summarise(across(cumSOE, sum)) %&gt;%\n  ungroup() \n\ntotalsoe$event_date &lt;- as.Date(totalsoe$event_date)\n\ntotalsoe &lt;- totalsoe %&gt;% filter(event_date &gt;\"2020-01-01\"& event_date &lt;\"2020-07-01\")\n\nggplot() +\n  geom_line(data=totalsoe, aes(x=event_date, y=cumSOE))+\n  scale_x_date(date_breaks = \"1 months\", date_labels = '%b %Y') +\n  labs ( colour = NULL, x = \"Date\", y =  \"Active States of Emergency\" )  \n\n\n\n\n\n\n\n\n\n\n\n\nModeling repression\nLet’s build a simple model predicting violence against civilians. The data here are daily, covering 167 countries between January 2020 and July 2022. We’ll predict the 7 day moving average of repression depending on whether a state of emergency is in place.\n\\(~\\)\nMy expectation is states of emergency will make repression easier to motivate and therefore, more frequent.\n\n\n\nThe model\n\\(y\\) = violence against civilians\n\\(x\\) = states of emergency\nControlling for:\n\nnew daily deaths from COVID (logged)\npercentage of the population over 65 years old\nthe Human Development Index\nthe 7 day average number of protests.\n\n\n\n\nModeling Repression and SoEs\n\n\ncode\n##################\n#models\n#################\n\nanalysisdata$soeprotests &lt;- analysisdata$cumSOE*analysisdata$protest7day\nanalysisdata$lnd =  log(analysisdata$new_deaths+.01)\n#label variables for coef plot\nlibrary(\"labelled\")\nanalysisdata &lt;- analysisdata %&gt;%\n  set_variable_labels(\n    cumSOE = \"State of Emergency\",\n    lnd = \"ln(New COVID deaths)\",\n    aged_65_older = \"% age 65 +\",\n    human_development_index = \"Human Development Index\",\n    protest7day = \"Protests, 7 day avg\",\n    soeprotests = \"SoE * Protests, 7 day avg\", \n    `Violence against civilians` = \"Violence against Civilians\"\n  )\n\n#model predicting repression during covid \n\nm1 &lt;- lm(data=analysisdata, `Violence against civilians` ~ cumSOE  + lnd + aged_65_older+human_development_index + protest7day )\n#summary(m1)  \n\nlibrary(sjPlot)\ntab_model(m1, show.se=TRUE,  p.style=\"stars\", show.ci=FALSE)\n\n\n\n\n\n\n\n\n\n\n \nViolence against\nCivilians\n\n\nPredictors\nEstimates\nstd. Error\n\n\n(Intercept)\n0.93 ***\n0.03\n\n\nState of Emergency\n0.26 ***\n0.01\n\n\nln(New COVID deaths)\n0.02 ***\n0.00\n\n\n%age 65+\n-0.02 ***\n0.00\n\n\nHuman Development Index\n-0.68 ***\n0.04\n\n\nProtests,7 day avg\n0.04 ***\n0.00\n\n\nObservations\n165211\n\n\nR2 / R2 adjusted\n0.057 / 0.057\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\n\nAnother look\n\n\ncode\n#coefficient plot\nggcoef_model(\n  m1,show_p_values = FALSE,\n  signif_stars = FALSE, exponentiate =FALSE, intercept=TRUE,\n  colour=NULL, include = c(\"cumSOE\", \"lnd\", \"aged_65_older\", \"human_development_index\", \"protest7day\")\n)+\n  xlab(\"Average Marginal Effects\") +\n  ggtitle(\"Predictors of Violence Against Civilians\") \n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe estimates indicate:\n\nif all the \\(X\\) variables are set to zero, the expected violence against civilians is 1.16 (the intercept). Can we reasonably set all the \\(X\\) variables to zero?\nthe difference between a state with a SoE and without is different from zero, but quite small; 0.13 uses of violence.\nmore deaths from COVID are associated with more violence against civilians; \\(ln(\\beta) = .05\\). Exponentiate that, and the effect on violence is 1.05, so large compared to anything else in the model.\n\n\n\n\nQuantities of interest (simulation)\nLet’s generate predictions from the model by simulating the distribution of \\(\\widehat{\\beta}\\):\n\nsample 1000 times from a multivariate normal distribution with mean \\(\\widehat{\\beta}\\)s and variances of \\(var(\\widehat{\\beta})\\).\nplug those simulated \\(\\widehat{\\beta}\\)s into the equation using mean/median/mode values for the \\(X\\)s.\n\n\n\n\nSimulating quantities\nFirst, set \\(SoE = 0\\); vary \\(covid~deaths = ln(1 \\ldots 28000)\\), \\(\\%~age~ 65 = 6\\) (the median), \\(HDI = .74\\) (the median), and \\(protests = 2\\) (the median).\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*   0 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\nthen, repeat but with \\(SoE = 1\\):\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*1 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\n\n\n\nPredicted Violence against Civilians\n\n\ncode\n#simulated quantities\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),\n      ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= \n      numeric(0), deaths = numeric(0))\n\nfor (p in seq(0,28000,100)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                       B$b5*2, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                      B$b5*2, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\npredictions$deaths &lt;- log(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"ln(New COVID Deaths)\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5.5, y = .82, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 6.5, y = .55, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nSoE states have higher levels of violence against civilians, but the difference is very small, making me think we haven’t found anything particularly interesting here.\n\nCovid deaths is associated with more violence; though violence nearly doubles over the range of deaths (x-axis), the range is huge - from 1 to 28,000. If we transform the x-axis to deaths rather than \\(ln(deaths)\\), we can see this effect is pretty isolated.\n\n\n\n\nSimulated effects (again)\n\n\ncode\npredictions$deaths &lt;- exp(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"New COVID Deaths\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5000, y = 1, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 10000, y = .65, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe increase in violence occurs almost entirely between zero and 5000 COVID deaths. We can speculate on why this might be; doing so might lead us to wonder how useful this model is.\n\\(~\\) \\(~\\)\nThat’s the primary question we’ll ask this semester."
  },
  {
    "objectID": "overview24s.html#all-models-are-wrong-ldots",
    "href": "overview24s.html#all-models-are-wrong-ldots",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise"
  },
  {
    "objectID": "overview24s.html#the-regression-model",
    "href": "overview24s.html#the-regression-model",
    "title": "Course overview",
    "section": "",
    "text": "This course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding."
  },
  {
    "objectID": "overview24s.html#course-goals",
    "href": "overview24s.html#course-goals",
    "title": "Course overview",
    "section": "",
    "text": "In May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling."
  },
  {
    "objectID": "overview24s.html#this-class",
    "href": "overview24s.html#this-class",
    "title": "Course overview",
    "section": "",
    "text": "Is not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal."
  },
  {
    "objectID": "overview24s.html#this-class-1",
    "href": "overview24s.html#this-class-1",
    "title": "Course overview",
    "section": "",
    "text": "is aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here."
  },
  {
    "objectID": "overview24s.html#this-class-2",
    "href": "overview24s.html#this-class-2",
    "title": "Course overview",
    "section": "",
    "text": "is a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together."
  },
  {
    "objectID": "overview24s.html#what-do-you-need-to-know-for-501",
    "href": "overview24s.html#what-do-you-need-to-know-for-501",
    "title": "Course overview",
    "section": "",
    "text": "What do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra"
  },
  {
    "objectID": "overview24s.html#matrix-algebra",
    "href": "overview24s.html#matrix-algebra",
    "title": "Course overview",
    "section": "",
    "text": "You should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\)."
  },
  {
    "objectID": "overview24s.html#probability-theory",
    "href": "overview24s.html#probability-theory",
    "title": "Course overview",
    "section": "",
    "text": "what are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?"
  },
  {
    "objectID": "overview24s.html#understanding-regression-models",
    "href": "overview24s.html#understanding-regression-models",
    "title": "Course overview",
    "section": "",
    "text": "The class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term."
  },
  {
    "objectID": "overview24s.html#regression-models",
    "href": "overview24s.html#regression-models",
    "title": "Course overview",
    "section": "",
    "text": "Posit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later."
  },
  {
    "objectID": "overview24s.html#regression-models-1",
    "href": "overview24s.html#regression-models-1",
    "title": "Course overview",
    "section": "",
    "text": "Can be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24s.html#models-to-understand-politics",
    "href": "overview24s.html#models-to-understand-politics",
    "title": "Course overview",
    "section": "Models to understand politics",
    "text": "Models to understand politics\n\\(~\\)\nThis a methods class, but the methods are only meaningful to us insofar as they help us understand politics. So let’s motivate the class with a question about politics."
  },
  {
    "objectID": "overview24s.html#careful-thinking",
    "href": "overview24s.html#careful-thinking",
    "title": "Course overview",
    "section": "Careful thinking",
    "text": "Careful thinking\nCorrelation does not imply causation.\n\n\n\nRepression during the COVID-19 Pandemic\nAmong the products of the pandemic is an opportunity for governments, under the guise of protecting public health, to repress citizens.\nThere is some evidence of democratic backsliding prior to the pandemic, but the global public health crisis gave governments a specific and universal opportunity to take advantage of this softened ground and to become more authoritarian.\nWe’re going to examine the covariates of violence against civilians during the COVID period, focusing on how states of emergency create changes in government violence.\n\n\n\nData\nLet’s look at ACLED event data, and data on states of emergency during the COVID period from Lundgren et al (2020).\nThe next figure (which should look familiar), plots protests and repression since mid-2019, the onset of the pandemic marked at March 15, 2020.\nThe blue lines are local regressions showing trends; while protests return to pre-pandemic levels (similar to the years prior), repression increases since the onset of COVID-19.\n\n\n\nProtests\n\n\ncode\nacled&lt;-read.csv(\"/Users/dave/Documents/2013/PITF/analysis/protests2021/data/2022analysis/acled2022.csv\")\n\n##Event_date variable has a \"Chr\" format. \n#Need to convert it into date format to calculate 7 day averages for Protests and Violence against Civilians\n\nacled$event_date&lt;-as.Date(acled$event_date,format=\"%d %B %Y\")\n#class(acled$event_date)\n\n##Compute number of events per day\n\nacled$perday&lt;-1\n\nprotests&lt;-filter(acled,event_type==\"Protests\" )\nstateviol&lt;-filter(acled, event_type==\"Violence against civilians\")\nprotests&lt;-aggregate(perday ~ event_date, data = protests, sum)\nstateviol&lt;-aggregate(perday ~ event_date, data = stateviol, sum)\n\n\n##Calculating a 7 day moving average\nprotests$rollmean&lt;-rollmean(protests$perday,k=7, fill=NA)\nstateviol$rollmean&lt;-rollmean(stateviol$perday,k=7, fill=NA)\n\nprotests &lt;- protests %&gt;% filter(protests$event_date &lt; \"2022-06-25\")\nstateviol &lt;- stateviol %&gt;% filter(stateviol$event_date &lt; \"2022-06-25\")\n\nprotestplot &lt;- ggplot(data=protests, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Protests\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; 'ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Protests During the Pandemic\") \n   \n\nprotestplot\n\n\n\n\n\n\n\n\n\n\n\n\nRepression\n\n\ncode\nrepressionplot &lt;- ggplot(data=stateviol, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Violence against civilians\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Violence Against Civilians During the Pandemic\") \n  \nrepressionplot\n\n\n\n\n\n\n\n\n\n\n\n\nStates of Emergency during the Pandemic\n\n\ncode\n####################\n#analysis data\n####################\n\n#make analysis data frame\nanalysisdata &lt;- left_join(perday, soe, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"event_date\"))\nanalysisdata &lt;- left_join(analysisdata, covid, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"date\"))\n\n#zeros for NA in soe_s\nanalysisdata &lt;- analysisdata %&gt;% mutate(across(soe_s, ~replace_na(., 0)))\n\n#fill soe series after onset\nanalysisdata &lt;- analysisdata %&gt;% group_by(ccode) %&gt;% mutate(cumSOE = cumsum(soe_s)) %&gt;% ungroup\n\n#plot cumulative states of emergency over time\ntotalsoe &lt;- analysisdata %&gt;% group_by(event_date) %&gt;%\n  summarise(across(cumSOE, sum)) %&gt;%\n  ungroup() \n\ntotalsoe$event_date &lt;- as.Date(totalsoe$event_date)\n\ntotalsoe &lt;- totalsoe %&gt;% filter(event_date &gt;\"2020-01-01\"& event_date &lt;\"2020-07-01\")\n\nggplot() +\n  geom_line(data=totalsoe, aes(x=event_date, y=cumSOE))+\n  scale_x_date(date_breaks = \"1 months\", date_labels = '%b %Y') +\n  labs ( colour = NULL, x = \"Date\", y =  \"Active States of Emergency\" )  \n\n\n\n\n\n\n\n\n\n\n\n\nModeling repression\nLet’s build a simple model predicting violence against civilians. The data here are daily, covering 167 countries between January 2020 and July 2022. We’ll predict the 7 day moving average of repression depending on whether a state of emergency is in place.\n\\(~\\)\nMy expectation is states of emergency will make repression easier to motivate and therefore, more frequent.\n\n\n\nThe model\n\\(y\\) = violence against civilians\n\\(x\\) = states of emergency\nControlling for:\n\nnew daily deaths from COVID (logged)\npercentage of the population over 65 years old\nthe Human Development Index\nthe 7 day average number of protests.\n\n\n\n\nModeling Repression and SoEs\n\n\ncode\n##################\n#models\n#################\n\nanalysisdata$soeprotests &lt;- analysisdata$cumSOE*analysisdata$protest7day\nanalysisdata$lnd =  log(analysisdata$new_deaths+.01)\n#label variables for coef plot\nlibrary(\"labelled\")\nanalysisdata &lt;- analysisdata %&gt;%\n  set_variable_labels(\n    cumSOE = \"State of Emergency\",\n    lnd = \"ln(New COVID deaths)\",\n    aged_65_older = \"% age 65 +\",\n    human_development_index = \"Human Development Index\",\n    protest7day = \"Protests, 7 day avg\",\n    soeprotests = \"SoE * Protests, 7 day avg\", \n    `Violence against civilians` = \"Violence against Civilians\"\n  )\n\n#model predicting repression during covid \n\nm1 &lt;- lm(data=analysisdata, `Violence against civilians` ~ cumSOE  + lnd + aged_65_older+human_development_index + protest7day )\n#summary(m1)  \n\nlibrary(sjPlot)\ntab_model(m1, show.se=TRUE,  p.style=\"stars\", show.ci=FALSE)\n\n\n\n\n\n\n\n\n\n\n \nViolence against\nCivilians\n\n\nPredictors\nEstimates\nstd. Error\n\n\n(Intercept)\n0.93 ***\n0.03\n\n\nState of Emergency\n0.26 ***\n0.01\n\n\nln(New COVID deaths)\n0.02 ***\n0.00\n\n\n%age 65+\n-0.02 ***\n0.00\n\n\nHuman Development Index\n-0.68 ***\n0.04\n\n\nProtests,7 day avg\n0.04 ***\n0.00\n\n\nObservations\n165211\n\n\nR2 / R2 adjusted\n0.057 / 0.057\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\n\nAnother look\n\n\ncode\n#coefficient plot\nggcoef_model(\n  m1,show_p_values = FALSE,\n  signif_stars = FALSE, exponentiate =FALSE, intercept=TRUE,\n  colour=NULL, include = c(\"cumSOE\", \"lnd\", \"aged_65_older\", \"human_development_index\", \"protest7day\")\n)+\n  xlab(\"Average Marginal Effects\") +\n  ggtitle(\"Predictors of Violence Against Civilians\") \n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe estimates indicate:\n\nif all the \\(X\\) variables are set to zero, the expected violence against civilians is 1.16 (the intercept). Can we reasonably set all the \\(X\\) variables to zero?\nthe difference between a state with a SoE and without is different from zero, but quite small; 0.13 uses of violence.\nmore deaths from COVID are associated with more violence against civilians; \\(ln(\\beta) = .05\\). Exponentiate that, and the effect on violence is 1.05, so large compared to anything else in the model.\n\n\n\n\nQuantities of interest (simulation)\nLet’s generate predictions from the model by simulating the distribution of \\(\\widehat{\\beta}\\):\n\nsample 1000 times from a multivariate normal distribution with mean \\(\\widehat{\\beta}\\)s and variances of \\(var(\\widehat{\\beta})\\).\nplug those simulated \\(\\widehat{\\beta}\\)s into the equation using mean/median/mode values for the \\(X\\)s.\n\n\n\n\nSimulating quantities\nFirst, set \\(SoE = 0\\); vary \\(covid~deaths = ln(1 \\ldots 28000)\\), \\(\\%~age~ 65 = 6\\) (the median), \\(HDI = .74\\) (the median), and \\(protests = 2\\) (the median).\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*   0 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\nthen, repeat but with \\(SoE = 1\\):\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*1 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\n\n\n\nPredicted Violence against Civilians\n\n\ncode\n#simulated quantities\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),\n      ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= \n      numeric(0), deaths = numeric(0))\n\nfor (p in seq(0,28000,100)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                       B$b5*2, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                      B$b5*2, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\npredictions$deaths &lt;- log(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"ln(New COVID Deaths)\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5.5, y = .82, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 6.5, y = .55, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nSoE states have higher levels of violence against civilians, but the difference is very small, making me think we haven’t found anything particularly interesting here.\n\nCovid deaths is associated with more violence; though violence nearly doubles over the range of deaths (x-axis), the range is huge - from 1 to 28,000. If we transform the x-axis to deaths rather than \\(ln(deaths)\\), we can see this effect is pretty isolated.\n\n\n\n\nSimulated effects (again)\n\n\ncode\npredictions$deaths &lt;- exp(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"New COVID Deaths\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5000, y = 1, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 10000, y = .65, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe increase in violence occurs almost entirely between zero and 5000 COVID deaths. We can speculate on why this might be; doing so might lead us to wonder how useful this model is.\n\\(~\\) \\(~\\)\nThat’s the primary question we’ll ask this semester."
  },
  {
    "objectID": "probability24s.html#why-probability-distributions",
    "href": "probability24s.html#why-probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns."
  },
  {
    "objectID": "probability24s.html#probability-distributions",
    "href": "probability24s.html#probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Probability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?"
  },
  {
    "objectID": "probability24s.html#things-we-want-to-know-about-x",
    "href": "probability24s.html#things-we-want-to-know-about-x",
    "title": "Basic probability",
    "section": "",
    "text": "\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value)."
  },
  {
    "objectID": "probability24s.html#types-of-variables",
    "href": "probability24s.html#types-of-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Variables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "probability24s.html#discrete-variables",
    "href": "probability24s.html#discrete-variables",
    "title": "Basic probability",
    "section": "",
    "text": "May be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "probability24s.html#continuous-variables",
    "href": "probability24s.html#continuous-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Can be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "probability24s.html#levels-of-measurement",
    "href": "probability24s.html#levels-of-measurement",
    "title": "Basic probability",
    "section": "",
    "text": "These four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "probability24s.html#levels-of-measurement-and-models",
    "href": "probability24s.html#levels-of-measurement-and-models",
    "title": "Basic probability",
    "section": "",
    "text": "In general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24s.html#pdf-and-cdf",
    "href": "probability24s.html#pdf-and-cdf",
    "title": "Basic probability",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nProbability distributions can be described by\n\nProbability Density Function (PDF) which maps \\(X\\) onto the probability space describing the probabilities of every value of \\(X\\), \\(x\\).\nCumulative Distribution Function (CDF) which maps \\(X\\) onto the probability space describing the probability \\(X\\) is less than some value, \\(x\\)."
  },
  {
    "objectID": "probability24s.html#pdf-density",
    "href": "probability24s.html#pdf-density",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nDefinition\n\n\n\nThe Probability Density Function or PDF describes the probability a random variable takes on a particular value, and it does so for all values of that random variable."
  },
  {
    "objectID": "probability24s.html#pdf-density-1",
    "href": "probability24s.html#pdf-density-1",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nExample\n\n\n\nSuppose we toss a fair coin 1 time and want to describe the probability distribution of the outcome, \\(Y\\) which is a random variable. The possible outcomes are heads and tails, and the probability of each is .5. This is the PDF because it describes the probabilities associated with all possible outcomes of \\(Y\\)."
  },
  {
    "objectID": "probability24s.html#pdf-density-2",
    "href": "probability24s.html#pdf-density-2",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\nWhile establishing the probability of a value of a discrete variable is possible, establishing the probability of a particular value of a continuous variable is not.\nInstead, the continuous PDF describes the instantaneous rate of change in \\(Pr(X=x)\\) for every value of \\(x\\)."
  },
  {
    "objectID": "probability24s.html#pdf-plots",
    "href": "probability24s.html#pdf-plots",
    "title": "Basic probability",
    "section": "PDF plots",
    "text": "PDF plots"
  },
  {
    "objectID": "probability24s.html#cdf",
    "href": "probability24s.html#cdf",
    "title": "Basic probability",
    "section": "CDF",
    "text": "CDF\n\n\n\n\n\n\nDefinition\n\n\n\nFor a random variable, \\(Y\\), the probability \\(Y\\) is less than or equal to some particular value, \\(y\\) in the range of \\(Y\\) defines the cumulative density function.\nFor a continuous variable:\n\\[P(Y \\leq j) =\\int\\limits_{-\\infty}^{j} f(X)dX\\]\nFor a discrete variable:\n\\[P(Y \\leq j) = \\sum\\limits_{y\\leq j} P(Y=y)\n= 1- \\sum\\limits_{y&gt; j} P(Y=y)\\]"
  },
  {
    "objectID": "probability24s.html#cdf-plots",
    "href": "probability24s.html#cdf-plots",
    "title": "Basic probability",
    "section": "CDF plots",
    "text": "CDF plots"
  },
  {
    "objectID": "probability24s.html#notation",
    "href": "probability24s.html#notation",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\\(f(x)\\) denotes the PDF of \\(x\\).\n\\(F(x)\\) denotes the CDF of \\(x\\).\nSubstitute either the appropriate name, symbol, or function for \\(F\\) or \\(f\\) to indicate the distribution.\nLower case Greek letters denote PDFs: e.g. \\(f(x)= \\phi(x)\\) denotes the normal PDF.\nUpper case Greek letters denote CDFs: e.g. \\(F(x)=\\Phi(x)\\) denotes the normal CDF."
  },
  {
    "objectID": "probability24s.html#notation-1",
    "href": "probability24s.html#notation-1",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nExample\n\n\n\n\\(x\\) is distributed Normal, \\(N(\\mu, \\sigma^{2})\\):\n\\[F(x) = \\Phi_{\\mu, \\sigma^{2}}(x)  =    \\int  \\phi_{\\mu, \\sigma^{2}} (x) f(x)d(x) \\]\n\\[ f(x) = \\phi_{\\mu, \\sigma^{2}}(x) =\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{(x-\\mu)^{2}}{2 \\sigma^{2}} \\right) \\]\nNote the first derivative of the CDF is the PDF; the integral of the PDF is the CDF."
  },
  {
    "objectID": "probability24s.html#bernoulli",
    "href": "probability24s.html#bernoulli",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nSuppose a binary variable \\(x\\) that takes on only the values of zero and one (\\(x \\in {0, 1}\\)):\n\\[\nPr(X=1)=\\pi\\nonumber \\\\\nPr(x=0)= 1-Pr(x=1) \\\\\n= 1-\\pi   \n\\]"
  },
  {
    "objectID": "probability24s.html#bernoulli-1",
    "href": "probability24s.html#bernoulli-1",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe PDF is:\n\\[ f(x) = \\\\\n          \\pi    ~~~~~~~~ ~ ~ ~~~~ \\text{if } x=1\\\\\n         1-\\pi   ~ ~ ~ ~~~~\\text{if } x=0\n     \\]\nor\n\\[f(x) = \\pi^{x}(1-\\pi)^{1-x} \\]"
  },
  {
    "objectID": "probability24s.html#bernoulli-2",
    "href": "probability24s.html#bernoulli-2",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe CDF is:\n\\[F(x) =\\sum_{x} f(x) \\]\nand the expected value is\n\\[\nE(x) =\\sum_{x}x f(x)    \\\\\n= (1)(\\pi)+(0)(1-\\pi)   \\\\\n= \\pi\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial",
    "href": "probability24s.html#binomial",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nBernoulli is important because it is the foundation for a lot of other distributions, including the binomial distribution. The binomial describes the success probability function (where \\(x=1\\) is a “success”) from a set of \\(n\\) independent Bernoulli trials. So, the binomial is the probability of successes (“ones”) in \\(n\\) independent Bernoulli trials with identical probabilities, \\(\\pi\\).\n\\(~\\)\nThere are two essential parts to the binomial PDF - the probability of success, and the number of ways a success can occur."
  },
  {
    "objectID": "probability24s.html#binomial-1",
    "href": "probability24s.html#binomial-1",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe probability of success is the Bernoulli probability:\n\\[\nf(x) = \\pi^{x}(1-\\pi)^{1-x}\n\\]\nand the number of ways success can occur (called “n-tuples”) is\n\\[  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} = \\frac{n!}{x!(n-x)!}\n\\]\nNotice the notation for the n-tuple."
  },
  {
    "objectID": "probability24s.html#binomial-2",
    "href": "probability24s.html#binomial-2",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe PDF combines these:\n\\[\nf(x)=\n  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} \\pi^{x}(1-\\pi)^{n-x}\n\\]\nThere are \\(n\\) trials, \\(x\\) is the number of successes, and \\(n-x\\) is the number of failures. Each event in the n-tuple arises with the Bernoulli probability."
  },
  {
    "objectID": "probability24s.html#binomial-3",
    "href": "probability24s.html#binomial-3",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are going to toss a fair coin 4 times and want to know how many ways we can have heads come up twice.\n\\[\n\\frac{4!}{2!(4-2)!} = 6\n\\] Now, suppose we want to know the probability exactly 2 of those 4 tosses is heads.\n\\[\nPr(x=2) =\\frac{4!}{2!(4-2)!} (.5)^{2}(.5)^{2} \\\\\n= 6(0.0625) \\\\\n= 0.375\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial-4",
    "href": "probability24s.html#binomial-4",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nHere’s another example: suppose we flip 5 fair coins once each; what is the PDF of the number of heads we expect? In other words, what is the probability associated with each possible number of successes (heads), from 0 to 5?\n\\[\nP(x=0)  =  \\begin{pmatrix}\n    5  \\\\\n    0\n  \\end{pmatrix} \\cdot  (.5)^{0} \\cdot (.5) ^{5} = 1 \\cdot 1 \\cdot 0.03125=0.03125 \\\\\nP(x=1) =  \\begin{pmatrix}\n    5  \\\\\n    1\n  \\end{pmatrix} \\cdot (.5)^{1}\\cdot (.5)^{4} = 5 \\cdot .5 \\cdot 0.0625=0.15625 \\\\\nP(x=2) =  \\begin{pmatrix}\n    5  \\\\\n    2\n  \\end{pmatrix} \\cdot (.5)^{2}\\cdot (.5)^{3} = 10 \\cdot .25 \\cdot 0.125=0.3125 \\\\\n\\vdots \\vdots \\vdots\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial-family-distributions",
    "href": "probability24s.html#binomial-family-distributions",
    "title": "Basic probability",
    "section": "Binomial family distributions",
    "text": "Binomial family distributions\n\nthe geometric distribution describes repeated Bernoulli trials with probability of success \\(\\pi\\), until the first success.\nthe negative binomial describes the number of Bernoulli failures prior to the first success - it can be thought of as a counting process up to the first success.\nthe poisson describes Bernoulli trials where \\(\\pi\\) for any particular trial is very small."
  },
  {
    "objectID": "probability24s.html#the-normal-distribution",
    "href": "probability24s.html#the-normal-distribution",
    "title": "Basic probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe Normal distribution is the most widely used distribution in the social sciences.\n\nThe normal seems to “fit” a lot of the variables social scientists measure and use in models.\nEstimation techniques we often employ assume the disturbance term is normally distributed; one result is the coefficients are either normally distributed or t-distributed."
  },
  {
    "objectID": "probability24s.html#normal-pdf",
    "href": "probability24s.html#normal-pdf",
    "title": "Basic probability",
    "section": "Normal PDF",
    "text": "Normal PDF\nThe normal PDF: \\[\nPr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]\n\\]\nwhere two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\) describe the location and shape of the distribution, the mean and variance respectively; we indicate a normally distributed variable and its parameters as\n\\[\nY \\sim \\text{Normal}(\\mu,\\sigma^{2}) \\nonumber\n\\]\n\n\nDescribe these dataNormal?\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "probability24s.html#standard-normal",
    "href": "probability24s.html#standard-normal",
    "title": "Basic probability",
    "section": "Standard Normal",
    "text": "Standard Normal\nA useful special case of the Normal is the standard normal, \\(z \\sim \\text{Normal}(0,1)\\). The PDF for the standard normal is given by\n\\[\n\\phi(z)=\\frac{1}{\\sqrt{2 \\pi }} exp \\left[\\frac{-z^{2}}{2}\\right] \\nonumber\n\\]\nwhere the parameters themselves drop out since we’d be subtracting a mean of zero and dividing/multiplying by a variance of 1. The standard normal CDF is denoted \\(\\Phi(z)\\); the standard normal PDF is denoted \\(\\phi(z)\\)."
  },
  {
    "objectID": "probability24s.html#normal-pdfs-different-moments",
    "href": "probability24s.html#normal-pdfs-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs different moments",
    "text": "Normal PDFs different moments"
  },
  {
    "objectID": "probability24s.html#models",
    "href": "probability24s.html#models",
    "title": "Basic probability",
    "section": "Models",
    "text": "Models\n\nProbability models include unobserved disturbances; we make assumptions about the distributions of those errors, \\(\\epsilon\\).\nWhat we assume about the unobservables is always informed by what we know about the observables, mainly \\(y\\).\nA useful way to describe or summarize \\(y\\) is to characterize its observed distribution.\nThe distribution of \\(y\\) informs our assumption about the distribution of \\(\\epsilon\\)."
  },
  {
    "objectID": "probability24s.html#why-this-matters-to-ols",
    "href": "probability24s.html#why-this-matters-to-ols",
    "title": "Basic probability",
    "section": "Why this matters to OLS",
    "text": "Why this matters to OLS\nLinear regression is such an inferential model; we have a number of sources of uncertainty. We represent that uncertainty in the model via the disturbance term, \\(\\epsilon\\).\n\\(~\\)\nIn order to know things about \\(\\epsilon\\) and other parts of the model, we assume that the disturbances are normally distributed; \\(y\\) should be normal too."
  },
  {
    "objectID": "probability24s.html#normality-and-centrality",
    "href": "probability24s.html#normality-and-centrality",
    "title": "Basic probability",
    "section": "Normality and Centrality",
    "text": "Normality and Centrality\nWe rely on stuff being normally distributed, and central tendency being meaningful. The Central Limit Theorem facilitates this."
  },
  {
    "objectID": "probability24s.html#central-limit-theorem",
    "href": "probability24s.html#central-limit-theorem",
    "title": "Basic probability",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ {\\sum\\limits_{n \\rightarrow \\infty} (\\bar{X_i})} / {n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "probability24s.html#clt---why-is-this-valuable",
    "href": "probability24s.html#clt---why-is-this-valuable",
    "title": "Basic probability",
    "section": "CLT - Why is this valuable?",
    "text": "CLT - Why is this valuable?\n\nIf we assume repeated sampling and/or infinitely large samples, we can assume normality.\nWe know the properties of the Normal intimately well. We can use this knowledge to evaluate where some value of \\(x\\) lies on the Normal CDF, what the probability less than that value is - we can figure out what the probability of observing that value is (on the PDF).\nAs we’ll see, \\(\\widehat{\\beta}\\) is normally distributed in the OLS model (it is in MLE as well). The CLT and what we know about normality facilitate inference."
  },
  {
    "objectID": "probability24s.html#inference",
    "href": "probability24s.html#inference",
    "title": "Basic probability",
    "section": "Inference",
    "text": "Inference\nInference is our effort to measure and characterize our uncertainty about the model and its parameters.\n\nuncertainty is the most important thing we estimate in inferential models.\ncharacterizing uncertainty relies on probability theory."
  },
  {
    "objectID": "matrix24s.html#matrix-notation",
    "href": "matrix24s.html#matrix-notation",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model."
  },
  {
    "objectID": "matrix24s.html#matrices",
    "href": "matrix24s.html#matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A."
  },
  {
    "objectID": "matrix24s.html#dimensions",
    "href": "matrix24s.html#dimensions",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "The dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar."
  },
  {
    "objectID": "matrix24s.html#symmetric-matrices",
    "href": "matrix24s.html#symmetric-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#rectangular-matrices",
    "href": "matrix24s.html#rectangular-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3)."
  },
  {
    "objectID": "matrix24s.html#vectors",
    "href": "matrix24s.html#vectors",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24s.html#transposition",
    "href": "matrix24s.html#transposition",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nThe transpose of a matrix A is the matrix flipped on its side such that its rows become columns and columns become rows; the transpose of A is denoted \\({\\mathbf A'}\\) and is referred to as “\\({\\mathbf A}\\) transpose.”\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right]\n~~~~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5 & 2\\\\\n\\end{array} \\right] \\]\nso \\({\\mathbf X}\\), a (3,2) matrix, becomes \\({\\mathbf X'}\\), a (2,3) matrix."
  },
  {
    "objectID": "matrix24s.html#transposition-1",
    "href": "matrix24s.html#transposition-1",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nIf \\({\\mathbf A}\\) is symmetric, then \\({\\mathbf A}={\\mathbf A'}\\) – take a look at the following symmetric matrix and you’ll see why:\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#transposition-2",
    "href": "matrix24s.html#transposition-2",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nAlso note that the transpose of a transposed matrix results in the original matrix: \\((\\mathbf{X}')'=\\mathbf{X}\\).\nTransposing a row vector results in a column vector and vice versa:\n\\[\\mathbf{x} =  \\left[\n\\begin{array}{c}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n\\end{array} \\right]\n~~~~~~~~~\n\\mathbf{x'} =  \\left[\n\\begin{array}{cccc}\n-1 & -9 &4 &16\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#trace-of-a-matrix",
    "href": "matrix24s.html#trace-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nThe trace of a matrix is the sum of the diagonal elements of a square matrix, and is denoted \\(tr(\\mathbf{A})=a_{11}+a_{22}+a_{33}\\ldots+a_{nn} = \\sum\\limits_{i=1}^{n}a_{ii}\\)\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nsuch that\n\\(tr(\\mathbf{A})=2+5+2=9\\)"
  },
  {
    "objectID": "matrix24s.html#addition-subtraction",
    "href": "matrix24s.html#addition-subtraction",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\n\nAddition and subtraction of matrices is analogous to the same scalar operations, but depend on the conformatibility of the matrices to be added or subtracted.\nTwo matrices are conformable for addition or subtraction iff they are of the same order.\nNote that conformability means something different for addition/subtraction than for multiplication."
  },
  {
    "objectID": "matrix24s.html#addition-subtraction-1",
    "href": "matrix24s.html#addition-subtraction-1",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider the following (2,2) matrices and the problem \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{C}\\):\n\\[ \\left[\n\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21}& a_{22} \\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{cc}\nb_{11} & b_{12}\\\\\nb_{21}& b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21}& a_{22}+b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\nc_{11} & c_{12}\\\\\nc_{21}& c_{22} \\\\\n\\end{array} \\right] \\]\nAddition and subtraction are only possible among matrices that share the same order; otherwise, they are nonconformable."
  },
  {
    "objectID": "matrix24s.html#addition-subtraction-2",
    "href": "matrix24s.html#addition-subtraction-2",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider a second example of addition, and note that subtraction would follow directly:\n\\[ \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{ccc}\n1 &5 &7\\\\\n3& 7& 1\\\\\n2& 4& 9\\\\\n\\end{array} \\right]\n=\n  \\left[\n\\begin{array}{ccc}\n3 &9 &15\\\\\n7& 12& 4\\\\\n10& 7& 11\\\\\n\\end{array} \\right] \\]\nMatrix addition adheres to the commutative and associative properties such that:\n\\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) (Commutative), and \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}= \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) (Associative)."
  },
  {
    "objectID": "matrix24s.html#multiplication",
    "href": "matrix24s.html#multiplication",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\nLet’s begin by multiplying a scalar and a matrix - the product is a matrix whose elements are the scalar multiplied by each element of the original matrix, so:\n\\[ \\beta\n\\left[\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\n\\beta x_{11} & \\beta x_{12}\\\\\n\\beta x_{21}&  \\beta x_{22} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#multiplication-1",
    "href": "matrix24s.html#multiplication-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\n\nIn order to multiply two matrices (or vectors), they must be conformable for multiplication.\nTwo matrices are conformable for multiplication iff the number of columns in the first matrix is equal to the number of rows in the second matrix. That is, \\(\\mathbf{A_{i,j}}\\) and \\(\\mathbf{B_{j,k}}\\).\nAn easy way to think of this is to write the dimensions of the two matrices, (i,j),(j,k) - the inner dimensions are the same, so the matrix is conformable for multiplication - moreover, the outer dimensions (i,k) give the dimension of the product matrix."
  },
  {
    "objectID": "matrix24s.html#multiplication---inner-product",
    "href": "matrix24s.html#multiplication---inner-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\n\nTo illustrate multiplication, let’s start with a column vector, \\(\\mathbf{e}\\) whose order is (N,1) and take its inner product.\nThe inner product of a column vector is the transpose of the column vector (thus, a row vector) post-multiplied by the column vector, so \\({\\mathbf e'\\mathbf e}\\). The inner product is a scalar."
  },
  {
    "objectID": "matrix24s.html#multiplication---inner-product-1",
    "href": "matrix24s.html#multiplication---inner-product-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\nWhen we transpose the column vector \\({\\mathbf e}\\) of (N,1) order, we get a row vector \\({\\mathbf e'}\\) of (1,N) order. Inner dimensions match, so they are conformable.\n\\[\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n~~~~~\n\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right] \\] \\[=\n(-1 \\cdot -1)+(-9 \\cdot -9)+(4 \\cdot 4)+ (16 \\cdot 16)+ (-10 \\cdot -10) = 454\\]"
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing",
    "href": "matrix24s.html#least-squares-foreshadowing",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nNote the inner product of a column vector is a scalar; the inner product of \\(\\mathbf{e}\\), is the sum of the squares of \\(\\mathbf{e}\\).\n\\[\\mathbf{e'e}= e_{1}e_{1}+e_{2}e_{2}+\\ldots e_{N}e_{N} = \\sum\\limits_{i=1}^{N}e_{i}^{2}\\]"
  },
  {
    "objectID": "matrix24s.html#multiplication---outer-product",
    "href": "matrix24s.html#multiplication---outer-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - outer product",
    "text": "Multiplication - outer product\nThe outer product of a column vector is the transpose of the column vector (thus, a row vector) pre-multiplied by the column vector, so \\({\\mathbf e\\mathbf e'}\\). The outer product is an (N,N) matrix.\n\\[\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right]\n\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n\\]\nLet’s multiply \\({\\mathbf e\\mathbf e'}\\), a column vector of (5,1) by a row vector of (1,5); we’ll obtain a square matrix of (5,5)."
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing-1",
    "href": "matrix24s.html#least-squares-foreshadowing-1",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nLet \\(\\mathbf{e}\\) represent the residuals or errors in our regression. The outer product\n\\[{\\mathbf e\\mathbf e'} =  \\left[\n\\begin{array}{rrrrr}\n1 & 9 &-4 &-16 &10\\\\\n9 & 81 &-36 &-144 &90\\\\\n-4 & -36 &16 &64 &-40\\\\\n-16 & -144 &64 &256 &-160\\\\\n10 & 90 &-40 &-160 &100\\\\\n\\end{array} \\right] \\]\nis the variance-covariance matrix of \\(\\mathbf{e}\\). The squares on the main diagonal (which sums to the sum of squares) and the symmetry of the off-diagonal elements."
  },
  {
    "objectID": "matrix24s.html#inverting-matrices-1",
    "href": "matrix24s.html#inverting-matrices-1",
    "title": "Matrix algebra basics",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\n\nYou’ll have noticed that division has been conspicuously absent so far. In scalar algebra, we sometimes represent division in fractions, \\(\\frac{1}{2}\\).\nAnother way to represent the same quantity is \\(2^{-1}\\), or the inverse of 2. Of course, in scalar algebra, a number multiplied by its inverse equals 1, e.g., \\(2 \\cdot 2^{-1}=1\\) or \\(2 \\cdot \\frac{1}{2}=1\\). So, if we are given \\(4x=1\\) and want to solve for \\(x\\) what we really want to know is “what is the inverse of 4 such that \\(4 \\cdot 4^{-1}=1\\)?” We consider matrices in a similar manner."
  },
  {
    "objectID": "matrix24s.html#inverting-square-matrices",
    "href": "matrix24s.html#inverting-square-matrices",
    "title": "Matrix algebra basics",
    "section": "Inverting Square Matrices",
    "text": "Inverting Square Matrices\nImagine a square matrix, \\(\\mathbf{X}\\) and its inverse, denoted \\({\\mathbf{X^{-1}}}\\) (read as “X inverse”). Analogous to scalar algebra, a matrix multiplied by its inverse is equal to the identity matrix, \\(\\mathbf{I}\\).\nSo in order to find \\({\\mathbf{X^{-1}}}\\), we must find the matrix that, when multiplied by \\(\\mathbf{X}\\), produces \\(\\mathbf{I}\\). Thus, for a square matrix, its inverse (if it exists) is such that\n\\[\\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X^{-1}} \\mathbf{X}=\\mathbf{I}\\]\nNotice the commutative property at work here - this is because \\(\\mathbf{X}\\) is square, such that \\(n=m\\), so \\(\\mathbf{X}\\) and \\({\\mathbf{X^{-1}}}\\) have the same dimensions."
  },
  {
    "objectID": "matrix24s.html#determinant",
    "href": "matrix24s.html#determinant",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nAn important characteristic of square matrices is the determinant. The determinant, denoted \\(|X|\\), is a scalar; every square matrix has one. We evaluate the determinant by examining the cross-products of the matrice’s elements. This is simple in the 2x2 matrix, less so in larger matrices.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right| = x_{11}x_{22}-x_{12}x_{21}\\]"
  },
  {
    "objectID": "matrix24s.html#determinant-1",
    "href": "matrix24s.html#determinant-1",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nIn the 2x2 matrix, the determinant is the cross-product of the main diagonals minus the cross-product of the off-diagonals.\n\\[ |\\mathbf{X_{1}}|=\\left|\n\\begin{array}{cc}\n2 & 4\\\\\n6& 3 \\\\\n\\end{array} \\right| = 2 \\cdot 3-4 \\cdot 6 = -18\n\\]\nThe determinant is -18; \\(|\\mathbf{X_{1}}|=-18\\)."
  },
  {
    "objectID": "matrix24s.html#determinant-2",
    "href": "matrix24s.html#determinant-2",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nThis matrix has a determinant of zero - this is an important case.\n\\[ |\\mathbf{X_{2}}|=\\left|\n\\begin{array}{cc}\n3 & 6\\\\\n2& 4 \\\\\n\\end{array} \\right| = 3 \\cdot 4-6 \\cdot 2 =0\n\\] A matrix with determinant zero is singular. A singluar matrix has no inverse."
  },
  {
    "objectID": "matrix24s.html#singular-matrices",
    "href": "matrix24s.html#singular-matrices",
    "title": "Matrix algebra basics",
    "section": "Singular matrices",
    "text": "Singular matrices\nThere are some important conditions that will produce a determinant of zero and thus a singular matrix:\n\nIf all the elements of any row or column of a matrix are equal to zero, then the determinant is zero.\nIf two rows or columns of a matrix are identical, the determinant is zero.\nIf a row or column of a matrix is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing-2",
    "href": "matrix24s.html#least-squares-foreshadowing-2",
    "title": "Matrix algebra basics",
    "section": "Least Squares Foreshadowing",
    "text": "Least Squares Foreshadowing\nFor the linear model in least squares, this is important because computing estimates of \\(\\beta\\) requires inverting a matrix. Let’s derive \\(\\beta\\) in matrix notation to see how:\nThe estimated model is:\n\\[\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}}\\] Minimizing \\(\\widehat{\\mathbf{\\epsilon}}'\\widehat{\\mathbf{\\epsilon}}\\) (skipping the math for now), we get\n\\[(\\mathbf{X'X}) \\widehat{\\beta}=\\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares",
    "href": "matrix24s.html#foreshadowing-least-squares",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThe matrix \\((\\mathbf{X'X})\\) gives the sums of squares (on the main diagonal) and the sums of cross products (in the off-diagonals) of all the \\(\\mathbf{X}\\) variables; the matrix is symmetric. Since \\(\\beta\\) is the unknown vector in this equation, solve for \\(\\beta\\) by dividing both sides by \\((\\mathbf{X'X})\\) - in matrix terms, we premultiply each side by \\((\\mathbf{X'X)^{-1}}\\):\n\\[(\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) \\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares-1",
    "href": "matrix24s.html#foreshadowing-least-squares-1",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nAs we know, \\((\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) = \\mathbf{I}\\). So,\n\\[\\mathbf{I}\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\] or \\[\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]\nEstimating \\(\\widehat{\\beta}\\) requires inverting \\(\\mathbf{(X'X)}\\) If the matrix \\(\\mathbf{(X'X)}\\) is singular, then \\(\\mathbf{(X'X)^{-1}}\\) does not exist, and we cannot estimate \\(\\widehat{\\beta}\\). Least Squares fails."
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares-2",
    "href": "matrix24s.html#foreshadowing-least-squares-2",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThinking specifically of the \\(\\mathbf{X}\\) variables in the model, any of these conditions produce perfect collinearity - estimation fails because the matrix can’t invert.\n\nIf all the elements of any row or column of \\(\\mathbf{X}\\) are equal to zero, then the determinant is zero.\nIf two rows or columns of \\(\\mathbf{X}\\) are identical, the determinant is zero.\nIf a row or column of \\(\\mathbf{X}\\) is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24s.html#examples",
    "href": "matrix24s.html#examples",
    "title": "Matrix algebra basics",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2Example 3\n\n\n\\({\\mathbf X}\\) below has a row of zeros; compute the determinant using the difference of cross-products.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n0 & 0\\\\\n19& 12 \\\\\n\\end{array} \\right| = 0 \\cdot 12-0 \\cdot 19 = 0 \\]\nYou can see in \\({\\mathbf X}\\) that because all the cross-products involve multiplying by zero, the determinant will always be zero; this applies to larger matrices as well.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n5 & 7\\\\\n5& 7 \\\\\n\\end{array} \\right| = 5 \\cdot 7-5 \\cdot 7 =0 \\]\nIn \\({\\mathbf X}\\) it’s easy to see that the cross-products will always be equal to one another if the rows or columns are identical, and the difference between identical values is zero.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n64 & 48\\\\\n16& 12 \\\\\n\\end{array} \\right| = 64 \\cdot 12-48 \\cdot 16 = 768-768=0\\]\nFinally \\({\\mathbf X}\\) shows that if one row or column is a linear combination of other rows or columns, the determinant will be zero; in that matrix, the top row is 4 times the bottom row."
  },
  {
    "objectID": "matrix24s.html#inversion",
    "href": "matrix24s.html#inversion",
    "title": "Matrix algebra basics",
    "section": "Inversion",
    "text": "Inversion\nThe matrix we need to invert, \\(\\mathbf{X'X}\\) is rectangular, so inversion is more complex. I’m going to illustrate the method of cofactor expansion - the purpose here is to give you an idea what software does every time you estimate an OLS model."
  },
  {
    "objectID": "matrix24s.html#minor-of-a-matrix",
    "href": "matrix24s.html#minor-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Minor of a matrix",
    "text": "Minor of a matrix\nThe minor of a matrix is the determinant of a submatrix created by deleting the \\(i\\)th row and the \\(j\\)th column of the full matrix, where the minor is denoted by the element where the deleted rows intersect.\n\\[ \\mathbf{A} =  \\left[\n\\begin{array}{ccc}\na_{11} & a_{12} &a_{13}\\\\\na_{21}& a_{22} &a_{23}\\\\\na_{31} &a_{32} & a_{33}\\\\\n\\end{array} \\right] \\] \\[\\text{so the minor of } a_{11} \\text{ is }\n|\\mathbf{M_{11}}| =  \\left|\n\\begin{array}{cc}\na_{22} &a_{23}\\\\\na_{32} & a_{33}\\\\\n\\end{array} \\right|  =  a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\]"
  },
  {
    "objectID": "matrix24s.html#cofactor-matrix",
    "href": "matrix24s.html#cofactor-matrix",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nThe cofactor of an element (\\(c_{ij}\\)) is the minor with a positive or negative sign depending on whether \\(i+j\\) is odd (negative) or even (positive). This is given by:\n\\[\\theta_{ij}=(-1)^{i+j}|\\mathbf{M_{ij}}|\\]\nso, from the example finding the minor of \\(a_{11}\\), \\(i\\)=1 and \\(j=1\\); their sum is 2 which is even, so the cofactor is \\(+a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\). If we find the cofactor for every element, \\(a_{ij}\\) in a matrix and replace each element with its cofactor, the new matrix is called the cofactor matrix of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24s.html#cofactor-matrix-1",
    "href": "matrix24s.html#cofactor-matrix-1",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nWhat we have so far is the signed matrix of minors:\n\\[\\mathbf{A} = \\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#cofactor-expansion",
    "href": "matrix24s.html#cofactor-expansion",
    "title": "Matrix algebra basics",
    "section": "Cofactor Expansion",
    "text": "Cofactor Expansion\nFind the signed determinant of each 2x2 submatrix.\n\\[\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} = \\left[\n\\begin{array}{rrr}\n10&-10&0\\\\\n-12&0&6\\\\\n-2&5&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#find-the-determinant",
    "href": "matrix24s.html#find-the-determinant",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nUsing the cofactor matrix, \\(\\mathbf{\\Theta_{A}}\\) we perform cofactor expansion in order to find the determinant, \\(|\\mathbf{A}|\\). Cofactor expansion is given by:\n\\[|\\mathbf{A}|=\\sum\\limits_{i,j=1}^{n}a_{ij}\\Theta_{ij}\\]\nwhich means that we take two corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) and sum the products of their elements - this gives us the determinant of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24s.html#find-the-determinant-1",
    "href": "matrix24s.html#find-the-determinant-1",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nSo taking the first row from \\(\\mathbf{A}\\) (1,4,2) and from \\(\\mathbf{\\Theta_{A}}\\) (10,-10,0) above, we compute\n\\[a_{11} \\cdot \\Theta_{A11}+a_{12} \\cdot \\Theta_{A12}+a_{13} \\cdot \\Theta_{A13}\\]\n\\[1 \\cdot 10+ 4 \\cdot -10 + 2 \\cdot 0 = -30\\] This is the determinant of \\(\\mathbf{A}\\) , \\(|\\mathbf{A}|\\). Note that any corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) will produce the same result."
  },
  {
    "objectID": "matrix24s.html#adjoint-matrix",
    "href": "matrix24s.html#adjoint-matrix",
    "title": "Matrix algebra basics",
    "section": "Adjoint Matrix",
    "text": "Adjoint Matrix\nIf we transpose the cofactor matrix, (cof \\(\\mathbf{A'}\\)), the new matrix is called the adjoint matrix, denoted adj\\(\\mathbf{A}\\)=(cof \\(\\mathbf{A'}\\)).\n\\[adj\\mathbf{A} = \\mathbf{\\Theta_{A}'} = \\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#inverting-the-matrix",
    "href": "matrix24s.html#inverting-the-matrix",
    "title": "Matrix algebra basics",
    "section": "Inverting the matrix",
    "text": "Inverting the matrix\nBelieve it or not, this all leads us to inverting the matrix, provided it is nonsingular.\n\\[\\mathbf{A^{-1}}=\\frac{1}{|\\mathbf{A}|}(adj {\\mathbf A})\\] The inverse of A is equal to one over the determinant of A times the adjoint matrix of A. So we’re pre-multiplying \\(adj {\\mathbf A}\\) by the (scalar) determinant, -30."
  },
  {
    "objectID": "matrix24s.html#inverse-of-matrix-a",
    "href": "matrix24s.html#inverse-of-matrix-a",
    "title": "Matrix algebra basics",
    "section": "Inverse of Matrix A",
    "text": "Inverse of Matrix A\n\\[{\\mathbf A^{-1}}=-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n=\\left[\n\\begin{array}{rrr}\n-\\frac{1}{3}&\\frac{2}{5}&\\frac{1}{15}\\\\\n\\frac{1}{3}&0&-\\frac{1}{6}\\\\\n0&-\\frac{1}{5}&\\frac{3}{10}\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#checking-our-work",
    "href": "matrix24s.html#checking-our-work",
    "title": "Matrix algebra basics",
    "section": "Checking our work",
    "text": "Checking our work\nWe can check our work to be sure we’ve inverted correctly by making sure that \\(\\mathbf{A^{-1}A}=\\mathbf{I_{3}}\\); so,\n\\[-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n\\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n=\n-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n-30&0&0\\\\\n0&-30&0\\\\\n0&0&-30\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols",
    "href": "matrix24s.html#connecting-data-matrices-to-ols",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\nLet’s examine these matrices a little to get a feel for what the computation of \\(\\widehat{\\beta}\\) involves: \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\n\\[\n\\mathbf{X}= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \n\\end{matrix}  \\right]\n\\] The dimensions of \\(\\mathbf{X}\\) are \\(n x k\\); the sample size by the number of regressors (including the constant)."
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols-1",
    "href": "matrix24s.html#connecting-data-matrices-to-ols-1",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \n\\end{matrix}  \\right] \\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols-2",
    "href": "matrix24s.html#connecting-data-matrices-to-ols-2",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i}\n\\end{matrix}  \\right] \\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationships among \\(X_k\\)."
  },
  {
    "objectID": "overview24s.html",
    "href": "overview24s.html",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise\n\n\n\n\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding.\n\n\n\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling.\n\n\n\n\nIs not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal.\n\n\n\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here.\n\n\n\n\n\nis a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together.\n\n\n\n\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra\n\n\n\n\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\).\n\n\n\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?\n\n\n\n\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term.\n\n\n\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later.\n\n\n\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "syllabus24.html",
    "href": "syllabus24.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dave Clark\n   LNG 57; LN2430\n   dclark@binghamton.edu\n   clavedark\noffice hours: M 1:30-3:30pm\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL"
  },
  {
    "objectID": "syllabus24.html#seminar-description",
    "href": "syllabus24.html#seminar-description",
    "title": "Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret an informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "syllabus24.html#course-purpose",
    "href": "syllabus24.html#course-purpose",
    "title": "Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "syllabus24.html#learning-objectives",
    "href": "syllabus24.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "syllabus24.html#reading",
    "href": "syllabus24.html#reading",
    "title": "Syllabus",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways.\nIn addition, I’ll assign a small number of articles over the course of the term."
  },
  {
    "objectID": "syllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "syllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it.\n\n\nHow to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#on-reading-or-how-to-read",
    "href": "syllabus24.html#on-reading-or-how-to-read",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "On reading, or How to Read",
    "text": "On reading, or How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever. \\\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing. \\\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#course-requirements",
    "href": "syllabus24.html#course-requirements",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Requirements",
    "text": "Course Requirements\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in \\(\\LaTeX\\) or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n“D”, “60-69%”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;70%"
  },
  {
    "objectID": "syllabus24.html#how-to-read",
    "href": "syllabus24.html#how-to-read",
    "title": "Syllabus",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#course-requirements-and-grades",
    "href": "syllabus24.html#course-requirements-and-grades",
    "title": "Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "syllabus24.html#course-schedule",
    "href": "syllabus24.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013\nWeek 1. 17 Jan Introduction, Regression Discussion\n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "pdf.html",
    "href": "pdf.html",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "",
    "text": "Prof. Dave Clark\n   LNG 57\n   dclark@binghamton.edu\n   clavedark\n\n\n\n\n\n   Kathleen Bannon\n   kbannon1@binghamton.edu\n\n\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL\n\n\n\n\n\n   Thursday\n   1:15-2:15\n   CW 309"
  },
  {
    "objectID": "pdf.html#seminar-description",
    "href": "pdf.html#seminar-description",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret and informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "pdf.html#course-purpose",
    "href": "pdf.html#course-purpose",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "pdf.html#learning-objectives",
    "href": "pdf.html#learning-objectives",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "pdf.html#reading",
    "href": "pdf.html#reading",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways."
  },
  {
    "objectID": "pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it."
  },
  {
    "objectID": "pdf.html#how-to-read",
    "href": "pdf.html#how-to-read",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "pdf.html#course-requirements-and-grades",
    "href": "pdf.html#course-requirements-and-grades",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in \\(\\LaTeX\\) or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "pdf.html#course-schedule",
    "href": "pdf.html#course-schedule",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013}\nWeek 1. 17 Jan Introduction, Regression Discussion \n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "syllabus24.html#attendance",
    "href": "syllabus24.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class."
  },
  {
    "objectID": "syllabus24.html#academic-integrity",
    "href": "syllabus24.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "syllabus24.html#course-policies",
    "href": "syllabus24.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Font Awesome Quarto Extension",
    "section": "",
    "text": "This extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; fa &lt;icon-name&gt; &gt;}}\nOptional &lt;group&gt;, &lt;size=...&gt;, and &lt;title=...&gt;:\n{{&lt; fa &lt;group&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;title=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n{{&lt; fa brands twitter size=2xl &gt;}} (HTML only)\n\n\n\n{{&lt; fa brands github size=5x &gt;}} (HTML only)\n\n\n\n{{&lt; fa battery-half size=Huge &gt;}}\n\n\n\n{{&lt; fa envelope title=\"An envelope\" &gt;}}\n\n\n\n\nNote that the icon sets are currently not perfectly interchangeable across formats:\n\nhtml uses FontAwesome 6.4.2\npdf uses the fontawesome5 package, based on FontAwesome 5.\nOther formats are currently not supported, but PRs are always welcome!"
  },
  {
    "objectID": "syllabus24pdf.html",
    "href": "syllabus24pdf.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dave Clark\n   LNG 57; LN2430\n   dclark@binghamton.edu\n   clavedark\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL"
  },
  {
    "objectID": "syllabus24pdf.html#seminar-description",
    "href": "syllabus24pdf.html#seminar-description",
    "title": "Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret and informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "syllabus24pdf.html#course-purpose",
    "href": "syllabus24pdf.html#course-purpose",
    "title": "Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "syllabus24pdf.html#learning-objectives",
    "href": "syllabus24pdf.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "syllabus24pdf.html#reading",
    "href": "syllabus24pdf.html#reading",
    "title": "Syllabus",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways.\nIn addition, I’ll assign a small number of articles over the course of the term."
  },
  {
    "objectID": "syllabus24pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "syllabus24pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it.\n\n\nHow to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24pdf.html#course-requirements-and-grades",
    "href": "syllabus24pdf.html#course-requirements-and-grades",
    "title": "Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "syllabus24pdf.html#course-policies",
    "href": "syllabus24pdf.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "syllabus24pdf.html#course-schedule",
    "href": "syllabus24pdf.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013\nWeek 1. 17 Jan Introduction, Regression Discussion\n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "data.html#get-to-know-your-data-explore-etc",
    "href": "data.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "data.html#data-generating-process",
    "href": "data.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\nA Terrible Map\n\nWar outcomes\n\nwar outcome/duration data\n\nObservability\n\nAsking the right question"
  },
  {
    "objectID": "data.html#data-in-the-regression-context",
    "href": "data.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "data.html#data",
    "href": "data.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "data.html#types-of-variables",
    "href": "data.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "data.html#discrete-variables",
    "href": "data.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "data.html#continuous-variables",
    "href": "data.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "data.html#levels-of-measurement",
    "href": "data.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "data.html#levels-of-measurement-and-models",
    "href": "data.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "data.html#describe-these-data",
    "href": "data.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "data.html#describe-these-data-1",
    "href": "data.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "data.html#overstaying-terms",
    "href": "data.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata.html",
    "href": "thinkingdata.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata.html#data-generating-process",
    "href": "thinkingdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\n\nA Terrible Map\n\n\n\nWar outcomes\n\nwar outcome/duration data\n\n\n\nObservability\n\n\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata.html#data-in-the-regression-context",
    "href": "thinkingdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata.html#data",
    "href": "thinkingdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata.html#types-of-variables",
    "href": "thinkingdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata.html#discrete-variables",
    "href": "thinkingdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata.html#continuous-variables",
    "href": "thinkingdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata.html#levels-of-measurement",
    "href": "thinkingdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata.html#levels-of-measurement-and-models",
    "href": "thinkingdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata.html#describe-these-data",
    "href": "thinkingdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata.html#describe-these-data-1",
    "href": "thinkingdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata.html#overstaying-terms",
    "href": "thinkingdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html",
    "href": "thinkingdata/thinking-data24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata/thinking-data24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data-generating-process",
    "href": "thinkingdata/thinking-data24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\n\nA Terrible Map\n\n\n\nWar outcomes\n\nwar outcome/duration data\n\n\n\nObservability\n\n\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data-in-the-regression-context",
    "href": "thinkingdata/thinking-data24.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data",
    "href": "thinkingdata/thinking-data24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#types-of-variables",
    "href": "thinkingdata/thinking-data24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#discrete-variables",
    "href": "thinkingdata/thinking-data24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#continuous-variables",
    "href": "thinkingdata/thinking-data24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#levels-of-measurement",
    "href": "thinkingdata/thinking-data24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#levels-of-measurement-and-models",
    "href": "thinkingdata/thinking-data24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#describe-these-data",
    "href": "thinkingdata/thinking-data24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#describe-these-data-1",
    "href": "thinkingdata/thinking-data24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#overstaying-terms",
    "href": "thinkingdata/thinking-data24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata/tdata.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata/tdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#data-generating-process",
    "href": "thinkingdata/tdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\nA Terrible Map\n\nWar outcomes\n\nwar outcome/duration data\n\nObservability\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata/tdata.html#data-in-the-regression-context",
    "href": "thinkingdata/tdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata/tdata.html#data",
    "href": "thinkingdata/tdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata/tdata.html#types-of-variables",
    "href": "thinkingdata/tdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata/tdata.html#discrete-variables",
    "href": "thinkingdata/tdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata/tdata.html#continuous-variables",
    "href": "thinkingdata/tdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata/tdata.html#levels-of-measurement",
    "href": "thinkingdata/tdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata/tdata.html#levels-of-measurement-and-models",
    "href": "thinkingdata/tdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata/tdata.html#describe-these-data",
    "href": "thinkingdata/tdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#describe-these-data-1",
    "href": "thinkingdata/tdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#overstaying-terms",
    "href": "thinkingdata/tdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "tdata.html#get-to-know-your-data-explore-etc",
    "href": "tdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "tdata.html#data-generating-process",
    "href": "tdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "tdata.html#data-in-the-regression-context",
    "href": "tdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "tdata.html#data",
    "href": "tdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "tdata.html#types-of-variables",
    "href": "tdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "tdata.html#discrete-variables",
    "href": "tdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "tdata.html#continuous-variables",
    "href": "tdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "tdata.html#levels-of-measurement",
    "href": "tdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "tdata.html#levels-of-measurement-and-models",
    "href": "tdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "tdata.html#describe-these-data",
    "href": "tdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "tdata.html#describe-these-data-1",
    "href": "tdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "tdata.html#overstaying-terms",
    "href": "tdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinking-data24.html",
    "href": "thinking-data24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinking-data24.html#get-to-know-your-data-explore-etc",
    "href": "thinking-data24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinking-data24.html#data-generating-process",
    "href": "thinking-data24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "thinking-data24.html#data-in-the-regression-context",
    "href": "thinking-data24.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinking-data24.html#data",
    "href": "thinking-data24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinking-data24.html#types-of-variables",
    "href": "thinking-data24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinking-data24.html#discrete-variables",
    "href": "thinking-data24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinking-data24.html#continuous-variables",
    "href": "thinking-data24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinking-data24.html#levels-of-measurement",
    "href": "thinking-data24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinking-data24.html#levels-of-measurement-and-models",
    "href": "thinking-data24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinking-data24.html#describe-these-data",
    "href": "thinking-data24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinking-data24.html#describe-these-data-1",
    "href": "thinking-data24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinking-data24.html#overstaying-terms",
    "href": "thinking-data24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "tdata.html#a-terrible-map",
    "href": "tdata.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "tdata.html#war-outcomes",
    "href": "tdata.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "tdata.html#observability",
    "href": "tdata.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "tdata.html#asking-the-right-question",
    "href": "tdata.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinking-data24.html#a-terrible-map",
    "href": "thinking-data24.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "thinking-data24.html#war-outcomes",
    "href": "thinking-data24.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "thinking-data24.html#observability",
    "href": "thinking-data24.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "thinking-data24.html#asking-the-right-question",
    "href": "thinking-data24.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinking-data24.html#ooof",
    "href": "thinking-data24.html#ooof",
    "title": "Thinking About Data",
    "section": "ooof",
    "text": "ooof\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1"
  },
  {
    "objectID": "tdata.html#ooof",
    "href": "tdata.html#ooof",
    "title": "Thinking About Data",
    "section": "ooof",
    "text": "ooof\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1"
  },
  {
    "objectID": "thinking-data24.html#data-generating-process-1",
    "href": "thinking-data24.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinking-data24.html#war-outcomes-lake92-p.-31",
    "href": "thinking-data24.html#war-outcomes-lake92-p.-31",
    "title": "Thinking About Data",
    "section": "War outcomes (Lake (1992) p. 31)",
    "text": "War outcomes (Lake (1992) p. 31)\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74\n47\n12 1\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "bivariate24.html",
    "href": "bivariate24.html",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Regression in any form is based on the conditional expectation of \\(Y\\) - the expected value of \\(Y\\) is conditional on some \\(X\\)s, but we don’t know the actual conditions or effects of those \\(X\\)s. So we can write the regression like this:\n\\[E[y|X_{1}, \\ldots,X_{k}]= \\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\]\n\n\nLet \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances.\n\n\n\nThe predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\).\n\n\n\nThe differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\).\n\n\n\nRestating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#regression",
    "href": "bivariate24.html#regression",
    "title": "The Bivariate Model",
    "section": "Regression",
    "text": "Regression\nRegression in any form is based on the conditional expectation of \\(Y\\) - the expected value of \\(Y\\) is conditional on some \\(X\\)s, but we don’t know the actual conditions or effects of those \\(X\\)s. So we can write the regression like this:\n\\[E[y|X_{1}, \\ldots,X_{k}]= \\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\]"
  },
  {
    "objectID": "bivariate24.html#regression-1",
    "href": "bivariate24.html#regression-1",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Let \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "bivariate24.html#linear-predictions-residuals",
    "href": "bivariate24.html#linear-predictions-residuals",
    "title": "The Bivariate Model",
    "section": "Linear predictions, Residuals",
    "text": "Linear predictions, Residuals\nThe predicted points that form the line are \\(\\widehat{Y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\).\nThe differences between those predicted points, \\(\\widehat{Y_{i}}\\) and the observed values \\(Y_i\\) are:\n\\[Y_{i}-\\widehat{Y_{i}} = e  \\\\\n= Y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i} \\nonumber \\\\\n= Y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\]\nThese are the residuals, \\(e\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#in-matrix",
    "href": "bivariate24.html#in-matrix",
    "title": "The Bivariate Model",
    "section": "in matrix",
    "text": "in matrix\nRestating in matrix notation:\n\\[\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{e_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{e_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-i",
    "href": "bivariate24.html#getting-to-ols-i",
    "title": "The Bivariate Model",
    "section": "Getting to OLS I",
    "text": "Getting to OLS I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-1",
    "href": "bivariate24.html#ols-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nTo minimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]\n\\(\\ldots\\) illustrating explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24.html#ols-1-1",
    "href": "bivariate24.html#ols-1-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-ii",
    "href": "bivariate24.html#getting-to-ols-ii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS II",
    "text": "Getting to OLS II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero.\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling that the mean of a variable\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]\nSo, substituting these equations into\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\\]\nand solving for \\(\\hat{\\beta}_1\\) gives:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-iii",
    "href": "bivariate24.html#getting-to-ols-iii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS III",
    "text": "Getting to OLS III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24.html#matrices",
    "href": "bivariate24.html#matrices",
    "title": "The Bivariate Model",
    "section": "Matrices",
    "text": "Matrices\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#covariation-xy",
    "href": "bivariate24.html#covariation-xy",
    "title": "The Bivariate Model",
    "section": "Covariation X’y",
    "text": "Covariation X’y\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\) and \\(X_{2}\\)."
  },
  {
    "objectID": "bivariate24.html#matrices-and-regression",
    "href": "bivariate24.html#matrices-and-regression",
    "title": "The Bivariate Model",
    "section": "Matrices and Regression",
    "text": "Matrices and Regression\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \\nonumber \\\\\n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#check-this-out",
    "href": "bivariate24.html#check-this-out",
    "title": "The Bivariate Model",
    "section": "check this out",
    "text": "check this out\nagain: \\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \\nonumber \\\\\n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#matrices-and-regression-1",
    "href": "bivariate24.html#matrices-and-regression-1",
    "title": "The Bivariate Model",
    "section": "Matrices and Regression",
    "text": "Matrices and Regression\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \\nonumber \\\\\n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \\nonumber \\\\\n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \\nonumber \\\\\n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\n\\[\n\\begin{align}\nE(\\mathbf{\\epsilon\\epsilon'})= E \\left[\n\\begin{array}{c}\n\\epsilon_{1} \\\\\n\\vdots \\\\\n\\epsilon_{n}\\\\\n\\end{array} \\right]\nE \\left[\n\\begin{array}{cccc}\n\\epsilon_{1} & \\cdots &\\epsilon_{n}\\\\\n\\end{array} \\right] \\\\ ~\\\\\n= E\n\\left[\n\\begin{array}{cccc}\n\\epsilon_{1}^{2} & \\epsilon_{1}\\epsilon_{2} &\\cdots &\\epsilon_{1}\\epsilon_{n}\\\\\n\\epsilon_{2}\\epsilon_{1}& \\epsilon_{2}^{2} &\\cdots &\\epsilon_{2}\\epsilon_{n}\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n\\epsilon_{n}\\epsilon_{1} &\\epsilon_{n}\\epsilon_{2} &\\cdots & \\epsilon_{n}^{2}\\\\\n\\end{array} \\right]\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon-1",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[\nE(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0\\\\\n0&\\sigma^{2} &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & \\sigma^{2}\\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0\\\\\n0&1 &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & 1\\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I}\n\\]\nThis is the variance-covariance matrix of the disturbances (var-cov(e)); it is symmetric, the main diagonal containing the variances, the off-diagonal containing the covariances."
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\nThe variance-covariance of the estimate, \\(\\widehat{\\beta}\\) derives as follows:\n\\[\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'y}\\]\nsubstituting \\(\\mathbf{X\\beta}+\\epsilon\\) for \\(\\mathbf{y}\\), we get\n\\[\n\\begin{align}\n\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'(X\\beta + \\epsilon)}  \\nonumber \\\\  \n= \\mathbf{(X'X)^{-1}}\\mathbf{X'X\\beta}+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\  \n=\\beta+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\\n\\widehat{\\mathbf{\\beta}}-\\beta=\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta-1",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta-2",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] = \\] \\(~\\)\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k)\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "bivariate24.html#standard-errors-of-beta_k",
    "href": "bivariate24.html#standard-errors-of-beta_k",
    "title": "The Bivariate Model",
    "section": "Standard Errors of \\(\\beta_k\\)",
    "text": "Standard Errors of \\(\\beta_k\\)\n\\[\n\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_k,\\beta_2) &\\cdots & \\sqrt{var(\\beta_k)}\\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#property-1",
    "href": "bivariate24.html#property-1",
    "title": "The Bivariate Model",
    "section": "Property 1",
    "text": "Property 1\nIf \\(X' \\widehat{\\epsilon} = 0\\) holds, then the following properties exist:\n\n\n\n\n\n\nProposition\n\n\n\nEach \\(x\\) variable (each column vector of \\(\\mathbf{X}\\)) is uncorrelated with \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#property-2",
    "href": "bivariate24.html#property-2",
    "title": "The Bivariate Model",
    "section": "Property 2",
    "text": "Property 2\nAssuming a constant in the matrix \\(\\mathbf{X}\\),\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\sum\\limits_{i-1}^{n} \\epsilon_i = 0\\)\nbecause each element of the matrix \\(X' \\widehat{\\epsilon}\\) would be nonzero due to the constant; only by multiplying by \\(\\epsilon_i=0\\) would each element equal zero, and then the sum must be zero."
  },
  {
    "objectID": "bivariate24.html#property-3",
    "href": "bivariate24.html#property-3",
    "title": "The Bivariate Model",
    "section": "Property 3",
    "text": "Property 3\n\n\n\n\n\n\nProposition\n\n\n\nThe mean of the residuals is zero.\nIf the sum of the residuals is zero, that sum divided by \\(N\\) must also be zero."
  },
  {
    "objectID": "bivariate24.html#property-4",
    "href": "bivariate24.html#property-4",
    "title": "The Bivariate Model",
    "section": "Property 4",
    "text": "Property 4\n\n\n\n\n\n\nProposition\n\n\n\nThe regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, \\(\\mathbf{X}\\) and \\(y\\).\n\\[\n\\begin{align}\n\\epsilon = y - X\\widehat{\\beta} \\nonumber \\\\ \\\\\n\\text{multiplying by} ~  N^{-1} \\nonumber \\\\ \\\\\n\\bar{\\epsilon} = \\bar{y} - \\bar{X} \\widehat{\\beta} = 0  \\nonumber\n\\end{align}\n\\]\n\\(\\bar{y} - \\bar{X} \\widehat{\\beta}=0\\) implies \\(\\bar{y} = \\bar{X} \\widehat{\\beta}\\), and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane \\(\\bar{y} - \\bar{X} \\widehat{\\beta}\\), the mean of the residuals equals zero, implying the regression line or hyperplane passes through it."
  },
  {
    "objectID": "bivariate24.html#linear-predictions",
    "href": "bivariate24.html#linear-predictions",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\)."
  },
  {
    "objectID": "bivariate24.html#residuals",
    "href": "bivariate24.html#residuals",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#in-matrix-notation",
    "href": "bivariate24.html#in-matrix-notation",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Restating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#the-equation",
    "href": "bivariate24.html#the-equation",
    "title": "The Bivariate Model",
    "section": "The equation",
    "text": "The equation\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#mathbfxx",
    "href": "bivariate24.html#mathbfxx",
    "title": "The Bivariate Model",
    "section": "\\(\\mathbf{X'X}\\)",
    "text": "\\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#the-components",
    "href": "bivariate24.html#the-components",
    "title": "The Bivariate Model",
    "section": "The components",
    "text": "The components\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#components---covariation-mathbfxx",
    "href": "bivariate24.html#components---covariation-mathbfxx",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'X}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#components---covariation-mathbfxy",
    "href": "bivariate24.html#components---covariation-mathbfxy",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'y}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'y}\\)\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation",
    "href": "bivariate24.html#ols-matrix-derivation",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation-1",
    "href": "bivariate24.html#ols-matrix-derivation-1",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\   \n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "tdata.html#data-generating-process-1",
    "href": "tdata.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinkingdata24.html",
    "href": "thinkingdata24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata24.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata24.html#data-generating-process",
    "href": "thinkingdata24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "thinkingdata24.html#data-generating-process-1",
    "href": "thinkingdata24.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinkingdata24.html#a-terrible-map",
    "href": "thinkingdata24.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "thinkingdata24.html#war-outcomes",
    "href": "thinkingdata24.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "thinkingdata24.html#observability",
    "href": "thinkingdata24.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "thinkingdata24.html#asking-the-right-question",
    "href": "thinkingdata24.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinkingdata24.html#data",
    "href": "thinkingdata24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata24.html#types-of-variables",
    "href": "thinkingdata24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata24.html#discrete-variables",
    "href": "thinkingdata24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata24.html#continuous-variables",
    "href": "thinkingdata24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata24.html#levels-of-measurement",
    "href": "thinkingdata24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata24.html#levels-of-measurement-and-models",
    "href": "thinkingdata24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata24.html#describe-these-data",
    "href": "thinkingdata24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata24.html#describe-these-data-1",
    "href": "thinkingdata24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata24.html#overstaying-terms",
    "href": "thinkingdata24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "probability24s.html",
    "href": "probability24s.html",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns.\n\n\n\n\nProbability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value).\n\n\n\n\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita.\n\n\n\n\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely.\n\n\n\n\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war.\n\n\n\n\nThese four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information.\n\n\n\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "multivariate24.html",
    "href": "multivariate24.html",
    "title": "The Multivariate Model",
    "section": "",
    "text": "The Multivariate Model\nThe key issue in the multivariate regression is statistical control - our effort to mimic experimental conditions.\n\n\nExperimental Control\nIn an experiment, the research randomizes on all variables except the variable of interest (the treatment). This means the sample and all conditions the subjects in the sample face are random, or are explicitly fixed by the researcher. For example, in a diet pill study, the sample will be randomized with respect to demographics, health, weight, etc. The subjects’ diets during the study will be fixed or controlled by the researcher so they all eat the same. The lone exception will the administration of the diet pill - the treatment group will get the pill, the control group will get a placebo.\n\n\nIn the regression setting\nOne of the reasons we use models like the linear regression is to mimic the conditions of an experiment. The model allows us to isolate the effect of \\(x_1\\) while accounting for the independent effect of \\(x_2\\) on \\(y\\). So we’re able to say ``the effect of \\(x_1\\), controlling for the influence of \\(x_2\\) is \\(\\beta_1\\).’’\n\n\nHolding constant at mean\nThis linear regression :\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2} x_2 + \\epsilon_i\\]\nis equivalent to this one ::\n\\[y_i = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\widehat{\\beta_2^*}( x_2-\\bar{x_2}) + \\epsilon_i\\]\nOnly the constant shifts (think back to linear transformations; \\(\\widehat{\\beta_0}\\) shifts by a factor of \\(-k\\widehat{\\beta}\\)).\n\n\nConstant at mean\nWhat this shows is that even without transforming \\(x_2\\), we are holding its effect constant at its mean while we estimate the effect of \\(x_1\\).\n\n\nConditional Means\nLet \\(D_1\\) be a binary variable:\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}D_1 + \\hat{\\beta_2} x_1 + \\epsilon_i\\]\nThe partial effect of \\(D_1\\) measures the difference between the means for \\(D_1=0,1\\), given the mean effect of \\(x_1\\).\nSuppose we estimate a model predicting anti-government protests, and we think the main predictor will be liberal political institutions, controlling for per capita wealth. We think as liberalism increases, so do protests; they decrease with authoritarianism. So the regression looks like this:\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nWhat is the effect of liberal political institutions controlling for GDP per capita?\nWhen we ``control’’ for a variable like GDP per capita, we are trying to evaluate two things:\n\nthe direct or immediate effect of institutions on protests -this is because liberalism might promote free assembly.\nthe indirect of effect of institutions on GDP per capita, and then on protests - this is because liberalism might promote economic growth and development, and thereby influence protests.\n\nIn our regression\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nthis means \\(\\hat{\\beta_1}\\) is the effect of liberalism that does not go through GDP (or any other control variable).\nFrom our regression,\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nPolitical institutions influence GDP per capita:\n\\[\\text{GDPpc}_i = \\hat{\\gamma_0} + \\hat{\\gamma_1}\\text{Politics}_i  + \\upsilon_i\\]\nSubstituting,\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i +\\hat{\\beta_2}(\\hat{\\gamma_0} + \\hat{\\gamma_1}\\text{Politics}_i  + \\upsilon_i )+ \\epsilon_i\\]\nso, the partial effect of liberal institutions is\n\\[\\frac{\\partial(protests)}{\\partial(politics)} = \\hat{\\beta_1} \\]\nThe effect is partial because it excludes the effect of politics that runs through GDP per capita. Here’s the total effect:\n\\[\\frac{\\partial(protests)}{\\partial(politics)} = \\hat{\\beta_1} + \\hat{\\beta_2}\\hat{\\gamma_1}\\]\nNotice that if liberalism has {} on GDP, so \\(\\hat{\\gamma_1}=0\\), then there is no indirect effect and the partial and total effects are the same.\n\n\nCounterfactuals\nAnother way to think about statistical control is to think about counterfactuals we’d want to evaluate:\n\ndemocracy promotes protests; but what if the state is rich?\ndemocracy promotes protests; but what if the state is poor?\n\n\n\nRandomization\nThe intuition is the same as in the experimental ideal. In the experiment the control and treatment groups are randomized with respect to all things but the treatment itself. In the regression, we want observations randomized over dimensions like wealth (i.e., we have rich and poor, etc) and all other things except the treatment itself - liberal institutions. If it were possible, we might collect data on protests and GDP but for states that are otherwise exactly the same.\n\n\nOther ways to exert control\nCollecting data on protests and GDP for states that are otherwise exactly the same - this the foundation of what are called methods. Matching is aimed at mimicking experimental design, exerting control via sampling. is a related technique that estimates weights for the control so it more closely resembles the treatment group.\n\n\nFrisch-Waugh-Lovell Theorem\nIn the linear least squares regression \\(y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\epsilon\\) produces an estimate for \\(\\hat{\\beta_1}\\) that is the same as the estimate of \\(\\hat{\\beta_1^*}\\) produced by estimating the regression \\(y\\) on \\(x_1\\), saving the residuals \\(\\hat{r_1}\\), then regressing \\(x_1\\) on \\(x_2\\), computing the residuals, \\(\\hat{r_2}\\), and then regressing \\(\\hat{r_1}\\) on \\(\\hat{r_2}\\).\n\n\\(\\hat{r_1}\\) measures the part of \\(y\\) that is unrelated to \\(x_1\\).\nThe regression \\(x_1=\\beta_0+ \\beta_2 x_2\\) measures the overlapping (correlated) parts of \\(x_1\\) and \\(x_2\\).\n\\(\\hat{r_2}\\) measures the unrelated parts of \\(x_1\\) and \\(x_2\\), the part of \\(x_1\\) unrelated to \\(x_2\\).\n\\(\\hat{r_1}\\) measures the part of \\(x_2\\) that is totally independent of \\(x_1\\)\nthe regression \\(\\hat{r_1}=\\beta_0+ \\beta_1^* \\hat{r_2}\\) estimates \\(\\beta_1^*\\) which measures the effect of the part of \\(x_1\\) that is unrelated to \\(x_2\\).\n\\(\\beta_1^*\\) is the partial effect of \\(x_1\\); we measured that part of \\(x_1\\) using \\(\\hat{r_1}\\).\n\n\n\nRegression Anatomy\nOne final way to think about this is a variant on FWL called ``regression anatomy’’ by Angrist and Pischke in their terrific book Mostly Harmless Econometrics.\n\n\nAnatomy\nIn the bivariate case, the slope on \\(\\beta_1\\) is:\n\\[\\beta_1 = \\frac{cov(y_i, x_i)}{V(x_i)}\\]\nIn the multivariate case, the slope on \\(\\beta_1\\) is:\n\\[\\beta_1 = \\frac{cov(y_i, \\tilde{x_{k,i}})}{V(\\tilde{x_{k,i}}) }\\]\nwhere \\(\\tilde{x_{k,i}}\\) refers to the residuals from the regression of \\(x_{k,i}\\) on \\(\\mathbf{X}\\), where \\(X\\) is all other right-side variables.\n\nThe residual from that regression measures the part of \\(x\\) that is unrelated to \\(X\\).\nThe estimate of \\(\\beta_1\\) then is the correlation of \\(y\\) and the part of \\(x\\) that is unrelated to \\(X\\).\nSince \\(\\tilde{x_{k,i}}\\) is unrelated to \\(X\\), its correlation to \\(y\\) is purged of any part through \\(X\\), and is therefore partial.\n\n\n\nOmitted Variable Bias\nIn the population, the true regression is:\n\\[y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\widehat{\\beta_2}x_2 + \\varepsilon \\]\nIn our sample, the regression we estimate is:\n\\[y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\varepsilon \\]\nSo the model omits a variable. What’s the effect?\n\n\nOmitted Variable Bias\n\nIn the bivariate regression, the estimate is \\(\\widehat{\\beta_1}\\)\nIn the multivariate regression, the estimate is \\(\\widetilde{\\beta_1} = \\widehat{\\beta_1} + \\widehat{\\beta_2}\\widetilde{\\delta_1}\\). That is, the estimate for \\(\\widetilde{\\beta_1}\\) now accounts for the relationship of \\(x_1,x_2\\), measured in \\(\\widetilde{\\delta_1}\\). \\(\\widetilde{\\beta_1}\\) now measures the partial effect of \\(x_1\\) on \\(y\\).\nIf we exclude \\(x_2\\), we misestimate \\(\\widehat{\\beta_1}\\) by \\(\\pm \\widehat{\\beta_2}\\widetilde{\\delta_1}\\)\n\n\n\n\nEvaluating an Estimator\nWhat criteria make for a “good” estimator?\n\nUnbiasedness:\n\n\nIs the expected value of \\(\\hat{\\beta}\\) equal to \\(\\beta\\)?\nHow far away do we expect \\(\\hat{\\beta}\\) to be from \\(\\beta\\)?\n\\(E[\\hat{\\beta} - \\beta]\\)\nThe estimator for which this quantity is smallest is the least biased.\n\nBias will always exist, but we want it to be as small as possible, and random (not systematic).\n\nEfficiency:\n\n\nEfficiency measures how close to the true \\(\\beta\\) we are {}.\nEfficiency describes the average size of bias; the average distance of \\(E[\\hat{\\beta} - \\beta]\\).\nThis is about the size of the variance; large variance estimates will, on average, miss the true \\(\\beta\\) by a lot. Small variance estimators will miss the true \\(\\beta\\) by a little, on average.\n\n\nConsistency:\n\n\nConsistency can be thought of as large-sample-unbiasedness.\n\\(E[\\hat{\\beta} - \\beta] \\rightarrow\\) as \\(N \\rightarrow \\infty\\)\nThis is considerably less important to us than bias and efficiency. In part, this is due to the reality that our (non experimental) data are small samples in many cases. It’s also true OLS has good small sample properties.\nMLE does not have good small sample properties, and relies strongly on consistency.\n\n\n\nAssumptions\nUnder the following four assumptions, the OLS estimator \\(\\hat{\\beta}\\) is an unbiased estimator of \\(\\beta\\):\n\nLinear in parameters.\nRandom Sampling.\nZero Conditional Mean of Disturbances.\nNo Perfect Collinearity.\n\n\n\n\n\n\n\nTheorem :Unbiasedness of OLS:\n\n\n\nUnder these four assumptions, OLS estimators are unbiased estimators of the population parameters:\n\\[\nE[{\\widehat{\\beta_{j}}]= \\beta_{j} ~\\forall ~ j} \\nonumber\n\\]\n\n\n\n\nOLS\nIf the model also meets these two assumptions, then the OLS estimator has the smallest variance of all estimators:\n\nHomoskedastic disturbances; \\(Var(u|x_1,x_2,\\ldots,x_k)=\\sigma^2\\).\nUncorrelated disturbances; \\(cov(u_i,u_j|x_1,x_2,\\ldots,x_k)=0\\).\n\nIf Assumptions 1-6 are all met, then the model is BLUE and thus satisfies the Gauss-Markov Theorem. Additionally, these assumptions provide us the first two moments of the sampling distribution for the \\(\\hat{\\beta}\\)s.\n\n\n\n\n\n\nTheorem: Gauss Markov\n\n\n\nUnder these assumptions, the OLS estimator is unbiased and has the smallest variance among all linear unbiased estimators.\n\n\n\n\nOLS\nHowever, in order to talk precisely about the uncertainty surrounding the point estimates, we need to make one more assumption about the error term:\n\nThe disturbances, \\(u_i\\) are independent of the \\(X\\)s and are normally distributed: \\(u \\sim N(0,\\sigma^{2})\\).\n\n\n\nIf assumptions fail \\(\\ldots\\)\n\nsample is not random - bias.\n\\(E[u | X] \\neq 0\\) - endogeneity; bias and standard errors are too small.\nperfect collinearity - matrix is singular, regression fails.\ncorrelated \\(X\\) variables (imperfect collinearity) - unbiased estimates, inefficient standard errors.\nerrors are not identically distributed (heteroskedastic) - inefficiency.\nerrors are not independently distributed (correlated errors) - inefficiency.\n\n\n\nSimulating OLS assumptions"
  },
  {
    "objectID": "bivariate24s.html#regression-1",
    "href": "bivariate24s.html#regression-1",
    "title": "The Bivariate Model",
    "section": "Regression",
    "text": "Regression\nLet \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "bivariate24s.html#linear-predictions",
    "href": "bivariate24s.html#linear-predictions",
    "title": "The Bivariate Model",
    "section": "Linear predictions",
    "text": "Linear predictions\nThe predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\)."
  },
  {
    "objectID": "bivariate24s.html#residuals",
    "href": "bivariate24s.html#residuals",
    "title": "The Bivariate Model",
    "section": "Residuals",
    "text": "Residuals\nThe differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24s.html#in-matrix-notation",
    "href": "bivariate24s.html#in-matrix-notation",
    "title": "The Bivariate Model",
    "section": "In matrix notation",
    "text": "In matrix notation\nRestating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-i",
    "href": "bivariate24s.html#getting-to-ols-i",
    "title": "The Bivariate Model",
    "section": "Getting to OLS I",
    "text": "Getting to OLS I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-1",
    "href": "bivariate24s.html#ols-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nTo minimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]\n\\(\\ldots\\) illustrating explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-1-1",
    "href": "bivariate24s.html#ols-1-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-ii",
    "href": "bivariate24s.html#getting-to-ols-ii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS II",
    "text": "Getting to OLS II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero.\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling that the mean of a variable\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]\nSo, substituting these equations into\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\\]\nand solving for \\(\\hat{\\beta}_1\\) gives:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-iii",
    "href": "bivariate24s.html#getting-to-ols-iii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS III",
    "text": "Getting to OLS III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24s.html#the-components",
    "href": "bivariate24s.html#the-components",
    "title": "The Bivariate Model",
    "section": "The components",
    "text": "The components\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24s.html#components---covariation-mathbfxx",
    "href": "bivariate24s.html#components---covariation-mathbfxx",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'X}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24s.html#components---covariation-mathbfxy",
    "href": "bivariate24s.html#components---covariation-mathbfxy",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'y}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'y}\\)\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation",
    "href": "bivariate24s.html#ols-matrix-derivation",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation-1",
    "href": "bivariate24s.html#ols-matrix-derivation-1",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\   \n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\n\\[\n\\begin{align}\nE(\\mathbf{\\epsilon\\epsilon'})= E \\left[\n\\begin{array}{c}\n\\epsilon_{1} \\\\\n\\vdots \\\\\n\\epsilon_{n}\\\\\n\\end{array} \\right]\nE \\left[\n\\begin{array}{cccc}\n\\epsilon_{1} & \\cdots &\\epsilon_{n}\\\\\n\\end{array} \\right] \\\\ ~\\\\\n= E\n\\left[\n\\begin{array}{cccc}\n\\epsilon_{1}^{2} & \\epsilon_{1}\\epsilon_{2} &\\cdots &\\epsilon_{1}\\epsilon_{n}\\\\\n\\epsilon_{2}\\epsilon_{1}& \\epsilon_{2}^{2} &\\cdots &\\epsilon_{2}\\epsilon_{n}\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n\\epsilon_{n}\\epsilon_{1} &\\epsilon_{n}\\epsilon_{2} &\\cdots & \\epsilon_{n}^{2}\\\\\n\\end{array} \\right]\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon-1",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[\nE(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0\\\\\n0&\\sigma^{2} &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & \\sigma^{2}\\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0\\\\\n0&1 &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & 1\\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I}\n\\]\nThis is the variance-covariance matrix of the disturbances (var-cov(e)); it is symmetric, the main diagonal containing the variances, the off-diagonal containing the covariances."
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\nThe variance-covariance of the estimate, \\(\\widehat{\\beta}\\) derives as follows:\n\\[\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'y}\\]\nsubstituting \\(\\mathbf{X\\beta}+\\epsilon\\) for \\(\\mathbf{y}\\), we get\n\\[\n\\begin{align}\n\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'(X\\beta + \\epsilon)}  \\nonumber \\\\  \n= \\mathbf{(X'X)^{-1}}\\mathbf{X'X\\beta}+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\  \n=\\beta+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\\n\\widehat{\\mathbf{\\beta}}-\\beta=\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta-1",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta-2",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] = \\] \\(~\\)\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k)\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "bivariate24s.html#standard-errors-of-beta_k",
    "href": "bivariate24s.html#standard-errors-of-beta_k",
    "title": "The Bivariate Model",
    "section": "Standard Errors of \\(\\beta_k\\)",
    "text": "Standard Errors of \\(\\beta_k\\)\n\\[\n\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & \\sqrt{var(\\beta_k)}\\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "bivariate24s.html#property-1",
    "href": "bivariate24s.html#property-1",
    "title": "The Bivariate Model",
    "section": "Property 1",
    "text": "Property 1\nIf \\(X' \\widehat{\\epsilon} = 0\\) holds, then the following properties exist:\n\n\n\nProposition\n\n\nEach \\(x\\) variable (each column vector of \\(\\mathbf{X}\\)) is uncorrelated with \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24s.html#property-2",
    "href": "bivariate24s.html#property-2",
    "title": "The Bivariate Model",
    "section": "Property 2",
    "text": "Property 2\nAssuming a constant in the matrix \\(\\mathbf{X}\\),\n\n\n\nProposition\n\n\n\\(\\sum\\limits_{i-1}^{n} \\epsilon_i = 0\\)\nbecause each element of the matrix \\(X' \\widehat{\\epsilon}\\) would be nonzero due to the constant; only by multiplying by \\(\\epsilon_i=0\\) would each element equal zero, and then the sum must be zero."
  },
  {
    "objectID": "bivariate24s.html#property-3",
    "href": "bivariate24s.html#property-3",
    "title": "The Bivariate Model",
    "section": "Property 3",
    "text": "Property 3\n\n\n\nProposition\n\n\nThe mean of the residuals is zero.\nIf the sum of the residuals is zero, that sum divided by \\(N\\) must also be zero."
  },
  {
    "objectID": "bivariate24s.html#property-4",
    "href": "bivariate24s.html#property-4",
    "title": "The Bivariate Model",
    "section": "Property 4",
    "text": "Property 4\n\n\n\nProposition\n\n\nThe regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, \\(\\mathbf{X}\\) and \\(y\\).\n\\[\n\\begin{align}\n\\epsilon = y - X\\widehat{\\beta} \\nonumber \\\\ \\\\\n\\text{multiplying by} ~  N^{-1} \\nonumber \\\\ \\\\\n\\bar{\\epsilon} = \\bar{y} - \\bar{X} \\widehat{\\beta} = 0  \\nonumber\n\\end{align}\n\\]\n\\(\\bar{y} - \\bar{X} \\widehat{\\beta}=0\\) implies \\(\\bar{y} = \\bar{X} \\widehat{\\beta}\\), and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane \\(\\bar{y} - \\bar{X} \\widehat{\\beta}\\), the mean of the residuals equals zero, implying the regression line or hyperplane passes through it."
  },
  {
    "objectID": "bivariate24.html#derivation-i",
    "href": "bivariate24.html#derivation-i",
    "title": "The Bivariate Model",
    "section": "Derivation I",
    "text": "Derivation I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i",
    "href": "bivariate24.html#ols-i",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nMinimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-1",
    "href": "bivariate24.html#ols-i-1",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-2",
    "href": "bivariate24.html#ols-i-2",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nThis shows explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24.html#derivation-ii",
    "href": "bivariate24.html#derivation-ii",
    "title": "The Bivariate Model",
    "section": "Derivation II",
    "text": "Derivation II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii",
    "href": "bivariate24.html#ols-ii",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-1",
    "href": "bivariate24.html#ols-ii-1",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-2",
    "href": "bivariate24.html#ols-ii-2",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero."
  },
  {
    "objectID": "bivariate24.html#ols-ii-3",
    "href": "bivariate24.html#ols-ii-3",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-4",
    "href": "bivariate24.html#ols-ii-4",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling the mean of a variable is\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-5",
    "href": "bivariate24.html#ols-ii-5",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-6",
    "href": "bivariate24.html#ols-ii-6",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#properties",
    "href": "bivariate24.html#properties",
    "title": "The Bivariate Model",
    "section": "Properties",
    "text": "Properties\nNote that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about \\(\\widehat{\\beta}\\), and inferences about those estimates."
  },
  {
    "objectID": "bivariate24s.html#derivation-i",
    "href": "bivariate24s.html#derivation-i",
    "title": "The Bivariate Model",
    "section": "Derivation I",
    "text": "Derivation I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i",
    "href": "bivariate24s.html#ols-i",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nMinimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-1",
    "href": "bivariate24s.html#ols-i-1",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-2",
    "href": "bivariate24s.html#ols-i-2",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nThis shows explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24s.html#derivation-ii",
    "href": "bivariate24s.html#derivation-ii",
    "title": "The Bivariate Model",
    "section": "Derivation II",
    "text": "Derivation II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii",
    "href": "bivariate24s.html#ols-ii",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-1",
    "href": "bivariate24s.html#ols-ii-1",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-2",
    "href": "bivariate24s.html#ols-ii-2",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero."
  },
  {
    "objectID": "bivariate24s.html#ols-ii-3",
    "href": "bivariate24s.html#ols-ii-3",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-4",
    "href": "bivariate24s.html#ols-ii-4",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling the mean of a variable is\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-5",
    "href": "bivariate24s.html#ols-ii-5",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-6",
    "href": "bivariate24s.html#ols-ii-6",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#properties",
    "href": "bivariate24s.html#properties",
    "title": "The Bivariate Model",
    "section": "Properties",
    "text": "Properties\nNote that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about \\(\\widehat{\\beta}\\), and inferences about those estimates."
  },
  {
    "objectID": "bivariate24.html#ols-i-3",
    "href": "bivariate24.html#ols-i-3",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-3",
    "href": "bivariate24s.html#ols-i-3",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-4",
    "href": "bivariate24.html#ols-i-4",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-4",
    "href": "bivariate24s.html#ols-i-4",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-5",
    "href": "bivariate24.html#ols-i-5",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-5",
    "href": "bivariate24s.html#ols-i-5",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-6",
    "href": "bivariate24.html#ols-i-6",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24.html#ols-ii-7",
    "href": "bivariate24.html#ols-ii-7",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-8",
    "href": "bivariate24.html#ols-ii-8",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-9",
    "href": "bivariate24.html#ols-ii-9",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSo, substituting and solving for \\(\\hat{\\beta}_1\\) gives:\n\n\n\n\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-10",
    "href": "bivariate24.html#ols-ii-10",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24.html#derivation-iii",
    "href": "bivariate24.html#derivation-iii",
    "title": "The Bivariate Model",
    "section": "Derivation III",
    "text": "Derivation III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation-2",
    "href": "bivariate24.html#ols-matrix-derivation-2",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon-2",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nThis matrix is important because it gives us our estimate of \\(\\sigma^2\\). This is important enough to merit its own slide."
  },
  {
    "objectID": "bivariate24s.html#ols-i-6",
    "href": "bivariate24s.html#ols-i-6",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24s.html#ols-ii-7",
    "href": "bivariate24s.html#ols-ii-7",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-8",
    "href": "bivariate24s.html#ols-ii-8",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-9",
    "href": "bivariate24s.html#ols-ii-9",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSo, substituting and solving for \\(\\hat{\\beta}_1\\) gives:\n\n\n\n\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-10",
    "href": "bivariate24s.html#ols-ii-10",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24s.html#derivation-iii",
    "href": "bivariate24s.html#derivation-iii",
    "title": "The Bivariate Model",
    "section": "Derivation III",
    "text": "Derivation III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation-2",
    "href": "bivariate24s.html#ols-matrix-derivation-2",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon-2",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nThis matrix is important because it gives us our estimate of \\(\\sigma^2\\). This is important enough to merit its own slide."
  },
  {
    "objectID": "bivariate24.html#aside-on-sigma2",
    "href": "bivariate24.html#aside-on-sigma2",
    "title": "The Bivariate Model",
    "section": "Aside on \\(\\sigma^2\\)",
    "text": "Aside on \\(\\sigma^2\\)\n\\(\\widehat{\\sigma^2}\\) is our estimate of the error variance.\n\\[\\widehat{\\sigma^2} = (u'u) \\frac{1}{n-k-1}\\] This is the sum squared error (inner product of the residuals) distributed across our degrees of freedom. You should see that small samples will have smaller denominators, so potentially larger error variance."
  },
  {
    "objectID": "bivariate24.html#average-error",
    "href": "bivariate24.html#average-error",
    "title": "The Bivariate Model",
    "section": "Average error",
    "text": "Average error\n\\[\\widehat{\\sigma} = \\sqrt{\\widehat{\\sigma^2}}\\] The square root of \\(\\widehat{\\sigma^2}\\) gives us the the residual standard error also called the root mean squared error (RMSE) - it is in the same metric as \\(y\\), so is useful as an indicator of the average error in the regression."
  },
  {
    "objectID": "bivariate24s.html#aside-on-sigma2",
    "href": "bivariate24s.html#aside-on-sigma2",
    "title": "The Bivariate Model",
    "section": "Aside on \\(\\sigma^2\\)",
    "text": "Aside on \\(\\sigma^2\\)\n\\(\\widehat{\\sigma^2}\\) is our estimate of the error variance.\n\\[\\widehat{\\sigma^2} = (u'u) \\frac{1}{n-k-1}\\] This is the sum squared error (inner product of the residuals) distributed across our degrees of freedom. You should see that small samples will have smaller denominators, so potentially larger error variance."
  },
  {
    "objectID": "bivariate24s.html#average-error",
    "href": "bivariate24s.html#average-error",
    "title": "The Bivariate Model",
    "section": "Average error",
    "text": "Average error\n\\[\\widehat{\\sigma} = \\sqrt{\\widehat{\\sigma^2}}\\] The square root of \\(\\widehat{\\sigma^2}\\) gives us the the residual standard error also called the root mean squared error (RMSE) - it is in the same metric as \\(y\\), so is useful as an indicator of the average error in the regression."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "exercise #1 answers\nexercise #2 answers\nexercise #3 answers\nexercise #4 answers\nsimulation shiny app\nsimulation code\ncentral limit theorem shiny app"
  },
  {
    "objectID": "ex1answers2024.html",
    "href": "ex1answers2024.html",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\npdf-cdf"
  },
  {
    "objectID": "ex1answers2024.html#q1",
    "href": "ex1answers2024.html#q1",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\npdf-cdf"
  },
  {
    "objectID": "ex1answers2024.html#q2",
    "href": "ex1answers2024.html#q2",
    "title": "exercise #1 answers",
    "section": "Q2",
    "text": "Q2\n\n\ncode\nxb &lt;- runif(1000, min=-4, max=4)\npdf1 &lt;- dnorm(xb, mean=0, sd=1)\npdf2 &lt;- dnorm(xb, mean=0, sd=sqrt(.5))\npdf3 &lt;- dnorm(xb, mean=-1, sd=sqrt(1.5))\npdf4 &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, pdf1, pdf2, pdf3, pdf4)\n\n\n\nggplot(data=df, aes(x=xb, y=pdf1)) +\n  geom_line() +\n  geom_line(aes(y=pdf2), linetype=\"dotted\") +\n  geom_line(aes(y=pdf3), linetype=\"longdash\" ) +\n  annotate(\"text\", x = 2.5, y = .1, label = \"Normal (0,1)\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"Normal (0, .5)\") +\n  annotate(\"text\", x = -3.3, y = .2, label = \"Normal (-1, 1.5)\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal PDFs\")"
  },
  {
    "objectID": "ex1answers2024.html#dance-model",
    "href": "ex1answers2024.html#dance-model",
    "title": "exercise #1 answers",
    "section": "Dance model",
    "text": "Dance model\nSimple regression - is one artist’s music more danceable than the other - dummy variable for the Rolling Stones.\n\n\ncode\nd &lt;- summary(lm(data=bsum, danceability ~ as.factor(artist_name)))\nmodelsummary(d, stars=TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.593***\n\n\n\n(0.004)\n\n\nas.factor(artist_name)The Rolling Stones\n−0.124***\n\n\n\n(0.005)\n\n\nNum.Obs.\n2875\n\n\nR2\n0.186\n\n\nR2 Adj.\n0.185\n\n\nRMSE\n0.13\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nThe Stones are statistically less danceable than Taylor Swift is.\n\nDanceability over time\nDoes danceability change over time? Let’s use dummmies for years to relax linearity.\n\n\ncode\n# modeling danceability over time for each artist\n\n#taylor swift\ntsreg &lt;- summary(lm(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), danceability ~ as.factor(album_release_year)))\nmodelsummary(tsreg, stars=TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.571***\n\n\n\n(0.011)\n\n\nas.factor(album_release_year)2008\n0.016\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2010\n−0.015\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2012\n0.062***\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2014\n0.066***\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2015\n0.044\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2017\n0.064***\n\n\n\n(0.016)\n\n\nas.factor(album_release_year)2018\n0.031\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2019\n0.087**\n\n\n\n(0.027)\n\n\nas.factor(album_release_year)2020\n−0.026+\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2021\n−0.015\n\n\n\n(0.015)\n\n\nas.factor(album_release_year)2022\n0.062***\n\n\n\n(0.017)\n\n\nNum.Obs.\n1265\n\n\nR2\n0.115\n\n\nR2 Adj.\n0.108\n\n\nRMSE\n0.10\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nCompared to her first release, it appears TS has gotten more danceable since 2012, though her music varies considerably.\n\n\ncode\n#stones\nrsreg &lt;- summary(lm(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), danceability ~ as.factor(album_release_year)))\nmodelsummary(rsreg, stars=TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.599***\n\n\n\n(0.016)\n\n\nas.factor(album_release_year)1965\n−0.033+\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)1966\n−0.105***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)1967\n−0.085***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)1968\n−0.101**\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1969\n−0.105***\n\n\n\n(0.028)\n\n\nas.factor(album_release_year)1970\n−0.135***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)1971\n−0.150***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)1972\n−0.139***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)1973\n−0.148***\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1974\n−0.067*\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1976\n0.006\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)1977\n−0.168***\n\n\n\n(0.026)\n\n\nas.factor(album_release_year)1978\n−0.026\n\n\n\n(0.023)\n\n\nas.factor(album_release_year)1980\n0.006\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1981\n−0.050\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1982\n−0.240***\n\n\n\n(0.039)\n\n\nas.factor(album_release_year)1983\n−0.017\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1986\n−0.052+\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1989\n−0.069*\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1991\n−0.176***\n\n\n\n(0.026)\n\n\nas.factor(album_release_year)1994\n−0.113**\n\n\n\n(0.035)\n\n\nas.factor(album_release_year)1995\n−0.069*\n\n\n\n(0.028)\n\n\nas.factor(album_release_year)1997\n−0.080**\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2004\n−0.250***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)2005\n−0.044\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)2011\n−0.308***\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)2012\n−0.210***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)2016\n−0.224***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)2017\n−0.137***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2018\n−0.197***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2019\n−0.140***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)2020\n−0.213***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)2021\n−0.179***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2022\n−0.233***\n\n\n\n(0.024)\n\n\nNum.Obs.\n1610\n\n\nR2\n0.272\n\n\nR2 Adj.\n0.256\n\n\nRMSE\n0.12\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nThe Stones have generally been as danceable or less so since their first album.\n\n\ncode\n#plot avg dancability \n\nggplot() +\n  geom_density(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), aes(x=danceability))+\n  geom_density(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), aes(x=danceability), linetype=\"dotted\") +\n  annotate(\"text\", x=.18, y=2, label=\"Stones\")+\n  annotate(\"text\", x=.45, y=3.5, label=\"Taylor\") +\n  labs(x=\"Danceability\" , y=\"Density\")\n\n\n\n\n\n\n\n\n\n\n\nArtists over time\nBorrowing Mary L’s idea:\n\n\ncode\n#plot  dancability over time\n\nggplot() +\n  geom_smooth(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), aes(y=danceability, x=album_release_year)) +\n  geom_smooth(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), aes(y=danceability, x=album_release_year)) +\n  annotate(\"text\", x=1980, y=.6, label=\"Stones\")+\n  annotate(\"text\", x=2017, y=.55, label=\"Taylor\") +\n  labs(x=\"Years\" , y=\"Danceability\")"
  },
  {
    "objectID": "ex2answers2024.html",
    "href": "ex2answers2024.html",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\ndata &lt;- c(-1.21,    0,  -.31,\n-.216,  0   ,.92,\n-2.74,  1   ,-1.19, \n.429,   1   ,2.40 ,\n-.40,   1,  .62 ,\n-1.42,  0   ,-.54,\n-1.14,  0   ,-2.35,\n-.74,   0   ,-.137)\n\ndf &lt;- matrix(data, nrow=8, ncol=3, byrow=TRUE)\ncolnames(df) &lt;- c('x2','x1','y')\ndf &lt;- as.data.frame(df)\n\ny &lt;- df$y \ncons &lt;- rep(1, length(y)) # add a constant to the X matrix\nX &lt;- cbind(cons, df$x1, df$x2)\ncolnames(X) &lt;- c('cons','x1', 'x2') \n\n\n\n\n\n\ncode\nb &lt;- solve(t(X)%*%X) %*% t(X)%*%y\nbmat &lt;- as.data.frame(t(b))\nm1 &lt;- lm(data=df, y ~ x1 + x2) \nblm &lt;- as.data.frame(t(coef(m1)))\nblm &lt;- blm %&gt;% mutate(cons=`(Intercept)`) %&gt;% subset(select=c(cons, x1, x2))\nB &lt;- rbind(blm, bmat)\nB &lt;- as.data.frame(t(B))\nB &lt;- rename(B, c(`LM ests` = V1, `Matrix ests` = V2))\n\ntt(B,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_c0qe5qxrp160y3pcxvvc\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                LM ests\n                Matrix ests\n              \n        \n        \n        \n                \n                  0.585513723356887\n                  0.585513723356887\n                \n                \n                  1.0464305226653\n                  1.0464305226653\n                \n                \n                  1.13088629216768\n                  1.13088629216768\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\ncode\n#residuals\ne &lt;- y-X%*%b\ne\n\n\n           [,1]\n[1,]  0.4728587\n[2,]  0.5787577\n[3,]  0.2766842\n[4,]  0.2829055\n[5,] -0.5595897\n[6,]  0.4803448\n[7,] -1.6463034\n[8,]  0.1143421\n\n\ncode\n#estimate of sigma squared \nsse&lt;-(t(e)%*%e)\nsigma2 &lt;- (t(e)%*%e) * 1/(8-3)  #dividing by N-k\nsigma2 \n\n\n          [,1]\n[1,] 0.7964812\n\n\ncode\n#variance-covariance matrix of B\nvcb &lt;- drop(sigma2) * solve(t(X)%*%X)  \nvcb\n\n\n           cons           x1           x2\ncons  0.2720816 -0.164252184  0.119324327\nx1   -0.1642522  0.425007762 -0.005243268\nx2    0.1193243 -0.005243268  0.126242412\n\n\ncode\n#compare to lm var-cov matrix\n\nlmvcov &lt;- as.data.frame(vcov(m1))\nmvcov &lt;- as.data.frame(vcb)\n\ntt(lmvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_6halg9n3qm45vugrs1eu\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                (Intercept)\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368847\n                \n                \n                  -0.164252183738144\n                  0.425007761536868\n                  -0.00524326815847027\n                \n                \n                  0.119324327368847\n                  -0.00524326815847027\n                  0.126242411520152\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\ncode\ntt(mvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_yf6xrlt5wt9cra19wyir\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                cons\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368848\n                \n                \n                  -0.164252183738144\n                  0.425007761536869\n                  -0.0052432681584703\n                \n                \n                  0.119324327368848\n                  -0.00524326815847029\n                  0.126242411520152\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\ncode\n# now, get the vc of b : var(p) = XVX'\n# X nxk; V kxk; X' kxn\n\nV &lt;- vcb \n\nvcp = X%*%V%*%t(X) #vcov of xb (yhats)\n\nvarp&lt;- diag(vcp, names = TRUE) #main diagonal is variances of xbs \n\n# compute predictions for x1=0, x1=1\nXp &lt;- as.data.frame(X) #data frame to change values of x1\nXp$x1 = 0\n  xb0 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\nXp$x1 = 1\n  xb1 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\npred &lt;- data.frame(cbind(X, xb0, xb1)) \n\nggplot() +\n  geom_line(data=pred, aes(x = x2, y = xb0)) +\n  geom_line(data=pred, aes(x=x2, y=xb1))+\n  annotate(\"text\", x = -.5, y = 1.5, label = \"x1=1\") +\n  annotate(\"text\", x = -1, y = -1, label = \"x1=0\") +\n  labs(x=\"x2\", y=\"predicted y\")"
  },
  {
    "objectID": "ex2answers2024.html#q2",
    "href": "ex2answers2024.html#q2",
    "title": "exercise #2 answers",
    "section": "Q2",
    "text": "Q2\n\n\ncode\ncell &lt;- read.csv(\"/users/dave/documents/teaching/501/2024/exercises/ex2/cellphones.csv\")\n\ncell$population&lt;-as.numeric(cell$population)\n\nsummary(cell)\n\n\n      year         state           state_numeric     population      \n Min.   :2012   Length:50          Min.   : 1.00   Min.   :  576412  \n 1st Qu.:2012   Class :character   1st Qu.:14.25   1st Qu.: 1855441  \n Median :2012   Mode  :character   Median :26.50   Median : 4491154  \n Mean   :2012                      Mean   :26.34   Mean   : 6265634  \n 3rd Qu.:2012                      3rd Qu.:38.75   3rd Qu.: 6834295  \n Max.   :2012                      Max.   :51.00   Max.   :38041430  \n numberofdeaths   urban_percent   cell_subscription    cell_ban  \n Min.   :  59.0   Min.   : 0.00   Min.   :  518     Min.   :0.0  \n 1st Qu.: 213.2   1st Qu.:20.25   1st Qu.: 1617     1st Qu.:0.0  \n Median : 488.5   Median :34.50   Median : 4150     Median :0.0  \n Mean   : 670.9   Mean   :34.82   Mean   : 6001     Mean   :0.2  \n 3rd Qu.: 853.8   3rd Qu.:45.75   3rd Qu.: 6648     3rd Qu.:0.0  \n Max.   :3398.0   Max.   :84.00   Max.   :35616     Max.   :1.0  \n    text_ban    total_miles_driven\n Min.   :0.00   Min.   :  4792    \n 1st Qu.:0.00   1st Qu.: 19238    \n Median :1.00   Median : 47116    \n Mean   :0.68   Mean   : 59305    \n 3rd Qu.:1.00   3rd Qu.: 73461    \n Max.   :1.00   Max.   :326272    \n\n\ncode\ndatasummary(population + numberofdeaths + urban_percent ~ mean + median + min\n            + max + var, data=cell)\n\n\n\n\n\n\nmean\nmedian\nmin\nmax\nvar\n\n\n\n\npopulation\n6265634.34\n4491154.00\n576412.00\n38041430.00\n4.900426e+13\n\n\nnumberofdeaths\n670.92\n488.50\n59.00\n3398.00\n456928.56\n\n\nurban_percent\n34.82\n34.50\n0.00\n84.00\n388.23"
  },
  {
    "objectID": "ex2answers2024.html#q1a",
    "href": "ex2answers2024.html#q1a",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\nb &lt;- solve(t(X)%*%X) %*% t(X)%*%y\nbmat &lt;- as.data.frame(t(b))\nm1 &lt;- lm(data=df, y ~ x1 + x2) \nblm &lt;- as.data.frame(t(coef(m1)))\nblm &lt;- blm %&gt;% mutate(cons=`(Intercept)`) %&gt;% subset(select=c(cons, x1, x2))\nB &lt;- rbind(blm, bmat)\nB &lt;- as.data.frame(t(B))\nB &lt;- rename(B, c(`LM ests` = V1, `Matrix ests` = V2))\n\ntt(B,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_c0qe5qxrp160y3pcxvvc\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                LM ests\n                Matrix ests\n              \n        \n        \n        \n                \n                  0.585513723356887\n                  0.585513723356887\n                \n                \n                  1.0464305226653\n                  1.0464305226653\n                \n                \n                  1.13088629216768\n                  1.13088629216768"
  },
  {
    "objectID": "ex2answers2024.html#q1b",
    "href": "ex2answers2024.html#q1b",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\n#residuals\ne &lt;- y-X%*%b\ne\n\n\n           [,1]\n[1,]  0.4728587\n[2,]  0.5787577\n[3,]  0.2766842\n[4,]  0.2829055\n[5,] -0.5595897\n[6,]  0.4803448\n[7,] -1.6463034\n[8,]  0.1143421\n\n\ncode\n#estimate of sigma squared \nsse&lt;-(t(e)%*%e)\nsigma2 &lt;- (t(e)%*%e) * 1/(8-3)  #dividing by N-k\nsigma2 \n\n\n          [,1]\n[1,] 0.7964812\n\n\ncode\n#variance-covariance matrix of B\nvcb &lt;- drop(sigma2) * solve(t(X)%*%X)  \nvcb\n\n\n           cons           x1           x2\ncons  0.2720816 -0.164252184  0.119324327\nx1   -0.1642522  0.425007762 -0.005243268\nx2    0.1193243 -0.005243268  0.126242412\n\n\ncode\n#compare to lm var-cov matrix\n\nlmvcov &lt;- as.data.frame(vcov(m1))\nmvcov &lt;- as.data.frame(vcb)\n\ntt(lmvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_6halg9n3qm45vugrs1eu\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                (Intercept)\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368847\n                \n                \n                  -0.164252183738144\n                  0.425007761536868\n                  -0.00524326815847027\n                \n                \n                  0.119324327368847\n                  -0.00524326815847027\n                  0.126242411520152\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\ncode\ntt(mvcov,theme = \"striped\")\n\n\n\n \n\n  \n    \n    \n    tinytable_yf6xrlt5wt9cra19wyir\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                cons\n                x1\n                x2\n              \n        \n        \n        \n                \n                  0.272081600903792\n                  -0.164252183738144\n                  0.119324327368848\n                \n                \n                  -0.164252183738144\n                  0.425007761536869\n                  -0.0052432681584703\n                \n                \n                  0.119324327368848\n                  -0.00524326815847029\n                  0.126242411520152"
  },
  {
    "objectID": "ex2answers2024.html#q1c",
    "href": "ex2answers2024.html#q1c",
    "title": "exercise #2 answers",
    "section": "",
    "text": "code\n# now, get the vc of b : var(p) = XVX'\n# X nxk; V kxk; X' kxn\n\nV &lt;- vcb \n\nvcp = X%*%V%*%t(X) #vcov of xb (yhats)\n\nvarp&lt;- diag(vcp, names = TRUE) #main diagonal is variances of xbs \n\n# compute predictions for x1=0, x1=1\nXp &lt;- as.data.frame(X) #data frame to change values of x1\nXp$x1 = 0\n  xb0 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\nXp$x1 = 1\n  xb1 &lt;- as.matrix(Xp)%*%b #compute xb (yhats); use Xp as matrix\npred &lt;- data.frame(cbind(X, xb0, xb1)) \n\nggplot() +\n  geom_line(data=pred, aes(x = x2, y = xb0)) +\n  geom_line(data=pred, aes(x=x2, y=xb1))+\n  annotate(\"text\", x = -.5, y = 1.5, label = \"x1=1\") +\n  annotate(\"text\", x = -1, y = -1, label = \"x1=0\") +\n  labs(x=\"x2\", y=\"predicted y\")"
  },
  {
    "objectID": "ex2answers2024.html#model",
    "href": "ex2answers2024.html#model",
    "title": "exercise #2 answers",
    "section": "model",
    "text": "model\n\n\ncode\nm2 &lt;- lm(data=cell, numberofdeaths ~ text_ban+ miles + pop)\nmodelsummary(m2, stars=TRUE, coef_rename = TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n145.859*\n\n\n\n(54.596)\n\n\nTexting Ban\n−186.190**\n\n\n\n(54.018)\n\n\nTotal Miles Driven in State (billions)\n16.903***\n\n\n\n(2.470)\n\n\nState population (millions)\n−55.986*\n\n\n\n(21.295)\n\n\nNum.Obs.\n50\n\n\nR2\n0.938\n\n\nR2 Adj.\n0.934\n\n\nAIC\n663.4\n\n\nBIC\n673.0\n\n\nLog.Lik.\n−326.695\n\n\nF\n232.354\n\n\nRMSE\n166.50\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\ncode\ndfavg &lt;- cell \n\npredictions&lt;- data.frame(upper=numeric(0),lower=numeric(0),xb=numeric(0), miles=numeric(0), ban=numeric(0))\nc&lt;-1\nfor (i in seq(0,1,1)) { \n  for (j in seq(1,320,1)) {\n  dfavg$text_ban = i\n  dfavg$miles = j+5\n  all &lt;- data.frame(predict(m2, se.fit=TRUE, newdata=dfavg))\n  upper = median(all$fit, na.rm=TRUE)+1.96*(median(all$se.fit, na.rm=TRUE))\n  lower = median(all$fit, na.rm=TRUE)-1.96*(median(all$se.fit, na.rm=TRUE))\n  xb = median(all$fit, na.rm=TRUE)\n  predictions[c,] &lt;-data.frame(upper,lower,xb,j, i)\n  c=c+1\n  }\n}\n  \n  ggplot()+\n  geom_line(data= predictions%&gt;%filter(ban==0), aes(x=miles, y=xb))+\n  geom_line(data= predictions%&gt;%filter(ban==1), aes(x=miles, y=xb)) +\n  labs ( colour = NULL, x = \"Miles Driven (Billions)\", y =  \"Expected Traffic Fatalities\" ) +\n  annotate(\"text\", x = 200, y = 2500, label = \"Text Ban\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 80, y = 2000, label = \"No Text Ban\", size=3.5, colour=\"gray30\")+\n    labs(x=\"Miles Driven (Billions)\", y=\"Expected Traffic Fatalities\")"
  },
  {
    "objectID": "ex2answers2024.html#model-south",
    "href": "ex2answers2024.html#model-south",
    "title": "exercise #2 answers",
    "section": "model (south)",
    "text": "model (south)\n\n\ncode\nm3 &lt;- lm(data=cell, numberofdeaths ~ text_ban+ miles + pop + south)\nmodelsummary(m3, stars=TRUE, coef_rename = TRUE)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n138.381**\n\n\n\n(47.751)\n\n\nTexting Ban\n−180.066***\n\n\n\n(47.233)\n\n\nTotal Miles Driven in State (billions)\n11.662***\n\n\n\n(2.542)\n\n\nState population (millions)\n−14.502\n\n\n\n(21.432)\n\n\nsouth\n246.421***\n\n\n\n(63.144)\n\n\nNum.Obs.\n50\n\n\nR2\n0.954\n\n\nR2 Adj.\n0.950\n\n\nAIC\n650.8\n\n\nBIC\n662.3\n\n\nLog.Lik.\n−319.408\n\n\nF\n231.981\n\n\nRMSE\n143.91\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\ncode\ndfavg &lt;- cell \n\npredictions&lt;- data.frame(upper=numeric(0),lower=numeric(0),xb=numeric(0), miles=numeric(0), south=numeric(0))\nc&lt;-1\nfor (i in seq(0,1,1)) { \n  for (j in seq(1,320,1)) {\n  dfavg$south = i\n  dfavg$miles = j+5\n  all &lt;- data.frame(predict(m3, se.fit=TRUE, newdata=dfavg))\n  upper = median(all$fit, na.rm=TRUE)+1.96*(median(all$se.fit, na.rm=TRUE))\n  lower = median(all$fit, na.rm=TRUE)-1.96*(median(all$se.fit, na.rm=TRUE))\n  xb = median(all$fit, na.rm=TRUE)\n  predictions[c,] &lt;-data.frame(upper,lower,xb,j, i)\n  c=c+1\n  }\n}\n  \n  ggplot()+\n  geom_line(data= predictions%&gt;%filter(south==0), aes(x=miles, y=xb))+\n  geom_line(data= predictions%&gt;%filter(south==1), aes(x=miles, y=xb)) +\n  labs ( colour = NULL, x = \"Miles Driven (Billions)\", y =  \"Expected Traffic Fatalities\" ) +\n  annotate(\"text\", x = 250, y = 2500, label = \"Non-south\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 80, y = 2000, label = \"South\", size=3.5, colour=\"gray30\")+\n    labs(x=\"Miles Driven (Billions)\", y=\"Expected Traffic Fatalities\")"
  },
  {
    "objectID": "ex3answers2024.html",
    "href": "ex3answers2024.html",
    "title": "Exercise #3",
    "section": "",
    "text": "code\nresults &lt;- data.frame ( var = character (0), coef = numeric(0), se = numeric (0), n = numeric (0))\n\nfor(i in seq(10,1000,1)) { \n  set.seed(12345)\n  data &lt;- tibble(\n    X &lt;- rnorm_multi(i, 3, \n                     mu=c(0, 0, 0), \n                     sd=1,\n                     r = c(0.0, 0.0, 0.0),\n                     varnames=c(\"x1\", \"x2\", \"e\"))\n  ) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e)\n  \n  mod &lt;- (lm(y ~ x1 + x2, data=data))\n  \n  results[((i-9)*3-2):((i-9)*3),1:4] &lt;- data.frame ( var = c(\"x0\",\"x1\",\"x2\"), coef(summary(mod))[,1:2], i)\n  \n}\n\nresults &lt;-data.frame(results, t=results$coef/results$se)\n\n\np1 &lt;- ggplot(results, aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs ( colour = NULL, x = \"Sample Size\", y =  \"Estimated Coefficient\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\np1\n\n\n\n\n\ncode\np2 &lt;- ggplot(results, aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs ( colour = NULL, x = \"Sample Size\", y =  \"Estimated Standard Error\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\np2"
  },
  {
    "objectID": "ex3answers2024.html#rhoxe--",
    "href": "ex3answers2024.html#rhoxe--",
    "title": "Exercise #3",
    "section": "rho(x,e) —-",
    "text": "rho(x,e) —-\n\n\ncode\nresults &lt;- data.frame()\ni=1\nfor(r in seq(0, .95, .01)) {\n  set.seed(8675309)\n  X &lt;- rnorm_multi(1000, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0, r, 0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) \n  y = .5 + X$x1 + 2*X$x2 + X$e\n  data &lt;- data.frame(X, y)\n  m &lt;- lm(y ~ x1 + x2, data=data)\n  \n  b0&lt;-coef(summary(m))[1,1]\n  b1&lt;-coef(summary(m))[2,1]\n  b2&lt;-coef(summary(m))[3,1]\n  se0&lt;-coef(summary(m))[1,2]\n  se1&lt;-coef(summary(m))[2,2]\n  se2&lt;-coef(summary(m))[3,2]\n  results[i,1:7] &lt;- data.frame(rho = r, b0, b1, b2, se0, se1, se2)\n  \n  i = i+1\n}\n\nb &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=b0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=b1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=b2), color=\"green\")+\n  labs ( colour = NULL, x = \"Correlation (x1, e)\", y =  \"Estimated Coefficients\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nb\n\n\n\n\n\ncode\nse &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=se0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=se1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=se2), color=\"green\") +\n  labs ( colour = NULL, x = \"Correlation (x1, e)\", y =  \"Estimated St. Errs\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nse"
  },
  {
    "objectID": "ex3answers2024.html#rhox1x2--",
    "href": "ex3answers2024.html#rhox1x2--",
    "title": "Exercise #3",
    "section": "rho(x1,x2) —-",
    "text": "rho(x1,x2) —-\n\n\ncode\nresults &lt;- data.frame()\ni=1\nfor(r in seq(0, .95, .01)) {\n  set.seed(8675309)\n  X &lt;- rnorm_multi(1000, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(r, 0, 0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) \n  y = .5 + X$x1 + 2*X$x2 + X$e\n  data &lt;- data.frame(X, y)\n  m &lt;- lm(y ~ x1 + x2, data=data)\n  i = i+1\n  b0&lt;-coef(summary(m))[1,1]\n  b1&lt;-coef(summary(m))[2,1]\n  b2&lt;-coef(summary(m))[3,1]\n  se0&lt;-coef(summary(m))[1,2]\n  se1&lt;-coef(summary(m))[2,2]\n  se2&lt;-coef(summary(m))[3,2]\n  results[i,1:7] &lt;- data.frame(rho = r, b0, b1, b2, se0, se1, se2)\n}\n\nb &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=b0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=b1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=b2), color=\"green\")+\n  labs ( colour = NULL, x = \"Correlation (x1, x2)\", y =  \"Estimated Coefficients\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nb\n\n\n\n\n\ncode\nse &lt;- ggplot() +\n  geom_line(data=results, aes(x=rho, y=se0), color=\"red\")+\n  geom_line(data=results, aes(x=rho, y=se1), color=\"blue\")+\n  geom_line(data=results, aes(x=rho, y=se2), color=\"green\") +\n  labs ( colour = NULL, x = \"Correlation (x1, x2)\", y =  \"Estimated St. Errs\" ) +\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\nse"
  },
  {
    "objectID": "normality24.html",
    "href": "normality24.html",
    "title": "Normality",
    "section": "",
    "text": "Let’s put the pieces of the OLS model together.\n\nthe data matrix consists of a vector, \\(y\\), and a matrix \\(\\mathbf{X}\\) including a constant.\n\\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) :: the covariance of \\(\\mathbf{X}\\) and \\(y\\) weighted by the covariance of \\(\\mathbf{X}\\).\n\\(\\widehat{\\sigma^2} = (\\epsilon' \\epsilon) * 1/(N - k - 1)\\)\nthe variance-covariance matrix of \\(\\widehat{\\beta}\\) is \\(\\widehat{\\sigma^2} \\mathbf{(X'X)^{-1}}\\)\nthe square root of the main diagonal gives the standard errors of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "normality24.html#implementing-the-model",
    "href": "normality24.html#implementing-the-model",
    "title": "Normality",
    "section": "",
    "text": "Let’s put the pieces of the OLS model together.\n\nthe data matrix consists of a vector, \\(y\\), and a matrix \\(\\mathbf{X}\\) including a constant.\n\\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) :: the covariance of \\(\\mathbf{X}\\) and \\(y\\) weighted by the covariance of \\(\\mathbf{X}\\).\n\\(\\widehat{\\sigma^2} = (\\epsilon' \\epsilon) * 1/(N - k - 1)\\)\nthe variance-covariance matrix of \\(\\widehat{\\beta}\\) is \\(\\widehat{\\sigma^2} \\mathbf{(X'X)^{-1}}\\)\nthe square root of the main diagonal gives the standard errors of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "normality24.html#implementing-the-model-1",
    "href": "normality24.html#implementing-the-model-1",
    "title": "Normality",
    "section": "Implementing the model",
    "text": "Implementing the model\nIn a little more detail, note we’re making use of every part of the regression here:\n\nEstimate \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\nCompute \\(u = y - \\mathbf{X \\widehat{\\beta}}\\)\nCompute the sum of squared residuals, \\((u' u)\\).\nUse that to compute the error variance, \\(\\widehat{\\sigma^2} = (u'u) * 1/(N - k - 1)\\)\nTo measure uncertainty around \\(\\mathbf{\\widehat{\\beta}}\\), divide the error variance by the covariance of \\(\\mathbf{X}\\).\nThat’s the variance-covariance matrix of \\(\\widehat{\\beta} = \\widehat{\\sigma^2} \\mathbf{(X'X)^{-1}}\\) the square root of the main diagonal gives the standard errors of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "normality24.html#quantities",
    "href": "normality24.html#quantities",
    "title": "Normality",
    "section": "Quantities",
    "text": "Quantities\nWe end up with a number of interesting and useful quantities:\n\n\\(\\widehat{y} = \\mathbf{X} \\widehat{\\beta}\\) :: linear predictions\n\\(u = y- \\widehat{y}\\) :: these are the residuals.\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\).\n\\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\)\n\\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\). $ TSS = SSE + MSS$"
  },
  {
    "objectID": "normality24.html#uncertainty-about-mathbfwidehatbeta",
    "href": "normality24.html#uncertainty-about-mathbfwidehatbeta",
    "title": "Normality",
    "section": "Uncertainty about \\(\\mathbf{\\widehat{\\beta}}\\)",
    "text": "Uncertainty about \\(\\mathbf{\\widehat{\\beta}}\\)\nWhy are we uncertain about the estimates?\n\n\\(\\widehat{\\beta}\\) represents our sample-based estimates of the population parameters, assuming the sample is random, representative, etc.\nIf we imagine our one sample to be one of many possible samples, then \\(\\widehat{\\beta}\\) is one estimate of many possible estimates from those hypothetical samples.\nIf we have many samples, each with a mean, we know those means have a normal distribution. So \\(\\widehat{\\beta}\\) is drawn from a normal distribution."
  },
  {
    "objectID": "normality24.html#distribution-of-hatbeta",
    "href": "normality24.html#distribution-of-hatbeta",
    "title": "Normality",
    "section": "Distribution of \\(\\hat{\\beta}\\)",
    "text": "Distribution of \\(\\hat{\\beta}\\)\nThe \\(\\widehat{\\beta}\\)s are normally distributed:\n\\[ \\widehat{\\beta} \\sim \\mathcal{N}(\\beta, var({\\beta})) \\]\nThe estimates are normally distributed, and the \\(\\widehat{\\beta}\\)s we estimate are drawn from that distribution, conditional on the sample of data, \\(N\\). The sample itself is one of a (theoretically) infinite number of samples.\nThink of it this way - the forces in \\(\\epsilon\\) shaped which of these samples of \\(\\widehat{\\beta}\\) we happened to draw. If we’d gotten a different sample, our estimates of \\(\\widehat{\\beta}\\) would have been different. So which distribution we’re drawing from changes as we change model specification even in the same data sample.\nIf we estimated regressions in all those possible samples, the estimates would comprise the entire distribution of \\(\\widehat{\\beta}\\). The mean would be \\(\\beta\\); the variances would be \\(var({\\beta})\\).\nIf the \\(\\widehat{\\beta}\\)s are normally distributed with mean \\(\\beta\\) and we have an estimate of the variance, we can evaluate in probability terms how ``close’’ \\(\\widehat{\\beta}\\) is to \\(\\beta\\); after all, we know the properties of the normal distribution. That’s inference.\nHow do we have such confidence \\(\\widehat{\\beta}\\) is normally distributed?\nWe can mainly rely on the normality of \\(\\widehat{\\beta}\\) because of the central limit theorem.\nIt’s also true that the variance in \\(\\widehat{\\beta}\\) arises from our particular sample (note the variance of the assumed distribution of \\(\\widehat{\\beta}\\) is estimated). So large samples will be associated with smaller variances. Another reason we can rely on normality is that, in large samples, \\(\\mathbf{X' \\widehat{\\beta}}\\) is normal, and in the OLS model, \\(y\\) is normal - therefore \\(\\widehat{y}\\) is normal, and so is \\(y -\\widehat{y} = \\epsilon\\).}"
  },
  {
    "objectID": "normality24.html#central-limit-theorem",
    "href": "normality24.html#central-limit-theorem",
    "title": "Normality",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ \\sum\\limits_{n \\rightarrow \\infty} \\frac{(\\bar{X_i})}{n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "normality24.html#question",
    "href": "normality24.html#question",
    "title": "Normality",
    "section": "Question",
    "text": "Question\nSuppose we estimated a regression, but then wanted to simulate the distribution of \\(\\hat{\\beta}\\)s. How would we do it? \\ ~\\\nOnce we generated that distribution, how would we characterize \\(\\hat{\\beta}, \\hat{\\sigma^2}\\)?"
  },
  {
    "objectID": "normality24.html#simulating-the-distribution-of-widehatbeta",
    "href": "normality24.html#simulating-the-distribution-of-widehatbeta",
    "title": "Normality",
    "section": "Simulating the distribution of \\(\\widehat{\\beta}\\)",
    "text": "Simulating the distribution of \\(\\widehat{\\beta}\\)\n\nsuppose, from our regression, we have one estimate of each \\(\\widehat{\\beta}\\), one estimate of the variance of each.\nwe know these are draws from a normal distribution.\nsuppose we assume they are drawn from a normal distribution with mean \\(\\widehat{\\beta}\\), and variance \\(\\widehat{\\sigma^2}\\).\nwe could make a large number of draws from that distribution, say 10,000. What would this look like? Why?\nif we did this, we’d have the simulated distribution of \\(\\widehat{\\beta}\\). How would we summarize that distribution?"
  },
  {
    "objectID": "normality24.html#measuring-uncertainty",
    "href": "normality24.html#measuring-uncertainty",
    "title": "Normality",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\n-We can think of \\(\\widehat{\\sigma^2}\\) as the average (squared) error per observation.\n\nThe var-cov \\(\\widehat{\\beta}\\) matrix weights the average squared error by the covariance of \\(\\mathbf{X}\\).\nThe main diagonal of the var-cov \\(\\widehat{\\beta}\\) is the variance of \\(\\widehat{\\beta}\\); the square root is the standard deviation of \\(\\widehat{\\beta}\\).\nIf \\(\\widehat{\\beta}\\) is the estimate of the mean of all normally distributed \\(\\beta\\)s, its standard deviation is the average distance of estimates like this one from the population parameter.\nIf it’s large, the average \\(\\widehat{\\beta}\\) is far from the population parameter.\nIf it’s small, the average \\(\\widehat{\\beta}\\) is close to the population parameter."
  },
  {
    "objectID": "normality24.html#simulating-uncertainty",
    "href": "normality24.html#simulating-uncertainty",
    "title": "Normality",
    "section": "Simulating Uncertainty",
    "text": "Simulating Uncertainty\nIf we simulate the distribution of \\(\\widehat{\\beta}\\) based on the estimated coefficients and var-cov matrix, we can rely on the moments of that distribution:\n\nthe median would be our point estimate of \\(\\widehat{\\beta}\\).\nthe percentiles (2.5, 97.5) would be our confidence boundaries.\n\nNotice the generality here - we could simulate \\(\\widehat{\\beta}\\), multiply by interesting values of \\(\\mathbf{X}\\) to generate predictions, and rely on those moments of the simulated distribution."
  },
  {
    "objectID": "matrix24s.html",
    "href": "matrix24s.html",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model.\n\n\n\n\nA single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A.\n\n\n\nThe dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar.\n\n\n\n\nA is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]\n\n\n\n\n\n\nA rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3).\n\n\n\nA vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "predictionmethods24.html",
    "href": "predictionmethods24.html",
    "title": "Prediction Methods",
    "section": "",
    "text": "We’ll use a dataset of Major League Baseball attendance to illustrate different methods of generating predictions from multivariate models.\n\n\ncode\nmlb &lt;- read.csv(\"/users/dave/documents/teaching/501/2023/slides/L3_multivariate/code/mlbattendance/MLBattend.csv\")  \n\n# rescale home attendance\nmlb$Home_attend &lt;- mlb$Home_attend/1000\n\ndatasummary(All(mlb) ~ mean+ median+ sd+min+max,  data=mlb, style=\"grid\", title = \"MLB attendance data\")\n\n\n\nMLB attendance data\n\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\nSeason\n1985.06\n1985.00\n9.27\n1969.00\n2000.00\n\n\nHome_attend\n1777.99\n1681.90\n755.87\n306.76\n4483.35\n\n\nRuns_scored\n694.94\n691.50\n105.17\n329.00\n1009.00\n\n\nRuns_allowed\n694.89\n693.00\n105.52\n331.00\n1103.00\n\n\nWins\n78.85\n79.00\n12.67\n37.00\n114.00\n\n\nLosses\n78.88\n79.00\n12.65\n40.00\n110.00\n\n\nGames_behind\n14.39\n13.00\n11.75\n0.00\n52.00\n\n\n\n\n\n\n\n\n\nHere’s a simple model predicting season attendance at MLB games, regressing home attendance on runs scored, then generating “in-sample” predictions by computing:\n\\[\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\]\nwhere \\(x_i\\) is the number of runs scored for each row/observation in the estimation data.\n\n\ncode\nm1 &lt;- lm(data=mlb, Home_attend ~  Runs_scored)\nmodelsummary(m1)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−691.488\n\n\n\n(151.853)\n\n\nRuns_scored\n3.554\n\n\n\n(0.216)\n\n\nNum.Obs.\n838\n\n\nR2\n0.244\n\n\nR2 Adj.\n0.244\n\n\nAIC\n13256.5\n\n\nBIC\n13270.7\n\n\nLog.Lik.\n−6625.265\n\n\nF\n270.514\n\n\nRMSE\n656.62\n\n\n\n\n\n\n\ncode\np1 &lt;- data.frame(predict(m1), mlb$Runs_scored)\n\nggplot(p1, aes(x=mlb.Runs_scored, y=predict.m1.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\nHere’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nmodelsummary(m3)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−70849.854\n\n\n\n(4473.400)\n\n\nRuns_scored\n3.123\n\n\n\n(0.338)\n\n\nRuns_allowed\n−1.887\n\n\n\n(0.357)\n\n\nSeason\n36.215\n\n\n\n(2.284)\n\n\nGames_behind\n−8.273\n\n\n\n(2.755)\n\n\nNum.Obs.\n838\n\n\nR2\n0.471\n\n\nR2 Adj.\n0.469\n\n\nAIC\n12963.1\n\n\nBIC\n12991.5\n\n\nLog.Lik.\n−6475.561\n\n\nF\n185.758\n\n\nRMSE\n549.20\n\n\n\n\n\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from. What’s missing here?\n\n\n\n\n\n\nControl\n\n\n\nHold all variables constant at means, medians, or modes, except the one of interest, so we can focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "predictionmethods24.html#bivariate-model-predictions",
    "href": "predictionmethods24.html#bivariate-model-predictions",
    "title": "Prediction Methods",
    "section": "",
    "text": "Here’s a simple model predicting season attendance at MLB games, regressing home attendance on runs scored, then generating “in-sample” predictions by computing:\n\\[\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\]\nwhere \\(x_i\\) is the number of runs scored for each row/observation in the estimation data.\n\n\ncode\nm1 &lt;- lm(data=mlb, Home_attend ~  Runs_scored)\nmodelsummary(m1)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−691.488\n\n\n\n(151.853)\n\n\nRuns_scored\n3.554\n\n\n\n(0.216)\n\n\nNum.Obs.\n838\n\n\nR2\n0.244\n\n\nR2 Adj.\n0.244\n\n\nAIC\n13256.5\n\n\nBIC\n13270.7\n\n\nLog.Lik.\n−6625.265\n\n\nF\n270.514\n\n\nRMSE\n656.62\n\n\n\n\n\n\n\ncode\np1 &lt;- data.frame(predict(m1), mlb$Runs_scored)\n\nggplot(p1, aes(x=mlb.Runs_scored, y=predict.m1.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")"
  },
  {
    "objectID": "predictionmethods24.html#mltivariate-model-predictions",
    "href": "predictionmethods24.html#mltivariate-model-predictions",
    "title": "Prediction methods",
    "section": "",
    "text": "Here’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nsummary(m3)\n\n\n\nCall:\nlm(formula = Home_attend ~ Runs_scored + Runs_allowed + Season + \n    Games_behind, data = mlb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1273.3  -402.7   -51.4   342.7  2921.6 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -7.085e+04  4.473e+03 -15.838  &lt; 2e-16 ***\nRuns_scored   3.123e+00  3.383e-01   9.230  &lt; 2e-16 ***\nRuns_allowed -1.887e+00  3.571e-01  -5.286  1.6e-07 ***\nSeason        3.621e+01  2.284e+00  15.858  &lt; 2e-16 ***\nGames_behind -8.273e+00  2.755e+00  -3.003  0.00276 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 550.8 on 833 degrees of freedom\nMultiple R-squared:  0.4715,    Adjusted R-squared:  0.4689 \nF-statistic: 185.8 on 4 and 833 DF,  p-value: &lt; 2.2e-16\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from.\nWhat’s missing here? Ideally, we’d hold all variables, except the one of interest, constant so we can just focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "predictionmethods24.html#at-means-predictions-adjusted-effects",
    "href": "predictionmethods24.html#at-means-predictions-adjusted-effects",
    "title": "Prediction Methods",
    "section": "At-means predictions (adjusted effects)",
    "text": "At-means predictions (adjusted effects)\nAt-means predictions (also called “adjusted effects”) set all variables except the \\(x\\) of interest at a sensible value - usually the mean, median, or mode depending on the level of measurment. Holding those constant, but varying just the \\(x\\) of interest produces predictions of \\(y\\) that are neater and easier to discuss than the in-sample ones above.\n\nSummarize the estimation data\nLet’s find the means etc. of the variables in the model. It’s important only to consider the cases that are in the estimation data, that is, actually used in the model. Write code to identify cases in the model, then use the means, etc. of these for the predictions:\n\n\ncode\nmlb$used &lt;- TRUE\nmlb$used[na.action(m3)] &lt;- FALSE\nestdata &lt;- mlb %&gt;%  filter(used==\"TRUE\")\n\ndatasummary(Home_attend+Runs_scored +Runs_allowed + Season + Games_behind ~ mean+ median+ sd+min+max,  data=mlb, style=\"grid\", title = \"MLB estimation data\")\n\n\n\nMLB estimation data\n\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\nHome_attend\n1777.99\n1681.90\n755.87\n306.76\n4483.35\n\n\nRuns_scored\n694.94\n691.50\n105.17\n329.00\n1009.00\n\n\nRuns_allowed\n694.89\n693.00\n105.52\n331.00\n1103.00\n\n\nSeason\n1985.06\n1985.00\n9.27\n1969.00\n2000.00\n\n\nGames_behind\n14.39\n13.00\n11.75\n0.00\n52.00\n\n\n\n\n\n\n\nIn this case, it turns out we use all the cases in the data - it’s important to check this any time you’re making model predictions.\n\n\nGenerate at-mean predictions\nCreate a new data frame with as many observations as there are interesting values of the \\(x\\) variable of interest. Then, set all variables except the \\(x\\) of interest at their means, medians, or modes. Using the standard errors of the predictions, generate the boundaries of the confidence intervals by end point transformation.\n\\[ \\widehat{y} \\pm 1.96 \\times se(\\widehat{y}) \\]\n\n\ncode\noosdata &lt;- data.frame(Intercept=1, Runs_allowed=694 , Season=1985 , Games_behind=14 , Runs_scored= c(seq(330,1000,10)))\n\nmlb.predict &lt;- data.frame(oosdata, predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=oosdata))\n\n# confidence bounds by end point transformation\nmlb.predict &lt;- mlb.predict %&gt;% mutate(ub=fit.fit+1.96*se.fit) %&gt;% mutate(lb=fit.fit-1.96*se.fit)\n\natmean &lt;- ggplot(mlb.predict, aes(x=Runs_scored, y=fit.fit)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") + \n  theme_minimal() +\n  ggtitle(\"At-mean effects\")\n\natmean\n\n\n\n\n\n\n\nAside on standard errors of \\(\\widehat{y}\\)\nThe standard errors of \\(\\widehat{y}\\) are calculated using the \\(x\\) values, and the variance-covariance matrix of the coefficients. These are usually calculated as follows:\n\\[ X ~VX' \\]\nmultiplying the \\(X\\) matrix (remembering this could be the out of sample \\(X\\) matrix), the variance-covariance matrix of \\(\\widehat{\\beta}\\), and the transpose of the \\(X\\) matrix. The square root of the main diagonal gives the standard errors of \\(\\widehat{y}\\).\n\n\n\n\n\n\nUncertainty about \\(\\widehat{y}\\)\n\n\n\nThe intuition is that we’re using our uncertainty about the coefficients to generate measures of uncertainty about the predictions."
  },
  {
    "objectID": "predictionmethods24.html#average-effects-end-point-boundaries",
    "href": "predictionmethods24.html#average-effects-end-point-boundaries",
    "title": "Prediction Methods",
    "section": "Average effects, end point boundaries",
    "text": "Average effects, end point boundaries\nAverage effects are where we’re computing \\(N\\) predictions for every interesting value of \\(x\\), then taking the average of those predictions for each value of \\(x\\). This is a counterfactual approach - we’re using the actual data, changing only the variable of interest, as if all observations took on the same value of that variable.\nFor instance, in our MLB attendance model, we change the value of “Runs Scored” to the minimum value, 329, for every observation, keeping all the other variables as they are in the real estimation data. Compute the predictions for all the observations at \\(x = 329\\), then take the average of those predictions - now iterate to 330, 331, etc.\n\n\n\n\n\n\nCounterfactuals\n\n\n\nWe are treating the estimation data as if every team scored 329 runs - this is the counterfactual - then computing the average attendance for that counterfactual, and doing this for all counterfactuals (values of \\(x\\)).\n\n\n\n\ncode\n# preserve the original values of Runs Scored\nmlb &lt;- mlb%&gt;%\nmutate(original_runs_scored = Runs_scored)\n\nxb = 0\nse = 0\nrs = 0\n\nfor(i in seq(330,1000,1)){  #iterating by 1 to max number of obs we're taking medians for\n  mlb$Runs_scored &lt;- i\n  mlb.predict &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=mlb))\n  xb[i-329] &lt;- median(mlb.predict$fit.fit) #index using i but minus constant to start at row 1\n  se[i-329] &lt;- median(mlb.predict$se.fit)\n  rs[i-329] &lt;- i\n}\n\navg.pred &lt;- data.frame(xb, se, rs)\n\n# upper and lower bounds by end point transformation\n\navg.pred &lt;- avg.pred %&gt;% mutate(ub=xb+1.96*se) %&gt;% mutate(lb=xb-1.96*se)\n\n# reset the estimation data to the actual values of Runs Scored\nmlb &lt;- mlb%&gt;%\n  mutate(Runs_scored=original_runs_scored )\n\n#plot\naverage &lt;- ggplot(avg.pred, aes(x=rs, y=xb)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Average Effects\") +\n  theme_minimal()\n\naverage\n\n\n\n\n\n\nComparing at-mean and average effects\n\n\ncode\natmean + average"
  },
  {
    "objectID": "predictionmethods24.html#average-effects-percentile-boundaries",
    "href": "predictionmethods24.html#average-effects-percentile-boundaries",
    "title": "Prediction methods",
    "section": "Average effects, percentile boundaries",
    "text": "Average effects, percentile boundaries\nIn the last example, we computed confidence interval boundaries by end point transformation. Here, we’ll compute the same average effects as above, but using percentiles of the distribution of predictions as upper and lower bounds.\n\n\ncode\n# preserve the original values of Runs Scored\nmlb &lt;- mlb%&gt;%\n  mutate(original_runs_scored = Runs_scored)\n\nxb = 0\nrs = 0\nqtilepreds &lt;-  data.frame ( lb0 = numeric(0),med0= numeric(0),\n              ub0= numeric(0), rs = numeric(0))\n\nfor(i in seq(330,1100,1)){  #iterating by 1 to max number of obs we're taking medians for\n  mlb$Runs_scored &lt;- i\n  mlb.predict &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=mlb))\n  xb &lt;-  quantile(mlb.predict$fit.fit, probs=c(.025,.5,.975))\n  XB &lt;- data.frame(t(xb))\n  qtilepreds[(i-329):(i-329),1:4] &lt;- data.frame(XB, i)  \n}\n\n# reset the estimation data to the actual values of Runs Scored\nmlb &lt;- mlb%&gt;%\n  mutate(Runs_scored=original_runs_scored )\n\n#plot\n\navgqtile &lt;- ggplot(qtilepreds, aes(x=rs, y=med0)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb0, ymax=ub0), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Average Effects, Percentile Bounds\")\n\navgqtile\n\n\n\n\n\n\n\n\n\n\nComparing average effects with different CI methods\n\n#compare\navgqtile + average"
  },
  {
    "objectID": "predictionmethods24.html#simulated-effects",
    "href": "predictionmethods24.html#simulated-effects",
    "title": "Prediction Methods",
    "section": "Simulated effects",
    "text": "Simulated effects\nThe last method we’ll consider is simulated effects. This is a way to generate predictions that are based on the distribution of the coefficients. The \\(\\widehat{\\beta}\\)s are estimates of the mean of a normal distribution, and the var-cov matrix of the coefficients is an estimate of the variance of that distribution. We can assume the distribution of \\(\\beta\\) is Normal because of the Central Limit Theorem.\nHere’s the process. Treat our \\(\\widehat{\\beta}\\)s as the mean of a normal distribution, and the var-cov matrix of the coefficients as the variance. Generate a large number of draws from that distribution, say 10,000. This will give us the simulated distribution of \\(\\widehat{\\beta}\\). We’ll have 10,000 estimates of \\(\\widehat{\\beta}\\).\nUsing these, we can now produce 10,000 predictions for each interesting value of \\(x\\). We can then summarize the distribution of those predictions, using the median as our point estimate, and the percentiles (2.5, 97.5) as our confidence boundaries.\n\n\ncode\nB &lt;- data.frame(rmvnorm(n=10000, mean=coef(m3), vcov(m3))) #simulated distribution of the estimates using the var-cov matrix of B as the variance, and the estimates of B as the mean.\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4')\n\nsim.preds &lt;- data.frame ( lb = numeric(0),med= numeric(0),\n                            ub= numeric(0), r = numeric(0))\n\nfor(i in seq(330,1000,1)){\n  xbR  &lt;- quantile(B$b0 + B$b1*i + B$b2*694 + B$b3*1985 +B$b4*14, probs=c(.025,.5,.975))\n  xbR&lt;- data.frame(t(xbR))\n  sim.preds[(i-329):(i-329),1:4] &lt;- data.frame(xbR, i)\n}\n\n#plot\nsim &lt;- ggplot(sim.preds, aes(x=r, y=med)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Simulated Effects\") +\n  theme_minimal()\n\nsim\n\n\n\n\n\n\nComparing methods\n\n\ncode\n(atmean + average + sim)"
  },
  {
    "objectID": "predictionmethods24.html#multivariate-model-predictions",
    "href": "predictionmethods24.html#multivariate-model-predictions",
    "title": "Prediction Methods",
    "section": "",
    "text": "Here’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nmodelsummary(m3)\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−70849.854\n\n\n\n(4473.400)\n\n\nRuns_scored\n3.123\n\n\n\n(0.338)\n\n\nRuns_allowed\n−1.887\n\n\n\n(0.357)\n\n\nSeason\n36.215\n\n\n\n(2.284)\n\n\nGames_behind\n−8.273\n\n\n\n(2.755)\n\n\nNum.Obs.\n838\n\n\nR2\n0.471\n\n\nR2 Adj.\n0.469\n\n\nAIC\n12963.1\n\n\nBIC\n12991.5\n\n\nLog.Lik.\n−6475.561\n\n\nF\n185.758\n\n\nRMSE\n549.20\n\n\n\n\n\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from. What’s missing here?\n\n\n\n\n\n\nControl\n\n\n\nHold all variables constant at means, medians, or modes, except the one of interest, so we can focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "slides.html#web-format",
    "href": "slides.html#web-format",
    "title": "Slides",
    "section": "",
    "text": "Course overview\nMatrix algebra basics\nProbability basics\nThinking about data\nBivariate model\nMultivariate model\nPrediction methods\nNormality\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "ex4answers2024.html",
    "href": "ex4answers2024.html",
    "title": "exercise #4 answers",
    "section": "",
    "text": "code\nnsim &lt;- function(i){\n  set.seed(12345)\n  X &lt;- rnorm_multi(i, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0.0, 0.0, 0.0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e) %&gt;%\n    mutate(pry=plogis(y)) %&gt;%\n    mutate(ybin=ifelse(pry&gt;.5,1,0)) %&gt;%\n    mutate(yb = rbinom(i,1,pry))\n  \n  mod &lt;- (glm(yb ~ x1 + x2, data=X , family=binomial(link=\"logit\")))\n  # print(summary(mod))\n  coef &lt;- numeric(0)\n  coef&lt;-coef(summary(mod))[1:3,1]\n  se &lt;- numeric(0)\n  se &lt;-coef(summary(mod))[1:3,2]\n  n &lt;- numeric(0)\n  n &lt;- i\n  var = c(\"x0\",\"x1\",\"x2\")\n  out &lt;- data.frame(var, coef, se, n)\n}\n\ni &lt;- seq(10,1000,1) #vector of i for \"nsim\" function\ndfout &lt;- map(i,nsim) |&gt; list_rbind()\nmleout &lt;- dfout %&gt;% filter(n&lt; 300)\n#plot\n\nb &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Coefficient\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\n\nse &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Standard Error\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\nb+se"
  },
  {
    "objectID": "ex4answers2024.html#q1---mle",
    "href": "ex4answers2024.html#q1---mle",
    "title": "exercise #4 answers",
    "section": "",
    "text": "code\nnsim &lt;- function(i){\n  set.seed(12345)\n  X &lt;- rnorm_multi(i, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0.0, 0.0, 0.0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e) %&gt;%\n    mutate(pry=plogis(y)) %&gt;%\n    mutate(ybin=ifelse(pry&gt;.5,1,0)) %&gt;%\n    mutate(yb = rbinom(i,1,pry))\n  \n  mod &lt;- (glm(yb ~ x1 + x2, data=X , family=binomial(link=\"logit\")))\n  # print(summary(mod))\n  coef &lt;- numeric(0)\n  coef&lt;-coef(summary(mod))[1:3,1]\n  se &lt;- numeric(0)\n  se &lt;-coef(summary(mod))[1:3,2]\n  n &lt;- numeric(0)\n  n &lt;- i\n  var = c(\"x0\",\"x1\",\"x2\")\n  out &lt;- data.frame(var, coef, se, n)\n}\n\ni &lt;- seq(10,1000,1) #vector of i for \"nsim\" function\ndfout &lt;- map(i,nsim) |&gt; list_rbind()\nmleout &lt;- dfout %&gt;% filter(n&lt; 300)\n#plot\n\nb &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Coefficient\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\n\nse &lt;- ggplot(dfout%&gt;% filter(n&gt;20), aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Standard Error\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\nb+se"
  },
  {
    "objectID": "ex4answers2024.html#q1-ols",
    "href": "ex4answers2024.html#q1-ols",
    "title": "exercise #4 answers",
    "section": "Q1 OLS",
    "text": "Q1 OLS\n\n\ncode\nnsim &lt;- function(i){\n  set.seed(12345)\n  X &lt;- rnorm_multi(i, 3, \n                   mu=c(0, 0, 0), \n                   sd=1,\n                   r = c(0.0, 0.0, 0.0),\n                   varnames=c(\"x1\", \"x2\", \"e\")) %&gt;%\n    mutate(y = .5 + 1*x1 + 2*x2 + e)\n  \n  mod &lt;- (lm(y ~ x1 + x2, data=X))\n  # print(summary(mod))\n  coef &lt;- numeric(0)\n  coef&lt;-coef(summary(mod))[1:3,1]\n  se &lt;- numeric(0)\n  se &lt;-coef(summary(mod))[1:3,2]\n  n &lt;- numeric(0)\n  n &lt;- i\n  var = c(\"x0\",\"x1\",\"x2\")\n  out &lt;- data.frame(var, coef, se, n)\n}\n\ni &lt;- seq(10,1000,1) #vector of i for \"nsim\" function\ndfout &lt;- map(i,nsim) |&gt; list_rbind()\nolsout &lt;- dfout %&gt;% filter(n&lt; 300)\n#plot\n\nb &lt;- ggplot(dfout, aes(x=n, y=coef, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Coefficient\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\n\nse &lt;- ggplot(dfout, aes(x=n, y=se, color=var)) +\n  geom_line() +\n  labs(x=\"Sample Size\", y=\"Estimated Standard Error\")+\n  theme ( legend.position = \"bottom\",\n          legend.key = element_blank() )\n\nb+se\n\n\n\n\n\n\n\n\n\nMLE’s small sample properties are far more variable than OLS small sample properties. In samples smaller than 300, MLE estimates varied from -19 to 990; OLS estimates varied from .30 to 2.37."
  },
  {
    "objectID": "ex4answers2024.html#q2---ill-treatment-and-torture-data",
    "href": "ex4answers2024.html#q2---ill-treatment-and-torture-data",
    "title": "exercise #4 answers",
    "section": "Q2 - Ill-Treatment and Torture data",
    "text": "Q2 - Ill-Treatment and Torture data\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\n\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nitt &lt;- itt %&gt;%\n  set_variable_labels(\n    lagRA = \"Restricted Access, t-1\",\n    civilwar = \"Civil War\",\n    lagprotest = \"Protests, t-1\",\n    p1 = \"Polity\",\n    p2 = \"Polity^2\",\n    p3 = \"Polity^3\", \n    wdi_gdpc = \"GDP per Capita (WDI)\",\n    wdi_pop = \"Population (WDI)\"\n  )\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +p1 +I(wdi_gdpc/1000) + I(wdi_pop/100000), data=itt)\nm2 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1 +I(wdi_gdpc/1000) + I(wdi_pop/100000), data=itt)\n\n# modelsummary(m1, coef_rename = TRUE, stars = TRUE, gof_map = c(\"adj.r.squared\", \"nobs\", \"F\", \"rmse\" ), gof_digits = 2)\n\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),  column.labels=c(\"Model 1\", \"Model 2\"),  dep.var.caption=\"Dependent Variable: Scarring Torture\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Restricted Access, t-1\", \"Civil War\", \"Protests, t-1\", \"Polity\", \"&lt;p&gt;Polity&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;\", \"&lt;p&gt;Polity&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;\", \"GDP per capita\", \"Population\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Scarring Torture\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nRestricted Access, t-1\n\n\n9.476*** (1.075)\n\n\n9.549*** (1.073)\n\n\n\n\nCivil War\n\n\n6.926*** (1.627)\n\n\n6.805*** (1.626)\n\n\n\n\nProtests, t-1\n\n\n0.203** (0.078)\n\n\n0.217** (0.077)\n\n\n\n\nPolity\n\n\n-1.243 (0.785)\n\n\n0.021 (0.057)\n\n\n\n\n\nPolity2\n\n\n\n0.121 (0.074)\n\n\n\n\n\n\n\nPolity3\n\n\n\n-0.003 (0.002)\n\n\n\n\n\n\nGDP per capita\n\n\n0.022 (0.045)\n\n\n0.020 (0.035)\n\n\n\n\nPopulation\n\n\n0.001*** (0.0002)\n\n\n0.001*** (0.0002)\n\n\n\n\nConstant\n\n\n8.317*** (2.452)\n\n\n4.963*** (0.820)\n\n\n\n\n\n\n\n\nObservations\n\n\n899\n\n\n899\n\n\n\n\nR2\n\n\n0.175\n\n\n0.172\n\n\n\n\nAdjusted R2\n\n\n0.167\n\n\n0.167\n\n\n\n\nF Statistic\n\n\n23.575*** (df = 8; 890)\n\n\n30.961*** (df = 6; 892)\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05"
  },
  {
    "objectID": "ex4answers2024.html#q2---simulated-effects",
    "href": "ex4answers2024.html#q2---simulated-effects",
    "title": "exercise #4 answers",
    "section": "Q2 - simulated effects",
    "text": "Q2 - simulated effects\nSimulate the distribution of \\(\\hat{\\beta}\\)’s and plot the expected scarring torture reports for different levels of protests against the government.\n\n\ncode\n#simulate b's \nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc + wdi_pop, data=itt)\n\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n                \ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= numeric(0), protest = numeric(0))\n\nfor (p in seq(1,40,1)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*0 + B$b3*p +B$b4*13+ B$b5*169+B$b6*2197+B$b7*8333+B$b8*.00005, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*0 + B$b3*p +B$b4*13+ B$b5*169+B$b6*2197+B$b7*8333+B$b8*.00005, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\n\nggplot()+\n  geom_ribbon(data=predictions, aes(x=protest, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=protest, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=protest, y=med0))+\n  geom_line(data= predictions, aes(x=protest, y=med1))+\n  labs ( colour = NULL, x = \"Protests Against Government\", y =  \"Expected Scarring Torture Reports\" ) +\n  annotate(\"text\", x = 8, y = 22, label = \"Restricted Access\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 8, y = 11, label = \"Unrestricted Access\", size=3.5, colour=\"gray30\")"
  },
  {
    "objectID": "inference24.html",
    "href": "inference24.html",
    "title": "Inference",
    "section": "",
    "text": "We’ve dug pretty extensively into how we produce OLS estimates of \\(\\widehat{\\beta}\\). Now we turn to the question of uncertainty about those estimates. After all, the data are a sample; the variables are perhaps imperfect measures; the variables may contain errors; we have ideas about the data generating process, but we don’t know the true model, so the model is certainly misspecified.\nWe need ways to express our uncertainty about the estimates of \\(\\beta\\), and about our confidence in the claims we make from the model.\nIf we simply assume \\(\\widehat{\\beta} = \\beta\\), then we are assuming:\n\nall the regression assumptions are met;\nthe sample is exactly equivalent to or representative of the population;\nthe model is specified correctly, and there are no sources of measurement error.\n\nThe probability these are all true is slim, but we are uncertain about the extent to which we do or do not meet these requirements. We need some statistical tools for quantifying our uncertainty about \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "inference24.html#framework-for-inference",
    "href": "inference24.html#framework-for-inference",
    "title": "Inference",
    "section": "Framework for Inference",
    "text": "Framework for Inference\n\nestablish a null hypothesis, e.g. \\(\\beta_1=0\\).\nestimate \\(\\widehat{\\beta_1}\\)\nestimate the error variance \\(\\widehat{\\sigma^2}\\).\ndetermine the distribution of \\(\\widehat{\\beta_k}\\).\ncompute a test statistic for \\(\\widehat{\\beta_1}\\).\ncompare that test statistic to critical values on the distribution of \\(\\widehat{\\beta_k}\\).\ndetermine the probability of observing the test statistic value, given the distribution of \\(\\widehat{\\beta_k}\\)"
  },
  {
    "objectID": "inference24.html#measuring-uncertainty",
    "href": "inference24.html#measuring-uncertainty",
    "title": "Inference",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\n\nStandard errors are basic measures of uncertainty.\nThe standard errors of the estimates of \\(\\widehat{\\beta_k}\\) are the standard deviations of the sampling distribution of the estimator.\nStandard errors are analogous to standard deviations surrounding the estimates."
  },
  {
    "objectID": "inference24.html#variance-of-the-estimates",
    "href": "inference24.html#variance-of-the-estimates",
    "title": "Inference",
    "section": "Variance of the estimates",
    "text": "Variance of the estimates\nIn the bivariate model, the variances of \\(\\widehat{\\beta_{0}}\\) and \\(\\widehat{\\beta_{1}}\\) are:\n\\[\\text{var}(\\widehat{\\beta_{0}}) = \\frac{\\sum\\limits_{i=1}^{n}x_{i}^{2}}{n \\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}} \\sigma^{2}\\] and\n\\[\\text{var}(\\widehat{\\beta_{1}}) = \\frac{\\sigma^{2}}{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}} \\]\nThe covariance of the the two estimates is\n\\[\\text{cov}(\\widehat{\\beta_{0}},\\widehat{\\beta_{1}}) = \\frac{-\\bar{x}}{ \\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}} \\sigma^{2}\\]\nCompute \\(\\widehat{\\sigma}^2\\) by\n\\[\\widehat{\\sigma}^2 = \\frac{\\sum\\widehat{u}^2}{n-k-1} \\]\nand the standard errors of \\(\\widehat{\\beta_{0}}\\) and \\(\\widehat{\\beta_{1}}\\) are the square roots of the first two expressions.\nRecalling that \\(\\mathbf{\\widehat{u'u}}\\) = SSE = \\(\\sum_{i=1}^{N}\\widehat{u^2}\\),\n\\[\\widehat{\\sigma}^2 = \\mathbf{\\widehat{u}'\\widehat{u}}  \\frac{1}{(n-k-1)}\\]\n\nRepetition: Variance-Covariance of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[ E(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0 \\\\\n0&\\sigma^{2} &\\cdots &0 \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\n0 &0 &\\cdots & \\sigma^{2} \\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0 \\\\\n0&1 &\\cdots &0 \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\n0 &0 &\\cdots & 1 \\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I} \\]\nThis is the variance-covariance matrix of the disturbances (\\(var-cov(e)\\)); it is symmetric, the main diagonal containing the variances of \\(\\epsilon_i\\). Assuming \\(Var(u|X)= \\sigma^2\\), the average of the main diagonal is \\(\\widehat{\\sigma^2}\\); the average of a constant is the constant."
  },
  {
    "objectID": "inference24.html#repetition-variance-covariance-of-widehatbeta",
    "href": "inference24.html#repetition-variance-covariance-of-widehatbeta",
    "title": "Inference",
    "section": "Repetition: Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Repetition: Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "inference24.html#variance-covariance-of-widehatbeta",
    "href": "inference24.html#variance-covariance-of-widehatbeta",
    "title": "Inference",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)}",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)}\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] =\\]\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k) \\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k) \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k) \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "inference24.html#standard-errors-of-beta_k",
    "href": "inference24.html#standard-errors-of-beta_k",
    "title": "Inference",
    "section": "Standard Errors of \\(\\beta_k\\) ",
    "text": "Standard Errors of \\(\\beta_k\\) \n\\[\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k) \\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k) \\\\\n\\vdots&\\vdots&\\ddots& \\vdots \\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & \\sqrt{var(\\beta_k)} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "inference24.html#assume",
    "href": "inference24.html#assume",
    "title": "Inference",
    "section": "Assume",
    "text": "Assume\n\\[ \\widehat{u} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}) \\]\nIf we meet the GM assumptions, then:\n\\[ \\widehat{\\beta} \\sim \\mathcal{N}(\\beta, \\widehat{var(\\beta)}) \\]\n\\[\\widehat{var(\\beta)} = \\widehat{\\sigma^{2}} \\mathbf{(X'X)^{-1}} \\]\nThe estimated error variance will be smaller relative to large variation in the \\(X\\) variables, so as variation in \\(X\\) grows, the uncertainty surrounding \\(\\mathbf{\\widehat{\\beta_k}}\\) will get smaller."
  },
  {
    "objectID": "inference24.html#think-about-the-ols-simulations",
    "href": "inference24.html#think-about-the-ols-simulations",
    "title": "Inference",
    "section": "Think about the OLS simulations",
    "text": "Think about the OLS simulations\nThe estimated error variance will be smaller relative to large variation in the \\(X\\) variables, so as variation in \\(X\\) grows, the uncertainty surrounding \\(\\mathbf{\\widehat{\\beta_k}}\\) will get smaller.\nwhat does this suggest about sample size? what does this suggest about correlation among \\(x\\) variables?"
  },
  {
    "objectID": "inference24.html#characteristics-of-textvarwidehatbeta_j",
    "href": "inference24.html#characteristics-of-textvarwidehatbeta_j",
    "title": "Inference",
    "section": "Characteristics of \\(\\text{var}(\\widehat{\\beta_{j}})\\)",
    "text": "Characteristics of \\(\\text{var}(\\widehat{\\beta_{j}})\\)\n\n\n\n\n\n\nStandard Errors vary with \\(\\sigma^{2}\\)\n\n\n\nThe variance of \\(\\widehat{\\beta_{j}}\\) varies directly with \\(\\sigma^{2}\\), so as the error variance increases, so does the variance surrounding \\(\\widehat{\\beta_{j}}\\) - this makes sense if we think about what the variance of the error term suggests. Large \\(\\sigma^{2}\\) suggests our model is not predicting \\(Y\\) very effectively - if the model itself is imprecise, then its component parts (the \\(\\widehat{\\beta_{j}}\\)s) will also be imprecise. So as \\(\\sigma^{2}\\) increases, so does \\(\\text{var}(\\widehat{\\beta_{j}})\\).\n\n\n\n\n\n\n\n\nStandard Errors vary with variation in \\(\\mathbf{X}\\)\n\n\n\nThe variance of \\(\\widehat{\\beta_{j}}\\) varies indirectly with \\(\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}\\), so as the sum of squares of \\(x\\) increases, \\(\\text{var}(\\widehat{\\beta_{j}})\\) decreases. This too makes sense because it means the more variability in \\(x\\) or the more information our \\(x\\) variables contain, the better our estimates of \\(\\widehat{\\beta_{j}}\\) are. Thus, more variability in the \\(x\\)s reduces \\(\\text{var}(\\widehat{\\beta_{j}})\\).\n\n\n\n\n\n\n\n\nStandard Errors vary with sample size\n\n\n\nSample size varies inversely with \\(\\text{var}(\\widehat{\\beta_{j}})\\) - larger samples (probably with greater variation in the \\(x\\)s and thus larger \\(\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}\\)) will produce more precise estimates of \\(\\widehat{\\beta_{j}}\\) and smaller \\(\\text{var}(\\widehat{\\beta_{j}})\\)."
  },
  {
    "objectID": "inference24.html#inferences-about-widehatbeta",
    "href": "inference24.html#inferences-about-widehatbeta",
    "title": "Inference",
    "section": "Inferences about \\(\\widehat{\\beta}\\)",
    "text": "Inferences about \\(\\widehat{\\beta}\\)\nThe standard error of \\(\\widehat{\\beta_k}\\) is the square root of the \\(k^{th}\\) diagonal element of the variance-covariance matrix of \\(\\widehat{\\beta}\\) ~\nIn scalar terms,\n\\[s.e.(\\widehat{\\beta_{1}}) =\\sqrt{\\frac{\\widehat{\\sigma^{2}}}{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}} \\nonumber   \\nonumber  \n= \\frac{\\widehat{\\sigma}}{\\sqrt{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}} \\]\nand the critical value is given by\n\\[z=\\frac{\\widehat{\\beta_{j}}-\\beta_{j}}{\\frac{\\widehat{\\sigma}}{\\sqrt{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}}}\\]\n\\[t= \\frac{\\widehat{\\beta_{j}}-\\beta_{j}\\sqrt{\\sum\\limits_{i=1}^{n} (x_{i}-\\bar{x})^{2}}}{\\widehat{\\sigma}} \\]\nThis is not Normal, even though the numerator is - the denominator is composed of the squared residuals, each of which is a \\(\\chi^{2}\\) variable. A standard normal variable divided by a \\(\\chi^{2}\\) distributed variable is distributed \\(t\\) with \\(n-k-1\\) degrees of freedom."
  },
  {
    "objectID": "inference24.html#confidence-intervals",
    "href": "inference24.html#confidence-intervals",
    "title": "Inference",
    "section": "",
    "text": "The estimates of \\(\\widehat{\\beta_{j}}\\) are drawn from a normal distribution, and we know the distribution of the variance, and we know how those distributions are related. For the \\(t\\) distribution, we know that 95% of the probability mass falls within 2 standard deviations of the mean, so if we construct a 95% confidence interval, we can say that 95% of the time the true value of \\(\\beta\\) will fall within the interval. Put another way, the interval contains a range of probable values for the true value of \\(\\beta\\).\nBecause we know (from above) that\n\\[t= \\frac{\\widehat{\\beta_{j}}-\\beta_{j}}{s.e. \\widehat{\\beta_{j}}}\\]\nwe can easily compute a confidence interval surrounding \\(\\widehat{\\beta_{j}}\\) by\n\\[CI_{c} = \\widehat{\\beta_{j}} \\pm c \\cdot {s.e. \\widehat{\\beta_{j}}}\\]\nwhere \\(c\\) represents the size of the confidence interval, so if \\(c\\) is the 97.5th percentile in the \\(t\\) distribution with \\(6\\) degrees of freedom, then the value of \\(c\\) is 2.447 (see the t-table in the back of the text). Thus, we compute the confidence interval by:\n\\[CI_{97.5} = \\widehat{\\beta_{j}} +  2.447 \\cdot {s.e. \\widehat{\\beta_{j}}}, \\widehat{\\beta_{j}} -  2.447 \\cdot {s.e. \\widehat{\\beta_{j}}}\\]"
  },
  {
    "objectID": "inference24.html#point-estimates",
    "href": "inference24.html#point-estimates",
    "title": "Inference",
    "section": "",
    "text": "Using confidence intervals, we state the probability the true coefficient lies between the upper and lower bounds of the interval. Point estimate tests allow us to test specific hypotheses about the value of \\(\\widehat{\\beta_{j}}\\). Let’s go back to the computation of a \\(t\\) statistic:\n\\[t= \\frac{\\widehat{\\beta_{j}}-\\beta_{j}}{s.e. \\widehat{\\beta_{j}}}\\]\nYou’ll notice in the numerator we’re subtracting the true coefficient from our estimate. Of course, we don’t know the true values of \\(\\beta\\), so we choose a value to which we want to compare our estimate. Then we’re generating the probability of drawing a sample with \\(\\widehat{\\beta_{j}}\\) given the value we choose."
  },
  {
    "objectID": "inference24.html#hypotheses",
    "href": "inference24.html#hypotheses",
    "title": "Inference",
    "section": "",
    "text": "Normally, we set up our hypothesis tests such that the value of \\(\\beta\\) we select is zero. Thus, we state null and alternative hypotheses:\n\\[H_0:\\beta_j=0 \\nonumber  \\\\\nH_1:\\beta_j\\ne 0 \\]\nThen, we choose significance levels, find the critical value associated with that significance level, and compare \\(t_{\\widehat{\\beta_j}}=\\widehat{\\beta_j}/se(\\widehat{\\beta_j})\\) to the critical value. Finally, we either reject or fail to reject \\(H_0\\). Note that we could choose other values to which we could compare the probability of \\(\\widehat{\\beta_{j}}\\), though selecting those values can be tricky."
  },
  {
    "objectID": "inference24.html#hypotheses-1",
    "href": "inference24.html#hypotheses-1",
    "title": "Inference",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nThe relationship we expect between \\(x\\) and \\(y\\) is the alternative hypothesis.\n\\(H_a\\) is the alternative to the null - the null includes everything that’s not the alternative; mathematically, it always includes zero.\nNon-directional alternative hypotheses simply expect \\(\\widehat{\\beta}\\) will not be zero. These are not very specific, less desirable.\nDirectional hypotheses expect a direction, e.g., \\(\\widehat{\\beta}\\) will be positive; the null in this case is \\(\\widehat{\\beta} \\leq 0\\). This is more specific, more desirable.\n\n\n\n\n\n\n\nDirectional Hypotheses\n\n\n\nStatistical software doesn’t know your data or argument}\nR/Stata does not know your hypotheses, whether directional or not; it does not know if your data are panel, time series, or cross sectional.\n\nR/Stata report 2-tailed p-values.\nIf you have a directional hypothesis, you need the 1-tailed p-value.\nDivide the 2-tailed value by 2 - in a 2-tailed test, it’s divided between the two tails. You want just one tail, so dividing the 2-tailed probability by 2 gives you the 1-tailed probability.\n\n\n\n\nIf you expect \\(\\widehat{\\beta} &gt; 0\\), and your estimated \\(\\widehat{\\beta} = -1.2\\) with a p-value of .001, you cannot reject the null. While \\(\\widehat{\\beta}\\) is statistically different from zero, it’s not in the direction of your hypothesis, so the result is not meaningful, does not support the alternative."
  },
  {
    "objectID": "inference24.html#dont-expect-the-null",
    "href": "inference24.html#dont-expect-the-null",
    "title": "Inference",
    "section": "Don’t Expect the Null",
    "text": "Don’t Expect the Null\nThe classical hypothesis test is always constructed such that the null includes zero. It is not possible to expect as the alternative hypothesis, that \\(\\widehat{\\beta} = 0\\). We should never see the alternative hypothesis::\nX will have no relationship to y: \\(H_A: \\beta_1=0\\)\nIn the classical test, the null includes a specific point (usually zero), though it can contain that point and a range of other values, e.g.:\n\\[H_0: \\beta_1 \\leq 0\\]\nHere, the null that \\(\\beta_1\\) is less than or equal to zero includes zero, but also includes all negative values. The alternative is that \\(\\beta_1\\) is greater than zero, so the alternative is a range of values, not a specific point.\nIf we flip this around to say “I expect \\(\\beta_1\\) to be zero”, then we are expecting the null hypothesis. Note that the alternative hypothesis is now a specific point, and the null is a range of values without any specific point expectation.\nIt’s important to note that the null may be around a point other than zero. We might expect the coefficient to be different from 5, so the null is \\(H_0: \\beta_1=5\\). Or, we might expect the coefficient to be less than or equal to 5, so the null is \\(H_0: \\beta_1 \\leq 5\\).\nThe point is we cannot expect in the alternative that \\(\\beta_1\\) is zero where the null is that it is anything except zero unless we build a new type of test. See Gill (1999) for an excellent discussion of this."
  },
  {
    "objectID": "inference24.html#joint-hypothesis-tests",
    "href": "inference24.html#joint-hypothesis-tests",
    "title": "Inference",
    "section": "",
    "text": "Normally we test hypotheses about specific parameters, but there are cases where we are interested in a hypothesis like this:\n\\[H_0:\\beta_1=\\beta_2=0 \\nonumber  \\\\\nH_1:\\beta_1\\ne \\beta_2\\ne 0\\]\nTo test something like this (and really, tests like this are something we should be extremely interested in), we use the F-statistic.\nThe joint hypothesis above has two restrictions - we expect two parameters to be zero in the null. We can have as many as \\(k-1\\) exclusions or restrictions in the model, and we typically call the exclusions or restrictions \\(q\\)."
  },
  {
    "objectID": "inference24.html#intuition",
    "href": "inference24.html#intuition",
    "title": "Inference",
    "section": "Intuition",
    "text": "Intuition\nDecompose the variation in the data and model this way:\n\\[TSS= MSS+RSS\\]\nSuppose a regression like this:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\nOur usual, single coefficient hypothesis test, say on \\(\\beta_3\\) is\n\\[H_0: \\beta_3=0\\]\nwhich is equivalent to\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + {\\mathbf{0}} x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\] call this the unrestricted model:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\n\\[RSS_U = \\sum_{i=1}^N \\widehat{u_U}^2\\]\nand refer to this as the restricted model:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + {\\mathbf{0}} x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\n\\[RSS_R = \\sum_{i=1}^N \\widehat{u_R}^2\\]\nAre these quantities different from one another?\n\\[RSS_U \\neq RSS_R\\]\nIf\n\\[RSS_U = RSS_R = 0\\]\nthen the two models are indistinguishable, and \\(\\beta_3\\) is not different from zero. Alternatively, if\n\\[RSS_U \\neq RSS_R \\neq 0\\]\nwe can reject the null that the models are the same, and the specific null that \\(\\beta_3=0\\). By the way, this single coefficient F-test is inferentially equivalent to the t-test; in fact, \\(F_{\\beta_{k}}= t^2_{\\beta_{k}}\\)."
  },
  {
    "objectID": "inference24.html#extension",
    "href": "inference24.html#extension",
    "title": "Inference",
    "section": "Extension",
    "text": "Extension\nExtend this to the following hypothesis test:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\n\\[H_0: \\beta_3=\\beta_4=\\beta_5=0\\]\nor that the effects of \\(x_3, x_4, x_5\\) are individually and jointly zero.\nUnrestricted:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 +\\beta_5 x_5 + u_i \\]\nRestricted:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + {\\mathbf{0}} x_3 + {\\mathbf{0}} x_4 +{\\mathbf{0}} x_5 + u_i \\]"
  },
  {
    "objectID": "inference24.html#f-test",
    "href": "inference24.html#f-test",
    "title": "Inference",
    "section": "F-test",
    "text": "F-test\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)} \\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or “normed” by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\)."
  },
  {
    "objectID": "inference24.html#alternatively",
    "href": "inference24.html#alternatively",
    "title": "Inference",
    "section": "Alternatively",
    "text": "Alternatively\nWe could equivalently state this as:\n\\[F=\\frac{(\\sum\\widehat{u}^{2}_{R}-\\sum\\widehat{u}^{2}_{U})/q}{\\sum\\widehat{u}^{2}_{U}/(n-k-1)} \\]\nNotice that the denominator is also \\(\\widehat{\\sigma^2}\\). The resulting statistic is distributed \\(F\\sim F_{q,n-k-1}\\).\nor we can write the same thing in terms of the \\(R^2\\) estimates of the two models:\n\\[F=\\frac{(R^2_{U}-R^2_{R})/q}{(1-R^2_{U})/(n-k-1)} \\]"
  },
  {
    "objectID": "inference24.html#in-practice",
    "href": "inference24.html#in-practice",
    "title": "Inference",
    "section": "In practice ",
    "text": "In practice \nFor a joint hypothesis test, we estimate two nested models which we label the restricted and unrestricted models.\n\n\n\n\n\n\nNote\n\n\n\nTwo models are nested iff they contain precisely the same observations, and the variables in one model are a strict subset of the variables in the other.\n\n\nThe unrestricted model has all \\(k\\) variables; the restricted model has \\(k-q\\) variables, excluding the variables whose joint effect we want to test.\nEstimate the models, and examine how much the RSS increases when we exclude the \\(q\\) variables from the model - of course, the RSS will always increase when we drop variables, but the question is how much will it increase. The F-test measures the increase in RSS in the restricted model relative to that of the unrestricted model."
  },
  {
    "objectID": "inference24.html#a-common-example",
    "href": "inference24.html#a-common-example",
    "title": "Inference",
    "section": "A common example",
    "text": "A common example\nEvery OLS model reports a “model F-test” - this compares the model you estimated with all your \\(x\\) variables (this is the unrestricted model) to the null model - no variables, just a constant. The null hypothesis is\n\\[H_0: \\beta_1=\\beta_2 \\ldots = \\beta_k = 0\\]\nIt’s a test of your model against the model with only a constant - it’s comparing the value of your model against the model where our best guess at what explains \\(y\\) is \\(\\beta_0 = \\bar{y}\\)."
  },
  {
    "objectID": "inference24.html#tests-like-this-are-important-and-powerful",
    "href": "inference24.html#tests-like-this-are-important-and-powerful",
    "title": "Inference",
    "section": "Tests like this are important and powerful:",
    "text": "Tests like this are important and powerful:\nOur enterprise is really about comparative model testing. Hypothesis tests on individual coefficients do not allow us to compare models. Our theories imply different explanations for phenomena, and thus different empirical models. To treat the theories comparatively, we must test our models comparatively as well. So tests such as the joint hypothesis test are critically important to that endeavor.\nThis test is a distributional test; that is, its product is a point or range on a known probability distribution. Thus, we can be quite exactly about the probability with which we either do or do not reject the joint null hypothesis.\nThis is not the case of another popular measure of model fit, the \\(R^2\\), which has no sampling distribution. The \\(R^2\\) cannot tell us anything probabilistic about the model or more specifically, about the hypothesis that a variable or variables do or do not significantly affect the fit of the model."
  },
  {
    "objectID": "inference24.html#dont-expect-the-null-hypothesis",
    "href": "inference24.html#dont-expect-the-null-hypothesis",
    "title": "Inference",
    "section": "Don’t Expect the Null Hypothesis",
    "text": "Don’t Expect the Null Hypothesis"
  },
  {
    "objectID": "inference24.html#dont-expect-the-null-hypothesis-1",
    "href": "inference24.html#dont-expect-the-null-hypothesis-1",
    "title": "Inference",
    "section": "Don’t Expect the Null Hypothesis",
    "text": "Don’t Expect the Null Hypothesis"
  },
  {
    "objectID": "inference24.html#measuring-uncertainty-1",
    "href": "inference24.html#measuring-uncertainty-1",
    "title": "Inference",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\n\nStandard errors are basic measures of uncertainty.\nThe standard errors of the estimates of \\(\\widehat{\\beta_k}\\) are the standard deviations of the sampling distribution of the estimator.\nStandard errors are analogous to standard deviations surrounding the estimates."
  },
  {
    "objectID": "specification24.html",
    "href": "specification24.html",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "We’re trying to explain variation. What does this look like?\n\nbest guess at variation in \\(y\\) would be its mean (constant-only model). with variables, variation is composed of two parts: \\(y = \\mu + \\epsilon\\). The first is \\(x\\beta\\), our systematic component; the second is the random component. \\(x\\) shapes the systematic part of \\(y\\).\ntotal variation in \\(y\\) is the sum of the variance of these components: \\(\\text{var}(y) = \\text{var}(x)\\beta + \\text{var}(e)\\)\nthe squared variation in \\(y\\) due to \\(x\\) is the MSS.\nthe squared variation in \\(e\\) is the RSS.\ntheir sum is the TSS.\n\n\n\nRecall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\).\n\\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\).\n\\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\).\n\n\n\n\\[TSS= MSS+RSS\\]\nand\n\\[R^2 = \\frac{MSS}{TSS}\\] or\n\\[R^2 = 1- \\frac{RSS}{TSS}\\]\n\n\n\n\\(R^2\\) increases as \\(k\\) increases. The adjusted \\(R^2\\) penalizes for additional regressors (though still increases with \\(k\\)):\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(N-1)}{(N-k-1)} \\]\nin the numerator, adjusting for \\(N\\) minus one for the constant, and in the denominator for \\(N-k-1\\).\n\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight\n\n\n\n\n\n\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\nbecause \\(R^2\\) does not have a sampling distribution, we cannot generally say an \\(R^2\\) value is large or small - relative to what? If, say, it were distributed normally, we could make claims about the probability of observing a given \\(R^2\\) value.\nit is possible to construct such a sampling distribution, but it seems noone ever does, so the value of the \\(R^2\\) is not that great.\n\n\n\n\nRoot Mean Squared Error (standard error of the estimate):\n\\[RMSE = \\sqrt{\\frac{RSS}{(N-k)}}\\]\nIs the average (squared) error. It’s useful because it’s in the same units as \\(y\\), so interpretable as the average (squared) distance of an observation from the regression line.\n\n\n\nIn the linear model, the F-test is probably the most useful tool for comparing (nested) models. Reviewing, models are nested iff:\ntheir samples are identical. the variables in one are a strict subset of those in the other.\nThe nested models are the Restricted and Unrestricted.\n\n\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or “normed” by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\).\n\n\n\n\n\n\nHow much do individual observations shape the regression estimates? This discussion draws an excellent monograph by Fox (1991).\n\nleverage - how much an observation affects the slope of the regression line - it might have great leverage, but be on the line, so not discrepant.\ndiscrepancy - how unusual an observation is.\ninfluence - how much an unusual observation affects the slope of the regression line. How much a discrepant observation exerts leverage.\n\n\n\n\n\\(h\\) is an element of the \\(hat\\) matrix:\n\\[ y = X \\beta \\] \\[= X(X'X)^{-1} X'y \\] \\[=Hy \\] where \\[H = X(X'X)^{-1} X' \\]\nThe matrix \\(H\\) is a square \\(n,n\\) matrix; the main diagonal indicates the extent to which any observation has leverage on the regression line by virtue of being large relative to the other \\(X\\)s.\nObservations where \\(h&gt; 2(k/N)\\) are often thought of as outliers with the potential to influence the regression estimates.\n\n\n\n\nA common indicator of discrepancy is to standardize residuals to see which fall outside of standard confidence bounds. This is problematic (for reasons set aside), so the replacement is Studentized residuals. About 95% of these residuals should fall between -2/+2, and those that do not may be discrepant or unusual, and worth scrutiny.\n\n\n\n\nThere are a number of measures of influence including DFBETA, DFFITS, and Cook’s Distance. In general, measures of influence evaluate the estimates progressively excluding each observation; observations that substantialliy change the estimates are more influential.\n\n\n\n\nCook’s D combines measures of discrepancy and leverage to indicate how influential an observation is:\n\\[ \\frac{s(e^2)}{k} \\cdot \\frac{h_i}{1-h_i}\\]\nwhere the first term is standardized residuals indicating discrepancy, and the second is \\(h\\), measuring leverage. If both are high, then influence is high; otherwise, influence is low.\n\n\n\n\nFox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\n\n\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal() \n\n\n\n\n\nLooking at the upper-middle of the plot, Turkey, Sudan, Russia, and Indonesia seem especially influential. On the x-axis, the UK, France, and Qatar have high leverage, so inordinately affect the regression line. On the y-axis, Turkey (several years), Italy, Mexico, and Indonesia are discrepant or unusual.\n\n\n\n\n\nWhy are observations outliers?\nWhat should we do with them?\n\n\n\n\n\n\nwhat do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?\n\n\n\n\n\nTwo most common nonlinear functions applied to variables in political science are:\n\nnatural log\npolynomial\n\n\n\nPermit nonlinear but monotonic1 changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\n\n\n\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)\n\n\n\n\nUsing data we’ve seen before, let’s look at the effect of GDP per capita on infant mortality rate. We’ll look at the effect of GDP per capita on infant mortality rate using a linear model, a log-linear model, and a polynomial model, compared below.\n\n\ncode\ncensor &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/data/censorcy.csv\", header=TRUE)\ncensor &lt;- subset(censor, select=-c(lngdp))\n\nggplot(data=censor, aes(x=gdppc, y=IMR)) +\n   geom_point(data=censor, aes(x=gdppc, y=IMR) ,color=\"green\") + \n   geom_text_repel(label=censor$ctry, size=3) +\n  stat_poly_line(data=censor, formula = y ~ x, se=FALSE, color=\"black\") +\n  stat_poly_line(data=censor, formula = y ~ log(x), se=FALSE, linetype=\"dashed\", color=\"red\" )   +\n  stat_poly_line(data=censor, formula = y ~ poly(x,2), se=FALSE, linetype=\"dashed\" )   +\n  labs(x=\"Polity\", y=\"Predicted Infant Mortality Rate\")+\n  theme_minimal()\n\n\n\n\n\nYou can see the linear model is a poor fit. The other two are better fits to the observed data (blue line is polynomial, red line is logarithmic.)\n\n\n\n\n\nadding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)\n\n\n\n\nWe know perfect collinearity is not much of a problem since we can’t estimate \\(\\widehat{\\beta}\\) at all in its presence - the matrix \\((X'X)^{-1}\\) is singular and thus can’t be inverted.\nWhat about correlation among \\(X\\) variables short of perfect collinearity?\n\n\nRecall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE.\n\n\n\n\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)\n\n\n\n\n\n\nData is never missing randomly.\nThe process generating missing data is systematic.\n\n\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\).\n\n\n\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates.\n\n\n\n\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data.\n\n\n\n\n\n\n\n\n\n\n\nFox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#model-fit",
    "href": "specification24.html#model-fit",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "We’re trying to explain variation. What does this look like?\n\nbest guess at variation in \\(y\\) would be its mean (constant-only model). with variables, variation is composed of two parts: \\(y = \\mu + \\epsilon\\). The first is \\(x\\beta\\), our systematic component; the second is the random component. \\(x\\) shapes the systematic part of \\(y\\).\ntotal variation in \\(y\\) is the sum of the variance of these components: \\(\\text{var}(y) = \\text{var}(x)\\beta + \\text{var}(e)\\)\nthe squared variation in \\(y\\) due to \\(x\\) is the MSS.\nthe squared variation in \\(e\\) is the RSS.\ntheir sum is the TSS.\n\n\n\nRecall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\).\n\\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\).\n\\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\)."
  },
  {
    "objectID": "specification24.html#model-fit-1",
    "href": "specification24.html#model-fit-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Recall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\). \\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\). \\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\)."
  },
  {
    "objectID": "specification24.html#model-fit-r2",
    "href": "specification24.html#model-fit-r2",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\[TSS= MSS+RSS\\]\nand\n\\[R^2 = \\frac{MSS}{TSS}\\] or\n\\[R^2 = 1- \\frac{RSS}{TSS}\\]"
  },
  {
    "objectID": "specification24.html#fit-r2_adj",
    "href": "specification24.html#fit-r2_adj",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\(R^2\\) increases as \\(k\\) increases. The adjusted \\(R^2\\) penalizes for additional regressors (though still increases with \\(k\\)):\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(N-1)}{(N-k-1)} \\]\nin the numerator, adjusting for \\(N\\) minus one for the constant, and in the denominator for \\(N-k-1\\).\n\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight\n\n\n\n\n\n\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\nbecause \\(R^2\\) does not have a sampling distribution, we cannot generally say an \\(R^2\\) value is large or small - relative to what? If, say, it were distributed normally, we could make claims about the probability of observing a given \\(R^2\\) value.\nit is possible to construct such a sampling distribution, but it seems noone ever does, so the value of the \\(R^2\\) is not that great."
  },
  {
    "objectID": "specification24.html#fit-r2_adj-1",
    "href": "specification24.html#fit-r2_adj-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight"
  },
  {
    "objectID": "specification24.html#fit-rmse",
    "href": "specification24.html#fit-rmse",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Root Mean Squared Error (standard error of the estimate):\n\\[RMSE = \\sqrt{\\frac{RSS}{(N-k)}}\\]\nIs the average (squared) error. It’s useful because it’s in the same units as \\(y\\), so interpretable as the average (squared) distance of an observation from the regression line."
  },
  {
    "objectID": "specification24.html#fit-f-test",
    "href": "specification24.html#fit-f-test",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "In the linear model, the F-test is probably the most useful tool for comparing (nested) models. Reviewing, models are nested iff:\ntheir samples are identical. the variables in one are a strict subset of those in the other.\nThe nested models are the Restricted and Unrestricted.\n\n\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or “normed” by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\)."
  },
  {
    "objectID": "specification24.html#fit-f-test-1",
    "href": "specification24.html#fit-f-test-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "How do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or ``normed’’ by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\)."
  },
  {
    "objectID": "specification24.html#overview",
    "href": "specification24.html#overview",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "In the early 1990s, there was a significant (and very boring) debate over these measures of model fit. The useful parts focused on what we, as a discipline, are trying to accomplish in modeling. There was no real resolution. \\~\\\nThat same period was the early part of both a technological and methods revolution. Computing power democratized and increased exponentially. This enabled what some have called the MLE revolution. Whatever the case, debates over OLS fit diminished in relevance and frequency."
  },
  {
    "objectID": "specification24.html#outliers",
    "href": "specification24.html#outliers",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "How much do individual observations shape the regression estimates? This discussion draws an excellent monograph by Fox (1991).\n\nleverage - how much an observation affects the slope of the regression line - it might have great leverage, but be on the line, so not discrepant.\ndiscrepancy - how unusual an observation is.\ninfluence - how much an unusual observation affects the slope of the regression line. How much a discrepant observation exerts leverage.\n\n\n\n\n\\(h\\) is an element of the \\(hat\\) matrix:\n\\[ y = X \\beta \\] \\[= X(X'X)^{-1} X'y \\] \\[=Hy \\] where \\[H = X(X'X)^{-1} X' \\]\nThe matrix \\(H\\) is a square \\(n,n\\) matrix; the main diagonal indicates the extent to which any observation has leverage on the regression line by virtue of being large relative to the other \\(X\\)s.\nObservations where \\(h&gt; 2(k/N)\\) are often thought of as outliers with the potential to influence the regression estimates.\n\n\n\n\nA common indicator of discrepancy is to standardize residuals to see which fall outside of standard confidence bounds. This is problematic (for reasons set aside), so the replacement is Studentized residuals. About 95% of these residuals should fall between -2/+2, and those that do not may be discrepant or unusual, and worth scrutiny.\n\n\n\n\nThere are a number of measures of influence including DFBETA, DFFITS, and Cook’s Distance. In general, measures of influence evaluate the estimates progressively excluding each observation; observations that substantialliy change the estimates are more influential.\n\n\n\n\nCook’s D combines measures of discrepancy and leverage to indicate how influential an observation is:\n\\[ \\frac{s(e^2)}{k} \\cdot \\frac{h_i}{1-h_i}\\]\nwhere the first term is standardized residuals indicating discrepancy, and the second is \\(h\\), measuring leverage. If both are high, then influence is high; otherwise, influence is low.\n\n\n\n\nFox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\n\n\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal() \n\n\n\n\n\nLooking at the upper-middle of the plot, Turkey, Sudan, Russia, and Indonesia seem especially influential. On the x-axis, the UK, France, and Qatar have high leverage, so inordinately affect the regression line. On the y-axis, Turkey (several years), Italy, Mexico, and Indonesia are discrepant or unusual.\n\n\n\n\n\nWhy are observations outliers?\nWhat should we do with them?\n\n\n\n\n\n\nwhat do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?"
  },
  {
    "objectID": "specification24.html#outliers-1",
    "href": "specification24.html#outliers-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Fox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\n\n\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal()"
  },
  {
    "objectID": "specification24.html#thinking-about-outliers",
    "href": "specification24.html#thinking-about-outliers",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Why are observations outliers?\nWhat should we do with them?"
  },
  {
    "objectID": "specification24.html#model-outliers",
    "href": "specification24.html#model-outliers",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "what do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?"
  },
  {
    "objectID": "specification24.html#functional-form",
    "href": "specification24.html#functional-form",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Two most common nonlinear functions applied to variables in political science are:\n\nnatural log\npolynomial\n\n\n\nPermit nonlinear but monotonic1 changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\n\n\n\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)\n\n\n\n\nUsing data we’ve seen before, let’s look at the effect of GDP per capita on infant mortality rate. We’ll look at the effect of GDP per capita on infant mortality rate using a linear model, a log-linear model, and a polynomial model, compared below.\n\n\ncode\ncensor &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/data/censorcy.csv\", header=TRUE)\ncensor &lt;- subset(censor, select=-c(lngdp))\n\nggplot(data=censor, aes(x=gdppc, y=IMR)) +\n   geom_point(data=censor, aes(x=gdppc, y=IMR) ,color=\"green\") + \n   geom_text_repel(label=censor$ctry, size=3) +\n  stat_poly_line(data=censor, formula = y ~ x, se=FALSE, color=\"black\") +\n  stat_poly_line(data=censor, formula = y ~ log(x), se=FALSE, linetype=\"dashed\", color=\"red\" )   +\n  stat_poly_line(data=censor, formula = y ~ poly(x,2), se=FALSE, linetype=\"dashed\" )   +\n  labs(x=\"Polity\", y=\"Predicted Infant Mortality Rate\")+\n  theme_minimal()\n\n\n\n\n\nYou can see the linear model is a poor fit. The other two are better fits to the observed data (blue line is polynomial, red line is logarithmic.)"
  },
  {
    "objectID": "specification24.html#log-models",
    "href": "specification24.html#log-models",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)"
  },
  {
    "objectID": "specification24.html#polynomials",
    "href": "specification24.html#polynomials",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24.html#scaling-redux",
    "href": "specification24.html#scaling-redux",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "adding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24.html#collinearity",
    "href": "specification24.html#collinearity",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "We know perfect collinearity is not much of a problem since we can’t estimate \\(\\widehat{\\beta}\\) at all in its presence - the matrix \\((X'X)^{-1}\\) is singular and thus can’t be inverted.\nWhat about correlation among \\(X\\) variables short of perfect collinearity?\n\n\nRecall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE.\n\n\n\n\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24.html#collinearity-1",
    "href": "specification24.html#collinearity-1",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Recall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE."
  },
  {
    "objectID": "specification24.html#collinearity-is-a-data-problem",
    "href": "specification24.html#collinearity-is-a-data-problem",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "if the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24.html#missing-data",
    "href": "specification24.html#missing-data",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Data is never missing randomly.\nThe process generating missing data is systematic.\n\n\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\).\n\n\n\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates.\n\n\n\n\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24.html#missing-data-signifies-sample-problems",
    "href": "specification24.html#missing-data-signifies-sample-problems",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "If data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\)."
  },
  {
    "objectID": "specification24.html#dealing-with-missingness",
    "href": "specification24.html#dealing-with-missingness",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "This is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates."
  },
  {
    "objectID": "specification24.html#missingness",
    "href": "specification24.html#missingness",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "All this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24.html#dissertations",
    "href": "specification24.html#dissertations",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Fox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#fit-r2_adj-2",
    "href": "specification24.html#fit-r2_adj-2",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Because \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\ninference on \\(R^2_{adj}\\) is possible assuming it is a sample statistic estimating a population \\(R^2_{adj}\\). There are significant limits to what we can tell using this though."
  },
  {
    "objectID": "specification24s.html#model-fit",
    "href": "specification24s.html#model-fit",
    "title": "Model Specification and Fit",
    "section": "Model Fit",
    "text": "Model Fit\nWe’re trying to explain variation. What does this look like?\n\nbest guess at variation in \\(y\\) would be its mean (constant-only model). with variables, variation is composed of two parts: \\(y = \\mu + \\epsilon\\). The first is \\(x\\beta\\), our systematic component; the second is the random component. \\(x\\) shapes the systematic part of \\(y\\).\ntotal variation in \\(y\\) is the sum of the variance of these components: \\(\\text{var}(y) = \\text{var}(x)\\beta + \\text{var}(e)\\)\nthe squared variation in \\(y\\) due to \\(x\\) is the MSS.\nthe squared variation in \\(e\\) is the RSS.\ntheir sum is the TSS."
  },
  {
    "objectID": "specification24s.html#model-fit-1",
    "href": "specification24s.html#model-fit-1",
    "title": "Model Specification and Fit",
    "section": "Model fit",
    "text": "Model fit\nRecall\n\\(TSS = \\sum\\limits_{i=1}^{N} (y-\\bar{y})^2\\) :: Total Sum of Squares - the variation in \\(y\\). \\(MSS = \\sum\\limits_{i=1}^{N} (\\widehat{y} -\\bar{y})^2\\) :: Model or Explained Sum of Squares - variation in \\(\\widehat{y}\\). \\(SSE = \\sum\\limits_{i=1}^{N} (y-\\widehat{y} )^2\\) :: Residual Sum of Squares - \\(e'e\\) - variation in \\(e\\)."
  },
  {
    "objectID": "specification24s.html#model-fit-r2",
    "href": "specification24s.html#model-fit-r2",
    "title": "Model Specification and Fit",
    "section": "Model Fit – \\(R^2\\)",
    "text": "Model Fit – \\(R^2\\)\n\\[TSS= MSS+RSS\\]\nand\n\\[R^2 = \\frac{MSS}{TSS}\\] or\n\\[R^2 = 1- \\frac{RSS}{TSS}\\]"
  },
  {
    "objectID": "specification24s.html#fit-r2_adj",
    "href": "specification24s.html#fit-r2_adj",
    "title": "Model Specification and Fit",
    "section": "Fit – \\(R^2_{adj}\\)",
    "text": "Fit – \\(R^2_{adj}\\)\n\\(R^2\\) increases as \\(k\\) increases. The adjusted \\(R^2\\) penalizes for additional regressors (though still increases with \\(k\\)):\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(N-1)}{(N-k-1)} \\]\nin the numerator, adjusting for \\(N\\) minus one for the constant, and in the denominator for \\(N-k-1\\).\n\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight\n\n\n\n\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\nbecause \\(R^2\\) does not have a sampling distribution, we cannot generally say an \\(R^2\\) value is large or small - relative to what? If, say, it were distributed normally, we could make claims about the probability of observing a given \\(R^2\\) value.\nit is possible to construct such a sampling distribution, but it seems noone ever does, so the value of the \\(R^2\\) is not that great."
  },
  {
    "objectID": "specification24s.html#fit-r2_adj-1",
    "href": "specification24s.html#fit-r2_adj-1",
    "title": "Model Specification and Fit",
    "section": "Fit – \\(R^2_{adj}\\)",
    "text": "Fit – \\(R^2_{adj}\\)\n\\(R^2\\) describes the density of the cloud of points around the regression line. Data where \\(y\\) has greater variability or where \\(e\\) has greater variability will produce regressions with lower \\(R^2\\) values.\n\n\ncode\nx &lt;- rnorm(100)\ne &lt;- rnorm(100, 0, (2))\ny &lt;- -1+ 12*x+ e\ne2 &lt;- rnorm(100, 0, sqrt(140))\ny2 = -1+ 12*x+ e2\nm1 &lt;- lm(y~x)\nm2 &lt;- lm(y2~x)\n# summary(m1)$r.squared\n# summary(m2)$r.squared\n\ntight &lt;- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+\n  labs(title=\"Tight fit\", x=\"x\", y=\"y\")\n\nloose &lt;- ggplot(data.frame(x=x, y=y2), aes(x=x, y=y2)) + geom_point() +  stat_poly_line(se=FALSE) +\n  stat_poly_eq(use_label(c(\"R2\"))) +\n  geom_point()+ scale_y_continuous(limits=c(-25,25)) + \n  labs(title=\"Loose fit\", x=\"x\", y=\"y\")\n\nloose + tight"
  },
  {
    "objectID": "specification24s.html#fit-r2_adj-2",
    "href": "specification24s.html#fit-r2_adj-2",
    "title": "Model Specification and Fit",
    "section": "Fit – \\(R^2_{adj}\\)",
    "text": "Fit – \\(R^2_{adj}\\)\nBecause \\(R^2\\) describes the density of the cloud of points around the regression line, in an odd way, it rewards less variant \\(y\\) variables. \\(R^2\\) does the following:\n\nit tells us how much of the variation in \\(y\\) is explained by the systematic component, but only for the sample.\nit permits nested model comparison.\ninference on \\(R^2_{adj}\\) is possible assuming it is a sample statistic estimating a population \\(R^2_{adj}\\). There are significant limits to what we can tell using this though."
  },
  {
    "objectID": "specification24s.html#fit-rmse",
    "href": "specification24s.html#fit-rmse",
    "title": "Model Specification and Fit",
    "section": "Fit – RMSE",
    "text": "Fit – RMSE\nRoot Mean Squared Error (standard error of the estimate):\n\\[RMSE = \\sqrt{\\frac{RSS}{(N-k)}}\\]\nIs the average (squared) error. It’s useful because it’s in the same units as \\(y\\), so interpretable as the average (squared) distance of an observation from the regression line."
  },
  {
    "objectID": "specification24s.html#fit-f-test",
    "href": "specification24s.html#fit-f-test",
    "title": "Model Specification and Fit",
    "section": "Fit – F-test",
    "text": "Fit – F-test\nIn the linear model, the F-test is probably the most useful tool for comparing (nested) models. Reviewing, models are nested iff:\ntheir samples are identical. the variables in one are a strict subset of those in the other.\nThe nested models are the Restricted and Unrestricted."
  },
  {
    "objectID": "specification24s.html#fit-f-test-1",
    "href": "specification24s.html#fit-f-test-1",
    "title": "Model Specification and Fit",
    "section": "Fit – F-test",
    "text": "Fit – F-test\nHow do we test \\(RSS_U = RSS_R = 0\\)?\n\\[F=\\frac{(RSS_R-RSS_{U})/q}{RSS_{U}/(n-k-1)}\\]\nwhere \\(q\\) is the number of restrictions.\nThe numerator is the difference in fit between the two models (weighted or ``normed’’ by the different number of parameters, the restrictions), and the denominator is a baseline of how well the full model fits. So this is a ratio of improvement to the fit of the full model. Both are distributed \\(\\chi^2\\); their ratio is distributed \\(F\\)."
  },
  {
    "objectID": "specification24s.html#outliers",
    "href": "specification24s.html#outliers",
    "title": "Model Specification and Fit",
    "section": "Outliers",
    "text": "Outliers\nHow much do individual observations shape the regression estimates? This discussion draws an excellent monograph by Fox (1991).\n\nleverage - how much an observation affects the slope of the regression line - it might have great leverage, but be on the line, so not discrepant.\ndiscrepancy - how unusual an observation is.\ninfluence - how much an unusual observation affects the slope of the regression line. How much a discrepant observation exerts leverage."
  },
  {
    "objectID": "specification24s.html#outliers-1",
    "href": "specification24s.html#outliers-1",
    "title": "Model Specification and Fit",
    "section": "Outliers",
    "text": "Outliers\nFox suggests plotting \\(h\\) against Studentized Residuals - those high on both may be influential outliers. Let’s look at potential outliers from a model using the ITT data.\nOutliers - Ill-treatment and Torture Data}\n\n\ncode\nitt &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/exercises/ex4/ITT/data/ITT.csv\")\nitt$p1 &lt;- itt$polity2+11\nitt$p2 &lt;- itt$p1^2\nitt$p3 &lt;- itt$p1^3\n\n\nitt &lt;- \n  itt%&gt;% \n  group_by(ccode) %&gt;%\n  mutate( lagprotest= lag(protest), lagRA=lag(RstrctAccess), n=1)\n\n\nm1 &lt;- lm(scarring ~ lagRA + civilwar + lagprotest + p1+p2+p3 +wdi_gdpc, data=itt)\n#plot(m1, which=5, labels.id=itt$ctryname)\n\nitt$used &lt;- TRUE\nitt$used[na.action(m1)] &lt;- FALSE\nittesample &lt;- itt %&gt;%  filter(used==\"TRUE\")\n\nsr &lt;- studres(m1)\nh &lt;- hatvalues(m1)\ninfluence &lt;- cooks.distance(m1)\noutliers &lt;- data.frame(ctryname=ittesample$ctryname, year=ittesample$year, sr=sr, h=h, influence=influence)\n\nggplot(data=outliers, aes(x=h, y=sr), label=ctryname) +\n  geom_point(aes(size=influence), alpha=.2, show.legend = F) +\n  geom_text_repel(aes(label=ifelse(influence&gt;0.007742674|h&gt;.04, paste(outliers$ctryname,outliers$year), \"\"), force=2, size=.1, alpha=.4), show.legend = F) +\n  labs(x=\"Leverage (h)\", y=\"Discrepency (Studentized Residuals)\", size=\"Influence (Cook's D)\") +\ntheme_minimal()"
  },
  {
    "objectID": "specification24s.html#thinking-about-outliers",
    "href": "specification24s.html#thinking-about-outliers",
    "title": "Model Specification and Fit",
    "section": "Thinking about outliers",
    "text": "Thinking about outliers\n\nWhy are observations outliers?\nWhat should we do with them?"
  },
  {
    "objectID": "specification24s.html#model-outliers",
    "href": "specification24s.html#model-outliers",
    "title": "Model Specification and Fit",
    "section": "Model outliers",
    "text": "Model outliers\n\nwhat do the outliers have in common?\nwhat would predict them? Is there a missing variable that would “soak up” the unexplained variance of those observations?\nthe model does not explain outliers well - what can we learn from the model’s “success” in explaining other cases?"
  },
  {
    "objectID": "specification24s.html#functional-form",
    "href": "specification24s.html#functional-form",
    "title": "Model Specification and Fit",
    "section": "Functional Form",
    "text": "Functional Form\nTwo most common nonlinear functions applied to variables in political science are:\n\nnatural log\npolynomial\n\nNatural Logs\nPermit nonlinear but monotonic1 changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\nPolynomials\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)\n\nExample\nUsing data we’ve seen before, let’s look at the effect of GDP per capita on infant mortality rate. We’ll look at the effect of GDP per capita on infant mortality rate using a linear model, a log-linear model, and a polynomial model, compared below.\n\n\ncode\ncensor &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/data/censorcy.csv\", header=TRUE)\ncensor &lt;- subset(censor, select=-c(lngdp))\n\nggplot(data=censor, aes(x=gdppc, y=IMR)) +\n   geom_point(data=censor, aes(x=gdppc, y=IMR) ,color=\"green\") + \n   geom_text_repel(label=censor$ctry, size=3) +\n  stat_poly_line(data=censor, formula = y ~ x, se=FALSE, color=\"black\") +\n  stat_poly_line(data=censor, formula = y ~ log(x), se=FALSE, linetype=\"dashed\", color=\"red\" )   +\n  stat_poly_line(data=censor, formula = y ~ poly(x,2), se=FALSE, linetype=\"dashed\" )   +\n  labs(x=\"Polity\", y=\"Predicted Infant Mortality Rate\")+\n  theme_minimal()\n\n\n\n\n\nYou can see the linear model is a poor fit. The other two are better fits to the observed data (blue line is polynomial, red line is logarithmic.)\nA function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once."
  },
  {
    "objectID": "specification24s.html#log-models",
    "href": "specification24s.html#log-models",
    "title": "Model Specification and Fit",
    "section": "Log models",
    "text": "Log models\nModel interpretations using natural logs of variables (from Wooldridge, p. 44)"
  },
  {
    "objectID": "specification24s.html#polynomials",
    "href": "specification24s.html#polynomials",
    "title": "Model Specification and Fit",
    "section": "Polynomials",
    "text": "Polynomials\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24s.html#scaling-redux",
    "href": "specification24s.html#scaling-redux",
    "title": "Model Specification and Fit",
    "section": "Scaling redux",
    "text": "Scaling redux\n\nadding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24s.html#collinearity",
    "href": "specification24s.html#collinearity",
    "title": "Model Specification and Fit",
    "section": "Collinearity",
    "text": "Collinearity\nWe know perfect collinearity is not much of a problem since we can’t estimate \\(\\widehat{\\beta}\\) at all in its presence - the matrix \\((X'X)^{-1}\\) is singular and thus can’t be inverted.\nWhat about correlation among \\(X\\) variables short of perfect collinearity?\nCorrelation among \\(X\\)s\nRecall what the OLS simulations show:\n\nthe estimates are unbiased.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each.\nthe estimates are less precise than they otherwise would be.\nthe model is still BLUE.\n\nCollinearity is a data problem\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24s.html#collinearity-1",
    "href": "specification24s.html#collinearity-1",
    "title": "Model Specification and Fit",
    "section": "Collinearity",
    "text": "Collinearity\n\nAll else equal, the model is still BLUE. The estimates are unbiased.\nthe estimates are less precise than they otherwise would be.\nthe standard errors are inflated because of our uncertainty.\nwe’re asking the model to estimate 2 unknowns from very little independent information about each."
  },
  {
    "objectID": "specification24s.html#collinearity-is-a-data-problem",
    "href": "specification24s.html#collinearity-is-a-data-problem",
    "title": "Model Specification and Fit",
    "section": "Collinearity is a data problem",
    "text": "Collinearity is a data problem\n\nif the model is correctly specified, don’t remove one of the correlated variables - if you do, you’ve created omitted variable bias, and now the model is no longer BLUE. Theory is the key here - it’s the only way to arbitrate what should or shouldn’t be in the model.\ncollect more data or better data.\ntransform the collinear variables:\n\ndifference variables (but with consequences)\ncombine variables as an index or using a measurement model like factor analysis or IRT (but with consequences)"
  },
  {
    "objectID": "specification24s.html#missing-data",
    "href": "specification24s.html#missing-data",
    "title": "Model Specification and Fit",
    "section": "Missing Data",
    "text": "Missing Data\nData is never missing randomly.\nThe process generating missing data is systematic.\nMissing data signifies sample problems\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have \\(\\widehat{\\gamma}\\) effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\).\nDealing with Missingness\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates.\n\nMissingness\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24s.html#missing-data-signifies-sample-problems",
    "href": "specification24s.html#missing-data-signifies-sample-problems",
    "title": "Model Specification and Fit",
    "section": "Missing data signifies sample problems",
    "text": "Missing data signifies sample problems\nIf data are missing systematically, we can write a missing data function:\n\\[Pr(Missing_i) = F(Z_i \\widehat{\\gamma} + \\varepsilon_i)\\]\nSome variables \\(Z\\) have $ $ effects on the probability any particular case has missing data - those effects map onto the probability space via some cumulative distribution function \\(F\\)."
  },
  {
    "objectID": "specification24s.html#dealing-with-missingness",
    "href": "specification24s.html#dealing-with-missingness",
    "title": "Model Specification and Fit",
    "section": "Dealing with Missingness",
    "text": "Dealing with Missingness\nThis is not generally a model we’d run, but it’s a useful thought exercise. When data are missing, social scientists often turn to imputation methods.\n\nsingle imputation - replace missing value with the (panel) mean/median/mode of the variable; replace with a randomly chosen value of \\(x\\); replace it with an estimate of \\(x\\) from a model predicting \\(x\\).\nmultiple imputation - a form of simulation. Draw from a distribution of the missing data, to generate multiple data sets. Estimate the model in each data set, and use the distribution of estimates and uncertainty to generate estimates."
  },
  {
    "objectID": "specification24s.html#missingness",
    "href": "specification24s.html#missingness",
    "title": "Model Specification and Fit",
    "section": "Missingness",
    "text": "Missingness\nAll this said, it’s always important to rely on theory to understand both why observations might be missing, and how those missing observations might influence estimates. The best solution is often to collect better data."
  },
  {
    "objectID": "specification24s.html#model-specification-1",
    "href": "specification24s.html#model-specification-1",
    "title": "Model Specification and Fit",
    "section": "Model Specification",
    "text": "Model Specification"
  },
  {
    "objectID": "specification24s.html#dissertations",
    "href": "specification24s.html#dissertations",
    "title": "Model Specification and Fit",
    "section": "Dissertations",
    "text": "Dissertations"
  },
  {
    "objectID": "specification24s.html#references",
    "href": "specification24s.html#references",
    "title": "Model Specification and Fit",
    "section": "References",
    "text": "References\n\n\nFox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#references",
    "href": "specification24.html#references",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Fox, John. 1991. Regression Diagnostics. Quantitative Applications in the Social Sciences. SAGE Publications.\n\n\nWooldridge, Jeffrey M. 2013. Introductory Econometrics. South-Western/Cengage."
  },
  {
    "objectID": "specification24.html#scaling-variables",
    "href": "specification24.html#scaling-variables",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "adding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24.html#natural-logs",
    "href": "specification24.html#natural-logs",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "Permit nonlinear but monotonic changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)"
  },
  {
    "objectID": "specification24s.html#scaling-variables",
    "href": "specification24s.html#scaling-variables",
    "title": "Model Specification and Fit",
    "section": "Scaling Variables",
    "text": "Scaling Variables\n\nadding a constant, \\(k\\) to either \\(X\\) or \\(Y\\) will shift the location of the regression line, but will not change its shape/slope.\nadding a constant to \\(Y\\) shifts the intercept by \\(k\\); \\(\\widehat{\\beta_0}+k\\) - the standard error is unchanged.\nadding a constant to \\(X\\) shifts the intercept by \\(\\widehat{\\beta_0}-k\\widehat{\\beta_1}\\) - the standard error will change too.\nmultiplying either \\(X\\) or \\(Y\\) by some constant will distort the axis and affect the coefficient estimate accordingly.\nmultiplying \\(X\\) by \\(k\\) changes the slope to \\(\\widehat{\\beta_1}/k\\); intercept is unchanged.\nmultiplying \\(Y\\) by \\(k\\) shifts the intercept to \\(\\widehat{\\beta_0}k\\), and the slope to \\(\\widehat{\\beta_1}k\\)"
  },
  {
    "objectID": "specification24s.html#a-comment-on-dissertations",
    "href": "specification24s.html#a-comment-on-dissertations",
    "title": "Model Specification and Fit",
    "section": "A comment on dissertations",
    "text": "A comment on dissertations"
  },
  {
    "objectID": "specification24.html#x-montonicity",
    "href": "specification24.html#x-montonicity",
    "title": "Model Specification and Fit",
    "section": "",
    "text": "A function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once.\n\n\nPermit nonlinear but :monotonic changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\n\n\n\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24s.html#x-montonicity",
    "href": "specification24s.html#x-montonicity",
    "title": "Model Specification and Fit",
    "section": ":x montonicity",
    "text": ":x montonicity\nA function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once.\nNatural Logs\nPermit nonlinear but :monotonic changes in \\(y\\) over changes in \\(x\\). Model interpretations using natural logs of variables (from Wooldridge (2013) p. 44)\n\nPolynomials\n\\[y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3\\]\n\nPermits nonmonotonic changes in \\(y\\) given changes in \\(x\\)\nRaises the question of whether \\(x\\) exerts these higher order effects (joint hypothesis tests)\nThe marginal effect of a change in \\(x\\) now involves two estimates; \\(\\beta_1+2\\beta_2x\\)\nThe inflection point (minimum or maximum) is \\(\\frac{\\beta_1}{|2\\beta_2|}\\)"
  },
  {
    "objectID": "specification24.html#footnotes",
    "href": "specification24.html#footnotes",
    "title": "Model Specification and Fit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA function is monotonic if it is either entirely non-increasing or non-decreasing. A horizontal line only passes through a monotonic function once. A function if non-monotonic if a horizontal line passes through it more than once.↩︎"
  }
]