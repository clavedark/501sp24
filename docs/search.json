[
  {
    "objectID": "probability24.html",
    "href": "probability24.html",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns.\n\n\n\n\nProbability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value).\n\n\n\n\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita.\n\n\n\n\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely.\n\n\n\n\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war.\n\n\n\n\nThese four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information.\n\n\n\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24.html#why-probability-distributions",
    "href": "probability24.html#why-probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns."
  },
  {
    "objectID": "probability24.html#probability-distributions",
    "href": "probability24.html#probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Probability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?"
  },
  {
    "objectID": "probability24.html#things-we-want-to-know-about-x",
    "href": "probability24.html#things-we-want-to-know-about-x",
    "title": "Basic probability",
    "section": "",
    "text": "\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value)."
  },
  {
    "objectID": "probability24.html#types-of-variables",
    "href": "probability24.html#types-of-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Variables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "probability24.html#discrete-variables",
    "href": "probability24.html#discrete-variables",
    "title": "Basic probability",
    "section": "",
    "text": "May be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "probability24.html#continuous-variables",
    "href": "probability24.html#continuous-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Can be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "probability24.html#levels-of-measurement",
    "href": "probability24.html#levels-of-measurement",
    "title": "Basic probability",
    "section": "",
    "text": "These four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "probability24.html#levels-of-measurement-and-models",
    "href": "probability24.html#levels-of-measurement-and-models",
    "title": "Basic probability",
    "section": "",
    "text": "In general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24.html#pdf-and-cdf",
    "href": "probability24.html#pdf-and-cdf",
    "title": "Basic probability",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nProbability distributions can be described by\n\nProbability Density Function (PDF) which maps \\(X\\) onto the probability space describing the probabilities of every value of \\(X\\), \\(x\\).\nCumulative Distribution Function (CDF) which maps \\(X\\) onto the probability space describing the probability \\(X\\) is less than some value, \\(x\\)."
  },
  {
    "objectID": "probability24.html#pdf-density",
    "href": "probability24.html#pdf-density",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nDefinition\n\n\n\nThe Probability Density Function or PDF describes the probability a random variable takes on a particular value, and it does so for all values of that random variable."
  },
  {
    "objectID": "probability24.html#pdf-density-1",
    "href": "probability24.html#pdf-density-1",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nExample\n\n\n\nSuppose we toss a fair coin 1 time and want to describe the probability distribution of the outcome, \\(Y\\) which is a random variable. The possible outcomes are heads and tails, and the probability of each is .5. This is the PDF because it describes the probabilities associated with all possible outcomes of \\(Y\\)."
  },
  {
    "objectID": "probability24.html#pdf-density-2",
    "href": "probability24.html#pdf-density-2",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\nWhile establishing the probability of a value of a discrete variable is possible, establishing the probability of a particular value of a continuous variable is not.\nInstead, the continuous PDF describes the instantaneous rate of change in \\(Pr(X=x)\\) for every value of \\(x\\)."
  },
  {
    "objectID": "probability24.html#pdf-plots",
    "href": "probability24.html#pdf-plots",
    "title": "Basic probability",
    "section": "PDF plots",
    "text": "PDF plots"
  },
  {
    "objectID": "probability24.html#cdf",
    "href": "probability24.html#cdf",
    "title": "Basic probability",
    "section": "CDF",
    "text": "CDF\n\n\n\n\n\n\nDefinition\n\n\n\nFor a random variable, \\(Y\\), the probability \\(Y\\) is less than or equal to some particular value, \\(y\\) in the range of \\(Y\\) defines the cumulative density function.\nFor a continuous variable:\n\\[P(Y \\leq j) =\\int\\limits_{-\\infty}^{j} f(X)dX\\]\nFor a discrete variable:\n\\[P(Y \\leq j) = \\sum\\limits_{y\\leq j} P(Y=y)\n= 1- \\sum\\limits_{y&gt; j} P(Y=y)\\]"
  },
  {
    "objectID": "probability24.html#cdf-plots",
    "href": "probability24.html#cdf-plots",
    "title": "Basic probability",
    "section": "CDF plots",
    "text": "CDF plots"
  },
  {
    "objectID": "probability24.html#notation",
    "href": "probability24.html#notation",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\\(f(x)\\) denotes the PDF of \\(x\\).\n\\(F(x)\\) denotes the CDF of \\(x\\).\nSubstitute either the appropriate name, symbol, or function for \\(F\\) or \\(f\\) to indicate the distribution.\nLower case Greek letters denote PDFs: e.g. \\(f(x)= \\phi(x)\\) denotes the normal PDF.\nUpper case Greek letters denote CDFs: e.g. \\(F(x)=\\Phi(x)\\) denotes the normal CDF."
  },
  {
    "objectID": "probability24.html#notation-1",
    "href": "probability24.html#notation-1",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nExample\n\n\n\n\\(x\\) is distributed Normal, \\(N(\\mu, \\sigma^{2})\\):\n\\[F(x) = \\Phi_{\\mu, \\sigma^{2}}(x)  =    \\int  \\phi_{\\mu, \\sigma^{2}} (x) f(x)d(x) \\]\n\\[ f(x) = \\phi_{\\mu, \\sigma^{2}}(x) =\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{(x-\\mu)^{2}}{2 \\sigma^{2}} \\right) \\]\nNote the first derivative of the CDF is the PDF; the integral of the PDF is the CDF."
  },
  {
    "objectID": "probability24.html#bernoulli",
    "href": "probability24.html#bernoulli",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nSuppose a binary variable \\(x\\) that takes on only the values of zero and one (\\(x \\in {0, 1}\\)):\n\\[\nPr(X=1)=\\pi\\nonumber \\\\\nPr(x=0)= 1-Pr(x=1) \\\\\n= 1-\\pi   \n\\]"
  },
  {
    "objectID": "probability24.html#bernoulli-1",
    "href": "probability24.html#bernoulli-1",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe PDF is:\n\\[ f(x) = \\\\\n          \\pi    ~~~~~~~~ ~ ~ ~~~~ \\text{if } x=1\\\\\n         1-\\pi   ~ ~ ~ ~~~~\\text{if } x=0\n     \\]\nor\n\\[f(x) = \\pi^{x}(1-\\pi)^{1-x} \\]"
  },
  {
    "objectID": "probability24.html#bernoulli-2",
    "href": "probability24.html#bernoulli-2",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe CDF is:\n\\[F(x) =\\sum_{x} f(x) \\]\nand the expected value is\n\\[\nE(x) =\\sum_{x}x f(x)    \\\\\n= (1)(\\pi)+(0)(1-\\pi)   \\\\\n= \\pi\n\\]"
  },
  {
    "objectID": "probability24.html#binomial",
    "href": "probability24.html#binomial",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nBernoulli is important because it is the foundation for a lot of other distributions, including the binomial distribution. The binomial describes the success probability function (where \\(x=1\\) is a “success”) from a set of \\(n\\) independent Bernoulli trials. So, the binomial is the probability of successes (“ones”) in \\(n\\) independent Bernoulli trials with identical probabilities, \\(\\pi\\).\n\\(~\\)\nThere are two essential parts to the binomial PDF - the probability of success, and the number of ways a success can occur."
  },
  {
    "objectID": "probability24.html#binomial-1",
    "href": "probability24.html#binomial-1",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe probability of success is the Bernoulli probability:\n\\[\nf(x) = \\pi^{x}(1-\\pi)^{1-x}\n\\]\nand the number of ways success can occur (called “n-tuples”) is\n\\[  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} = \\frac{n!}{x!(n-x)!}\n\\]\nNotice the notation for the n-tuple."
  },
  {
    "objectID": "probability24.html#binomial-2",
    "href": "probability24.html#binomial-2",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe PDF combines these:\n\\[\nf(x)=\n  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} \\pi^{x}(1-\\pi)^{n-x}\n\\]\nThere are \\(n\\) trials, \\(x\\) is the number of successes, and \\(n-x\\) is the number of failures. Each event in the n-tuple arises with the Bernoulli probability."
  },
  {
    "objectID": "probability24.html#binomial-3",
    "href": "probability24.html#binomial-3",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are going to toss a fair coin 4 times and want to know how many ways we can have heads come up twice.\n\\[\n\\frac{4!}{2!(4-2)!} = 6\n\\] Now, suppose we want to know the probability exactly 2 of those 4 tosses is heads.\n\\[\nPr(x=2) =\\frac{4!}{2!(4-2)!} (.5)^{2}(.5)^{2} \\\\\n= 6(0.0625) \\\\\n= 0.375\n\\]"
  },
  {
    "objectID": "probability24.html#binomial-4",
    "href": "probability24.html#binomial-4",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nHere’s another example: suppose we flip 5 fair coins once each; what is the PDF of the number of heads we expect? In other words, what is the probability associated with each possible number of successes (heads), from 0 to 5?\n\\[\nP(x=0)  =  \\begin{pmatrix}\n    5  \\\\\n    0\n  \\end{pmatrix} \\cdot  (.5)^{0} \\cdot (.5) ^{5} = 1 \\cdot 1 \\cdot 0.03125=0.03125\\\\\n  \\] \\[\nP(x=1) =  \\begin{pmatrix}\n    5  \\\\\n    1\n  \\end{pmatrix} \\cdot (.5)^{1}\\cdot (.5)^{4} = 5 \\cdot .5 \\cdot 0.0625=0.15625 \\\\\n  \\] \\[P(x=2) =  \\begin{pmatrix}\n    5  \\\\\n    2\n  \\end{pmatrix} \\cdot (.5)^{2}\\cdot (.5)^{3} = 10 \\cdot .25 \\cdot 0.125=0.3125 \\\\\n\\] \\[\n\\vdots \\vdots \\vdots\n\\]"
  },
  {
    "objectID": "probability24.html#binomial-family-distributions",
    "href": "probability24.html#binomial-family-distributions",
    "title": "Basic probability",
    "section": "Binomial family distributions",
    "text": "Binomial family distributions\n\nthe geometric distribution describes repeated Bernoulli trials with probability of success \\(\\pi\\), until the first success.\nthe negative binomial describes the number of Bernoulli failures prior to the first success - it can be thought of as a counting process up to the first success.\nthe poisson describes Bernoulli trials where \\(\\pi\\) for any particular trial is very small."
  },
  {
    "objectID": "probability24.html#the-normal-distribution",
    "href": "probability24.html#the-normal-distribution",
    "title": "Basic probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe Normal distribution is the most widely used distribution in the social sciences.\n\nThe normal seems to “fit” a lot of the variables social scientists measure and use in models.\nEstimation techniques we often employ assume the disturbance term is normally distributed; one result is the coefficients are either normally distributed or t-distributed."
  },
  {
    "objectID": "probability24.html#normal-pdf",
    "href": "probability24.html#normal-pdf",
    "title": "Basic probability",
    "section": "Normal PDF",
    "text": "Normal PDF\nThe normal PDF: \\[\nPr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]\n\\]\nwhere two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\) describe the location and shape of the distribution, the mean and variance respectively; we indicate a normally distributed variable and its parameters as\n\\[\nY \\sim \\text{Normal}(\\mu,\\sigma^{2}) \\nonumber\n\\]\n\n\nDescribe these dataNormal?\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "probability24.html#standard-normal",
    "href": "probability24.html#standard-normal",
    "title": "Basic probability",
    "section": "Standard Normal",
    "text": "Standard Normal\nA useful special case of the Normal is the standard normal, \\(z \\sim \\text{Normal}(0,1)\\). The PDF for the standard normal is given by\n\\[\n\\phi(z)=\\frac{1}{\\sqrt{2 \\pi }} exp \\left[\\frac{-z^{2}}{2}\\right] \\nonumber\n\\]\nwhere the parameters themselves drop out since we’d be subtracting a mean of zero and dividing/multiplying by a variance of 1. The standard normal CDF is denoted \\(\\Phi(z)\\); the standard normal PDF is denoted \\(\\phi(z)\\)."
  },
  {
    "objectID": "probability24.html#normal-pdfs-different-moments",
    "href": "probability24.html#normal-pdfs-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs different moments",
    "text": "Normal PDFs different moments"
  },
  {
    "objectID": "probability24.html#models",
    "href": "probability24.html#models",
    "title": "Basic probability",
    "section": "Models",
    "text": "Models\n\nProbability models include unobserved disturbances; we make assumptions about the distributions of those errors, \\(\\epsilon\\).\nWhat we assume about the unobservables is always informed by what we know about the observables, mainly \\(y\\).\nA useful way to describe or summarize \\(y\\) is to characterize its observed distribution.\nThe distribution of \\(y\\) informs our assumption about the distribution of \\(\\epsilon\\)."
  },
  {
    "objectID": "probability24.html#why-this-matters-to-ols",
    "href": "probability24.html#why-this-matters-to-ols",
    "title": "Basic probability",
    "section": "Why this matters to OLS",
    "text": "Why this matters to OLS\nLinear regression is such an inferential model; we have a number of sources of uncertainty. We represent that uncertainty in the model via the disturbance term, \\(\\epsilon\\).\n\\(~\\)\nIn order to know things about \\(\\epsilon\\) and other parts of the model, we assume that the disturbances are normally distributed; \\(y\\) should be normal too."
  },
  {
    "objectID": "probability24.html#normality-and-centrality",
    "href": "probability24.html#normality-and-centrality",
    "title": "Basic probability",
    "section": "Normality and Centrality",
    "text": "Normality and Centrality\nWe rely on stuff being normally distributed, and central tendency being meaningful. The Central Limit Theorem facilitates this."
  },
  {
    "objectID": "probability24.html#central-limit-theorem",
    "href": "probability24.html#central-limit-theorem",
    "title": "Basic probability",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ {\\sum\\limits_{n \\rightarrow \\infty} (\\bar{X_i})} / {n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "probability24.html#clt---why-is-this-valuable",
    "href": "probability24.html#clt---why-is-this-valuable",
    "title": "Basic probability",
    "section": "CLT - Why is this valuable?",
    "text": "CLT - Why is this valuable?\n\nIf we assume repeated sampling and/or infinitely large samples, we can assume normality.\nWe know the properties of the Normal intimately well. We can use this knowledge to evaluate where some value of \\(x\\) lies on the Normal CDF, what the probability less than that value is - we can figure out what the probability of observing that value is (on the PDF).\nAs we’ll see, \\(\\widehat{\\beta}\\) is normally distributed in the OLS model (it is in MLE as well). The CLT and what we know about normality facilitate inference."
  },
  {
    "objectID": "probability24.html#inference",
    "href": "probability24.html#inference",
    "title": "Basic probability",
    "section": "Inference",
    "text": "Inference\nInference is our effort to measure and characterize our uncertainty about the model and its parameters.\n\nuncertainty is the most important thing we estimate in inferential models.\ncharacterizing uncertainty relies on probability theory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PLSC 501 - Spring 2024",
    "section": "",
    "text": "This is the course website for PLSC 501 - it is a data science course focused on data management, coding in R, and learning basic regression techniques in OLS and MLE.\n\nSyllabus\nSlides\nCode"
  },
  {
    "objectID": "matrix24.html",
    "href": "matrix24.html",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model.\n\n\n\n\nA single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A.\n\n\n\nThe dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar.\n\n\n\n\nA is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]\n\n\n\n\n\n\nA rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3).\n\n\n\nA vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24.html#matrix-notation",
    "href": "matrix24.html#matrix-notation",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model."
  },
  {
    "objectID": "matrix24.html#matrices",
    "href": "matrix24.html#matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A."
  },
  {
    "objectID": "matrix24.html#dimensions",
    "href": "matrix24.html#dimensions",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "The dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar."
  },
  {
    "objectID": "matrix24.html#symmetric-matrices",
    "href": "matrix24.html#symmetric-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#rectangular-matrices",
    "href": "matrix24.html#rectangular-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3)."
  },
  {
    "objectID": "matrix24.html#vectors",
    "href": "matrix24.html#vectors",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24.html#transposition",
    "href": "matrix24.html#transposition",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nThe transpose of a matrix A is the matrix flipped on its side such that its rows become columns and columns become rows; the transpose of A is denoted \\({\\mathbf A'}\\) and is referred to as “\\({\\mathbf A}\\) transpose.”\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right]\n~~~~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5 & 2\\\\\n\\end{array} \\right] \\]\nso \\({\\mathbf X}\\), a (3,2) matrix, becomes \\({\\mathbf X'}\\), a (2,3) matrix."
  },
  {
    "objectID": "matrix24.html#transposition-1",
    "href": "matrix24.html#transposition-1",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nIf \\({\\mathbf A}\\) is symmetric, then \\({\\mathbf A}={\\mathbf A'}\\) – take a look at the following symmetric matrix and you’ll see why:\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#transposition-2",
    "href": "matrix24.html#transposition-2",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nAlso note that the transpose of a transposed matrix results in the original matrix: \\((\\mathbf{X}')'=\\mathbf{X}\\).\nTransposing a row vector results in a column vector and vice versa:\n\\[\\mathbf{x} =  \\left[\n\\begin{array}{c}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n\\end{array} \\right]\n~~~~~~~~~\n\\mathbf{x'} =  \\left[\n\\begin{array}{cccc}\n-1 & -9 &4 &16\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#trace-of-a-matrix",
    "href": "matrix24.html#trace-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nThe trace of a matrix is the sum of the diagonal elements of a square matrix, and is denoted \\(tr(\\mathbf{A})=a_{11}+a_{22}+a_{33}\\ldots+a_{nn} = \\sum\\limits_{i=1}^{n}a_{ii}\\)\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nsuch that\n\\(tr(\\mathbf{A})=2+5+2=9\\)"
  },
  {
    "objectID": "matrix24.html#addition-subtraction",
    "href": "matrix24.html#addition-subtraction",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\n\nAddition and subtraction of matrices is analogous to the same scalar operations, but depend on the conformatibility of the matrices to be added or subtracted.\nTwo matrices are conformable for addition or subtraction iff they are of the same order.\nNote that conformability means something different for addition/subtraction than for multiplication."
  },
  {
    "objectID": "matrix24.html#addition-subtraction-1",
    "href": "matrix24.html#addition-subtraction-1",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider the following (2,2) matrices and the problem \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{C}\\):\n\\[ \\left[\n\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21}& a_{22} \\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{cc}\nb_{11} & b_{12}\\\\\nb_{21}& b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21}& a_{22}+b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\nc_{11} & c_{12}\\\\\nc_{21}& c_{22} \\\\\n\\end{array} \\right] \\]\nAddition and subtraction are only possible among matrices that share the same order; otherwise, they are nonconformable."
  },
  {
    "objectID": "matrix24.html#addition-subtraction-2",
    "href": "matrix24.html#addition-subtraction-2",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider a second example of addition, and note that subtraction would follow directly:\n\\[ \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{ccc}\n1 &5 &7\\\\\n3& 7& 1\\\\\n2& 4& 9\\\\\n\\end{array} \\right]\n=\n  \\left[\n\\begin{array}{ccc}\n3 &9 &15\\\\\n7& 12& 4\\\\\n10& 7& 11\\\\\n\\end{array} \\right] \\]\nMatrix addition adheres to the commutative and associative properties such that:\n\\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) (Commutative), and \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}= \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) (Associative)."
  },
  {
    "objectID": "matrix24.html#multiplication",
    "href": "matrix24.html#multiplication",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\nLet’s begin by multiplying a scalar and a matrix - the product is a matrix whose elements are the scalar multiplied by each element of the original matrix, so:\n\\[ \\beta\n\\left[\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\n\\beta x_{11} & \\beta x_{12}\\\\\n\\beta x_{21}&  \\beta x_{22} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24.html#multiplication-1",
    "href": "matrix24.html#multiplication-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\n\nIn order to multiply two matrices (or vectors), they must be conformable for multiplication.\nTwo matrices are conformable for multiplication iff the number of columns in the first matrix is equal to the number of rows in the second matrix. That is, \\(\\mathbf{A_{i,j}}\\) and \\(\\mathbf{B_{j,k}}\\).\nAn easy way to think of this is to write the dimensions of the two matrices, (i,j),(j,k) - the inner dimensions are the same, so the matrix is conformable for multiplication - moreover, the outer dimensions (i,k) give the dimension of the product matrix."
  },
  {
    "objectID": "matrix24.html#multiplication---inner-product",
    "href": "matrix24.html#multiplication---inner-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\n\nTo illustrate multiplication, let’s start with a column vector, \\(\\mathbf{e}\\) whose order is (N,1) and take its inner product.\nThe inner product of a column vector is the transpose of the column vector (thus, a row vector) post-multiplied by the column vector, so \\({\\mathbf e'\\mathbf e}\\). The inner product is a scalar."
  },
  {
    "objectID": "matrix24.html#multiplication---inner-product-1",
    "href": "matrix24.html#multiplication---inner-product-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\nWhen we transpose the column vector \\({\\mathbf e}\\) of (N,1) order, we get a row vector \\({\\mathbf e'}\\) of (1,N) order. Inner dimensions match, so they are conformable.\n\\[\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n~~~~~\n\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right] \\] \\[=\n(-1 \\cdot -1)+(-9 \\cdot -9)+(4 \\cdot 4)+ (16 \\cdot 16)+ (-10 \\cdot -10) = 454\\]"
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing",
    "href": "matrix24.html#least-squares-foreshadowing",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nNote the inner product of a column vector is a scalar; the inner product of \\(\\mathbf{e}\\), is the sum of the squares of \\(\\mathbf{e}\\).\n\\[\\mathbf{e'e}= e_{1}e_{1}+e_{2}e_{2}+\\ldots e_{N}e_{N} = \\sum\\limits_{i=1}^{N}e_{i}^{2}\\]"
  },
  {
    "objectID": "matrix24.html#multiplication---outer-product",
    "href": "matrix24.html#multiplication---outer-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - outer product",
    "text": "Multiplication - outer product\nThe outer product of a column vector is the transpose of the column vector (thus, a row vector) pre-multiplied by the column vector, so \\({\\mathbf e\\mathbf e'}\\). The outer product is an (N,N) matrix.\n\\[\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right]\n\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n\\]\nLet’s multiply \\({\\mathbf e\\mathbf e'}\\), a column vector of (5,1) by a row vector of (1,5); we’ll obtain a square matrix of (5,5)."
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing-1",
    "href": "matrix24.html#least-squares-foreshadowing-1",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nLet \\(\\mathbf{e}\\) represent the residuals or errors in our regression. The outer product\n\\[{\\mathbf e\\mathbf e'} =  \\left[\n\\begin{array}{rrrrr}\n1 & 9 &-4 &-16 &10\\\\\n9 & 81 &-36 &-144 &90\\\\\n-4 & -36 &16 &64 &-40\\\\\n-16 & -144 &64 &256 &-160\\\\\n10 & 90 &-40 &-160 &100\\\\\n\\end{array} \\right] \\]\nis the variance-covariance matrix of \\(\\mathbf{e}\\). The squares on the main diagonal (which sums to the sum of squares) and the symmetry of the off-diagonal elements."
  },
  {
    "objectID": "matrix24.html#inverting-matrices-1",
    "href": "matrix24.html#inverting-matrices-1",
    "title": "Matrix algebra basics",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\n\nYou’ll have noticed that division has been conspicuously absent so far. In scalar algebra, we sometimes represent division in fractions, \\(\\frac{1}{2}\\).\nAnother way to represent the same quantity is \\(2^{-1}\\), or the inverse of 2. Of course, in scalar algebra, a number multiplied by its inverse equals 1, e.g., \\(2 \\cdot 2^{-1}=1\\) or \\(2 \\cdot \\frac{1}{2}=1\\). So, if we are given \\(4x=1\\) and want to solve for \\(x\\) what we really want to know is “what is the inverse of 4 such that \\(4 \\cdot 4^{-1}=1\\)?” We consider matrices in a similar manner."
  },
  {
    "objectID": "matrix24.html#inverting-square-matrices",
    "href": "matrix24.html#inverting-square-matrices",
    "title": "Matrix algebra basics",
    "section": "Inverting Square Matrices",
    "text": "Inverting Square Matrices\nImagine a square matrix, \\(\\mathbf{X}\\) and its inverse, denoted \\({\\mathbf{X^{-1}}}\\) (read as “X inverse”). Analogous to scalar algebra, a matrix multiplied by its inverse is equal to the identity matrix, \\(\\mathbf{I}\\).\nSo in order to find \\({\\mathbf{X^{-1}}}\\), we must find the matrix that, when multiplied by \\(\\mathbf{X}\\), produces \\(\\mathbf{I}\\). Thus, for a square matrix, its inverse (if it exists) is such that\n\\[\\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X^{-1}} \\mathbf{X}=\\mathbf{I}\\]\nNotice the commutative property at work here - this is because \\(\\mathbf{X}\\) is square, such that \\(n=m\\), so \\(\\mathbf{X}\\) and \\({\\mathbf{X^{-1}}}\\) have the same dimensions."
  },
  {
    "objectID": "matrix24.html#determinant",
    "href": "matrix24.html#determinant",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nAn important characteristic of square matrices is the determinant. The determinant, denoted \\(|X|\\), is a scalar; every square matrix has one. We evaluate the determinant by examining the cross-products of the matrice’s elements. This is simple in the 2x2 matrix, less so in larger matrices.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right| = x_{11}x_{22}-x_{12}x_{21}\\]"
  },
  {
    "objectID": "matrix24.html#determinant-1",
    "href": "matrix24.html#determinant-1",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nIn the 2x2 matrix, the determinant is the cross-product of the main diagonals minus the cross-product of the off-diagonals.\n\\[ |\\mathbf{X_{1}}|=\\left|\n\\begin{array}{cc}\n2 & 4\\\\\n6& 3 \\\\\n\\end{array} \\right| = 2 \\cdot 3-4 \\cdot 6 = -18\n\\]\nThe determinant is -18; \\(|\\mathbf{X_{1}}|=-18\\)."
  },
  {
    "objectID": "matrix24.html#determinant-2",
    "href": "matrix24.html#determinant-2",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nThis matrix has a determinant of zero - this is an important case.\n\\[ |\\mathbf{X_{2}}|=\\left|\n\\begin{array}{cc}\n3 & 6\\\\\n2& 4 \\\\\n\\end{array} \\right| = 3 \\cdot 4-6 \\cdot 2 =0\n\\] A matrix with determinant zero is singular. A singluar matrix has no inverse."
  },
  {
    "objectID": "matrix24.html#singular-matrices",
    "href": "matrix24.html#singular-matrices",
    "title": "Matrix algebra basics",
    "section": "Singular matrices",
    "text": "Singular matrices\nThere are some important conditions that will produce a determinant of zero and thus a singular matrix:\n\nIf all the elements of any row or column of a matrix are equal to zero, then the determinant is zero.\nIf two rows or columns of a matrix are identical, the determinant is zero.\nIf a row or column of a matrix is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24.html#least-squares-foreshadowing-2",
    "href": "matrix24.html#least-squares-foreshadowing-2",
    "title": "Matrix algebra basics",
    "section": "Least Squares Foreshadowing",
    "text": "Least Squares Foreshadowing\nFor the linear model in least squares, this is important because computing estimates of \\(\\beta\\) requires inverting a matrix. Let’s derive \\(\\beta\\) in matrix notation to see how:\nThe estimated model is:\n\\[\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}}\\] Minimizing \\(\\widehat{\\mathbf{\\epsilon}}'\\widehat{\\mathbf{\\epsilon}}\\) (skipping the math for now), we get\n\\[(\\mathbf{X'X}) \\widehat{\\beta}=\\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares",
    "href": "matrix24.html#foreshadowing-least-squares",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThe matrix \\((\\mathbf{X'X})\\) gives the sums of squares (on the main diagonal) and the sums of cross products (in the off-diagonals) of all the \\(\\mathbf{X}\\) variables; the matrix is symmetric. Since \\(\\beta\\) is the unknown vector in this equation, solve for \\(\\beta\\) by dividing both sides by \\((\\mathbf{X'X})\\) - in matrix terms, we premultiply each side by \\((\\mathbf{X'X)^{-1}}\\):\n\\[(\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) \\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares-1",
    "href": "matrix24.html#foreshadowing-least-squares-1",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nAs we know, \\((\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) = \\mathbf{I}\\). So,\n\\[\\mathbf{I}\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\] or \\[\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]\nEstimating \\(\\widehat{\\beta}\\) requires inverting \\(\\mathbf{(X'X)}\\) If the matrix \\(\\mathbf{(X'X)}\\) is singular, then \\(\\mathbf{(X'X)^{-1}}\\) does not exist, and we cannot estimate \\(\\widehat{\\beta}\\). Least Squares fails."
  },
  {
    "objectID": "matrix24.html#foreshadowing-least-squares-2",
    "href": "matrix24.html#foreshadowing-least-squares-2",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThinking specifically of the \\(\\mathbf{X}\\) variables in the model, any of these conditions produce perfect collinearity - estimation fails because the matrix can’t invert.\n\nIf all the elements of any row or column of \\(\\mathbf{X}\\) are equal to zero, then the determinant is zero.\nIf two rows or columns of \\(\\mathbf{X}\\) are identical, the determinant is zero.\nIf a row or column of \\(\\mathbf{X}\\) is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24.html#examples",
    "href": "matrix24.html#examples",
    "title": "Matrix algebra basics",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2Example 3\n\n\n\\({\\mathbf X}\\) below has a row of zeros; compute the determinant using the difference of cross-products.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n0 & 0\\\\\n19& 12 \\\\\n\\end{array} \\right| = 0 \\cdot 12-0 \\cdot 19 = 0 \\]\nYou can see in \\({\\mathbf X}\\) that because all the cross-products involve multiplying by zero, the determinant will always be zero; this applies to larger matrices as well.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n5 & 7\\\\\n5& 7 \\\\\n\\end{array} \\right| = 5 \\cdot 7-5 \\cdot 7 =0 \\]\nIn \\({\\mathbf X}\\) it’s easy to see that the cross-products will always be equal to one another if the rows or columns are identical, and the difference between identical values is zero.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n64 & 48\\\\\n16& 12 \\\\\n\\end{array} \\right| = 64 \\cdot 12-48 \\cdot 16 = 768-768=0\\]\nFinally \\({\\mathbf X}\\) shows that if one row or column is a linear combination of other rows or columns, the determinant will be zero; in that matrix, the top row is 4 times the bottom row."
  },
  {
    "objectID": "matrix24.html#inversion",
    "href": "matrix24.html#inversion",
    "title": "Matrix algebra basics",
    "section": "Inversion",
    "text": "Inversion\nThe matrix we need to invert, \\(\\mathbf{X'X}\\) is rectangular, so inversion is more complex. I’m going to illustrate the method of cofactor expansion - the purpose here is to give you an idea what software does every time you estimate an OLS model."
  },
  {
    "objectID": "matrix24.html#minor-of-a-matrix",
    "href": "matrix24.html#minor-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Minor of a matrix",
    "text": "Minor of a matrix\nThe minor of a matrix is the determinant of a submatrix created by deleting the \\(i\\)th row and the \\(j\\)th column of the full matrix, where the minor is denoted by the element where the deleted rows intersect.\n\\[ \\mathbf{A} =  \\left[\n\\begin{array}{ccc}\na_{11} & a_{12} &a_{13}\\\\\na_{21}& a_{22} &a_{23}\\\\\na_{31} &a_{32} & a_{33}\\\\\n\\end{array} \\right] \\] \\[\\text{so the minor of } a_{11} \\text{ is }\n|\\mathbf{M_{11}}| =  \\left|\n\\begin{array}{cc}\na_{22} &a_{23}\\\\\na_{32} & a_{33}\\\\\n\\end{array} \\right|  =  a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\]"
  },
  {
    "objectID": "matrix24.html#cofactor-matrix",
    "href": "matrix24.html#cofactor-matrix",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nThe cofactor of an element (\\(c_{ij}\\)) is the minor with a positive or negative sign depending on whether \\(i+j\\) is odd (negative) or even (positive). This is given by:\n\\[\\theta_{ij}=(-1)^{i+j}|\\mathbf{M_{ij}}|\\]\nso, from the example finding the minor of \\(a_{11}\\), \\(i\\)=1 and \\(j=1\\); their sum is 2 which is even, so the cofactor is \\(+a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\). If we find the cofactor for every element, \\(a_{ij}\\) in a matrix and replace each element with its cofactor, the new matrix is called the cofactor matrix of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24.html#cofactor-matrix-1",
    "href": "matrix24.html#cofactor-matrix-1",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nWhat we have so far is the signed matrix of minors:\n\\[\\mathbf{A} = \\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#cofactor-expansion",
    "href": "matrix24.html#cofactor-expansion",
    "title": "Matrix algebra basics",
    "section": "Cofactor Expansion",
    "text": "Cofactor Expansion\nFind the signed determinant of each 2x2 submatrix.\n\\[\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} = \\left[\n\\begin{array}{rrr}\n10&-10&0\\\\\n-12&0&6\\\\\n-2&5&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#find-the-determinant",
    "href": "matrix24.html#find-the-determinant",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nUsing the cofactor matrix, \\(\\mathbf{\\Theta_{A}}\\) we perform cofactor expansion in order to find the determinant, \\(|\\mathbf{A}|\\). Cofactor expansion is given by:\n\\[|\\mathbf{A}|=\\sum\\limits_{i,j=1}^{n}a_{ij}\\Theta_{ij}\\]\nwhich means that we take two corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) and sum the products of their elements - this gives us the determinant of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24.html#find-the-determinant-1",
    "href": "matrix24.html#find-the-determinant-1",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nSo taking the first row from \\(\\mathbf{A}\\) (1,4,2) and from \\(\\mathbf{\\Theta_{A}}\\) (10,-10,0) above, we compute\n\\[a_{11} \\cdot \\Theta_{A11}+a_{12} \\cdot \\Theta_{A12}+a_{13} \\cdot \\Theta_{A13}\\]\n\\[1 \\cdot 10+ 4 \\cdot -10 + 2 \\cdot 0 = -30\\] This is the determinant of \\(\\mathbf{A}\\) , \\(|\\mathbf{A}|\\). Note that any corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) will produce the same result."
  },
  {
    "objectID": "matrix24.html#adjoint-matrix",
    "href": "matrix24.html#adjoint-matrix",
    "title": "Matrix algebra basics",
    "section": "Adjoint Matrix",
    "text": "Adjoint Matrix\nIf we transpose the cofactor matrix, (cof \\(\\mathbf{A'}\\)), the new matrix is called the adjoint matrix, denoted adj\\(\\mathbf{A}\\)=(cof \\(\\mathbf{A'}\\)).\n\\[adj\\mathbf{A} = \\mathbf{\\Theta_{A}'} = \\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#inverting-the-matrix",
    "href": "matrix24.html#inverting-the-matrix",
    "title": "Matrix algebra basics",
    "section": "Inverting the matrix",
    "text": "Inverting the matrix\nBelieve it or not, this all leads us to inverting the matrix, provided it is nonsingular.\n\\[\\mathbf{A^{-1}}=\\frac{1}{|\\mathbf{A}|}(adj {\\mathbf A})\\] The inverse of A is equal to one over the determinant of A times the adjoint matrix of A. So we’re pre-multiplying \\(adj {\\mathbf A}\\) by the (scalar) determinant, -30."
  },
  {
    "objectID": "matrix24.html#inverse-of-matrix-a",
    "href": "matrix24.html#inverse-of-matrix-a",
    "title": "Matrix algebra basics",
    "section": "Inverse of Matrix A",
    "text": "Inverse of Matrix A\n\\[{\\mathbf A^{-1}}=-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n=\\left[\n\\begin{array}{rrr}\n-\\frac{1}{3}&\\frac{2}{5}&\\frac{1}{15}\\\\\n\\frac{1}{3}&0&-\\frac{1}{6}\\\\\n0&-\\frac{1}{5}&\\frac{3}{10}\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#checking-our-work",
    "href": "matrix24.html#checking-our-work",
    "title": "Matrix algebra basics",
    "section": "Checking our work",
    "text": "Checking our work\nWe can check our work to be sure we’ve inverted correctly by making sure that \\(\\mathbf{A^{-1}A}=\\mathbf{I_{3}}\\); so,\n\\[-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n\\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n=\n-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n-30&0&0\\\\\n0&-30&0\\\\\n0&0&-30\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols",
    "href": "matrix24.html#connecting-data-matrices-to-ols",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\nLet’s examine these matrices a little to get a feel for what the computation of \\(\\widehat{\\beta}\\) involves: \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\n\\[\n\\mathbf{X}= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \n\\end{matrix}  \\right]\n\\] The dimensions of \\(\\mathbf{X}\\) are \\(n x k\\); the sample size by the number of regressors (including the constant)."
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols-1",
    "href": "matrix24.html#connecting-data-matrices-to-ols-1",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \n\\end{matrix}  \\right] \\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "matrix24.html#connecting-data-matrices-to-ols-2",
    "href": "matrix24.html#connecting-data-matrices-to-ols-2",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i}\n\\end{matrix}  \\right] \\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationships among \\(X_k\\)."
  },
  {
    "objectID": "overview24.html",
    "href": "overview24.html",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise\n\n\n\n\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding.\n\n\n\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling.\n\n\n\n\nIs not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal.\n\n\n\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here.\n\n\n\n\n\nis a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together.\n\n\n\n\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra\n\n\n\n\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\).\n\n\n\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?\n\n\n\n\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term.\n\n\n\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later.\n\n\n\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24.html#all-models-are-wrong-ldots",
    "href": "overview24.html#all-models-are-wrong-ldots",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise"
  },
  {
    "objectID": "overview24.html#the-regression-model",
    "href": "overview24.html#the-regression-model",
    "title": "Course overview",
    "section": "",
    "text": "This course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding."
  },
  {
    "objectID": "overview24.html#course-goals",
    "href": "overview24.html#course-goals",
    "title": "Course overview",
    "section": "",
    "text": "In May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling."
  },
  {
    "objectID": "overview24.html#this-class",
    "href": "overview24.html#this-class",
    "title": "Course overview",
    "section": "",
    "text": "Is not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal."
  },
  {
    "objectID": "overview24.html#this-class-1",
    "href": "overview24.html#this-class-1",
    "title": "Course overview",
    "section": "",
    "text": "is aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here."
  },
  {
    "objectID": "overview24.html#this-class-2",
    "href": "overview24.html#this-class-2",
    "title": "Course overview",
    "section": "",
    "text": "is a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together."
  },
  {
    "objectID": "overview24.html#what-do-you-need-to-know-for-501",
    "href": "overview24.html#what-do-you-need-to-know-for-501",
    "title": "Course overview",
    "section": "",
    "text": "What do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra"
  },
  {
    "objectID": "overview24.html#matrix-algebra",
    "href": "overview24.html#matrix-algebra",
    "title": "Course overview",
    "section": "",
    "text": "You should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\)."
  },
  {
    "objectID": "overview24.html#probability-theory",
    "href": "overview24.html#probability-theory",
    "title": "Course overview",
    "section": "",
    "text": "what are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?"
  },
  {
    "objectID": "overview24.html#understanding-regression-models",
    "href": "overview24.html#understanding-regression-models",
    "title": "Course overview",
    "section": "",
    "text": "The class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term."
  },
  {
    "objectID": "overview24.html#regression-models",
    "href": "overview24.html#regression-models",
    "title": "Course overview",
    "section": "",
    "text": "Posit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later."
  },
  {
    "objectID": "overview24.html#regression-models-1",
    "href": "overview24.html#regression-models-1",
    "title": "Course overview",
    "section": "",
    "text": "Can be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24.html#models-to-understand-politics",
    "href": "overview24.html#models-to-understand-politics",
    "title": "Course overview",
    "section": "Models to understand politics",
    "text": "Models to understand politics\n\\(~\\)\nThis a methods class, but the methods are only meaningful to us insofar as they help us understand politics. So let’s motivate the class with a question about politics."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Web formatSlide format\n\n\nCourse overview\nMatrix algebra basics\nProbability basics\nThinking about data\nBivariate model\n\n\nCourse overview\nMatrix algebra basics\nProbability basics\nBivariate model"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression."
  },
  {
    "objectID": "overview24.html#careful-thinking",
    "href": "overview24.html#careful-thinking",
    "title": "Course overview",
    "section": "Careful thinking",
    "text": "Careful thinking\nCorrelation does not imply causation.\n\n\n\nRepression during the COVID-19 Pandemic\nAmong the products of the pandemic is an opportunity for governments, under the guise of protecting public health, to repress citizens.\nThere is some evidence of democratic backsliding prior to the pandemic, but the global public health crisis gave governments a specific and universal opportunity to take advantage of this softened ground and to become more authoritarian.\nWe’re going to examine the covariates of violence against civilians during the COVID period, focusing on how states of emergency create changes in government violence.\n\n\n\nData\nLet’s look at ACLED event data, and data on states of emergency during the COVID period from Lundgren et al (2020).\nThe next figure (which should look familiar), plots protests and repression since mid-2019, the onset of the pandemic marked at March 15, 2020.\nThe blue lines are local regressions showing trends; while protests return to pre-pandemic levels (similar to the years prior), repression increases since the onset of COVID-19.\n\n\n\nProtests\n\n\ncode\nacled&lt;-read.csv(\"/Users/dave/Documents/2013/PITF/analysis/protests2021/data/2022analysis/acled2022.csv\")\n\n##Event_date variable has a \"Chr\" format. \n#Need to convert it into date format to calculate 7 day averages for Protests and Violence against Civilians\n\nacled$event_date&lt;-as.Date(acled$event_date,format=\"%d %B %Y\")\n#class(acled$event_date)\n\n##Compute number of events per day\n\nacled$perday&lt;-1\n\nprotests&lt;-filter(acled,event_type==\"Protests\" )\nstateviol&lt;-filter(acled, event_type==\"Violence against civilians\")\nprotests&lt;-aggregate(perday ~ event_date, data = protests, sum)\nstateviol&lt;-aggregate(perday ~ event_date, data = stateviol, sum)\n\n\n##Calculating a 7 day moving average\nprotests$rollmean&lt;-rollmean(protests$perday,k=7, fill=NA)\nstateviol$rollmean&lt;-rollmean(stateviol$perday,k=7, fill=NA)\n\nprotests &lt;- protests %&gt;% filter(protests$event_date &lt; \"2022-06-25\")\nstateviol &lt;- stateviol %&gt;% filter(stateviol$event_date &lt; \"2022-06-25\")\n\nprotestplot &lt;- ggplot(data=protests, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Protests\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; 'ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Protests During the Pandemic\") \n   \n\nprotestplot\n\n\n\n\n\n\n\n\n\n\n\n\nRepression\n\n\ncode\nrepressionplot &lt;- ggplot(data=stateviol, aes(x=event_date, y=rollmean)) +\n  geom_line()+\n  geom_smooth() +\n  geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n  labs(x=\"Date\", y=\"Violence against civilians\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; ACLED data retrieved 7.18.22, http://acleddata.com\") + \n  ggtitle(\"Violence Against Civilians During the Pandemic\") \n  \nrepressionplot\n\n\n\n\n\n\n\n\n\n\n\n\nStates of Emergency during the Pandemic\n\n\ncode\n####################\n#analysis data\n####################\n\n#make analysis data frame\nanalysisdata &lt;- left_join(perday, soe, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"event_date\"))\nanalysisdata &lt;- left_join(analysisdata, covid, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"date\"))\n\n#zeros for NA in soe_s\nanalysisdata &lt;- analysisdata %&gt;% mutate(across(soe_s, ~replace_na(., 0)))\n\n#fill soe series after onset\nanalysisdata &lt;- analysisdata %&gt;% group_by(ccode) %&gt;% mutate(cumSOE = cumsum(soe_s)) %&gt;% ungroup\n\n#plot cumulative states of emergency over time\ntotalsoe &lt;- analysisdata %&gt;% group_by(event_date) %&gt;%\n  summarise(across(cumSOE, sum)) %&gt;%\n  ungroup() \n\ntotalsoe$event_date &lt;- as.Date(totalsoe$event_date)\n\ntotalsoe &lt;- totalsoe %&gt;% filter(event_date &gt;\"2020-01-01\"& event_date &lt;\"2020-07-01\")\n\nggplot() +\n  geom_line(data=totalsoe, aes(x=event_date, y=cumSOE))+\n  scale_x_date(date_breaks = \"1 months\", date_labels = '%b %Y') +\n  labs ( colour = NULL, x = \"Date\", y =  \"Active States of Emergency\" )  \n\n\n\n\n\n\n\n\n\n\n\n\nModeling repression\nLet’s build a simple model predicting violence against civilians. The data here are daily, covering 167 countries between January 2020 and July 2022. We’ll predict the 7 day moving average of repression depending on whether a state of emergency is in place.\n\\(~\\)\nMy expectation is states of emergency will make repression easier to motivate and therefore, more frequent.\n\n\n\nThe model\n\\(y\\) = violence against civilians\n\\(x\\) = states of emergency\nControlling for:\n\nnew daily deaths from COVID (logged)\npercentage of the population over 65 years old\nthe Human Development Index\nthe 7 day average number of protests.\n\n\n\n\nModeling Repression and SoEs\n\n\ncode\n##################\n#models\n#################\n\nanalysisdata$soeprotests &lt;- analysisdata$cumSOE*analysisdata$protest7day\nanalysisdata$lnd =  log(analysisdata$new_deaths+.01)\n#label variables for coef plot\nlibrary(\"labelled\")\nanalysisdata &lt;- analysisdata %&gt;%\n  set_variable_labels(\n    cumSOE = \"State of Emergency\",\n    lnd = \"ln(New COVID deaths)\",\n    aged_65_older = \"% age 65 +\",\n    human_development_index = \"Human Development Index\",\n    protest7day = \"Protests, 7 day avg\",\n    soeprotests = \"SoE * Protests, 7 day avg\", \n    `Violence against civilians` = \"Violence against Civilians\"\n  )\n\n#model predicting repression during covid \n\nm1 &lt;- lm(data=analysisdata, `Violence against civilians` ~ cumSOE  + lnd + aged_65_older+human_development_index + protest7day )\n#summary(m1)  \n\nlibrary(sjPlot)\ntab_model(m1, show.se=TRUE,  p.style=\"stars\", show.ci=FALSE)\n\n\n\n\n\n\n\n\n\n\n \nViolence against\nCivilians\n\n\nPredictors\nEstimates\nstd. Error\n\n\n(Intercept)\n1.16 ***\n0.06\n\n\nState of Emergency\n0.13 ***\n0.02\n\n\nln(New COVID deaths)\n0.05 ***\n0.00\n\n\n%age 65+\n-0.03 ***\n0.00\n\n\nHuman Development Index\n-1.04 ***\n0.11\n\n\nProtests,7 day avg\n0.04 ***\n0.00\n\n\nObservations\n23625\n\n\nR2 / R2 adjusted\n0.070 / 0.070\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\n\nAnother look\n\n\ncode\n#coefficient plot\nggcoef_model(\n  m1,show_p_values = FALSE,\n  signif_stars = FALSE, exponentiate =FALSE, intercept=TRUE,\n  colour=NULL, include = c(\"cumSOE\", \"lnd\", \"aged_65_older\", \"human_development_index\", \"protest7day\")\n)+\n  xlab(\"Average Marginal Effects\") +\n  ggtitle(\"Predictors of Violence Against Civilians\") \n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe estimates indicate:\n\nif all the \\(X\\) variables are set to zero, the expected violence against civilians is 1.16 (the intercept). Can we reasonably set all the \\(X\\) variables to zero?\nthe difference between a state with a SoE and without is different from zero, but quite small; 0.13 uses of violence.\nmore deaths from COVID are associated with more violence against civilians; \\(ln(\\beta) = .05\\). Exponentiate that, and the effect on violence is 1.05, so large compared to anything else in the model.\n\n\n\n\nQuantities of interest (simulation)\nLet’s generate predictions from the model by simulating the distribution of \\(\\widehat{\\beta}\\):\n\nsample 1000 times from a multivariate normal distribution with mean \\(\\widehat{\\beta}\\)s and variances of \\(var(\\widehat{\\beta})\\).\nplug those simulated \\(\\widehat{\\beta}\\)s into the equation using mean/median/mode values for the \\(X\\)s.\n\n\n\n\nSimulating quantities\nFirst, set \\(SoE = 0\\); vary \\(covid~deaths = ln(1 \\ldots 28000)\\), \\(\\%~age~ 65 = 6\\) (the median), \\(HDI = .74\\) (the median), and \\(protests = 2\\) (the median).\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*   0 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\nthen, repeat but with \\(SoE = 1\\):\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*1 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\n\n\n\nPredicted Violence against Civilians\n\n\ncode\n#simulated quantities\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),\n      ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= \n      numeric(0), deaths = numeric(0))\n\nfor (p in seq(0,28000,100)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                       B$b5*2, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                      B$b5*2, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\npredictions$deaths &lt;- log(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"ln(New COVID Deaths)\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5.5, y = .82, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 6.5, y = .55, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nSoE states have higher levels of violence against civilians, but the difference is very small, making me think we haven’t found anything particularly interesting here.\n\nCovid deaths is associated with more violence; though violence nearly doubles over the range of deaths (x-axis), the range is huge - from 1 to 28,000. If we transform the x-axis to deaths rather than \\(ln(deaths)\\), we can see this effect is pretty isolated.\n\n\n\n\nSimulated effects (again)\n\n\ncode\npredictions$deaths &lt;- exp(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"New COVID Deaths\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5000, y = 1, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 10000, y = .65, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe increase in violence occurs almost entirely between zero and 5000 COVID deaths. We can speculate on why this might be; doing so might lead us to wonder how useful this model is.\n\\(~\\) \\(~\\)\nThat’s the primary question we’ll ask this semester."
  },
  {
    "objectID": "overview24s.html#all-models-are-wrong-ldots",
    "href": "overview24s.html#all-models-are-wrong-ldots",
    "title": "Course overview",
    "section": "All models are wrong \\(\\ldots\\)",
    "text": "All models are wrong \\(\\ldots\\)\n“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise"
  },
  {
    "objectID": "overview24s.html#the-regression-model",
    "href": "overview24s.html#the-regression-model",
    "title": "Course overview",
    "section": "The regression model",
    "text": "The regression model\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding."
  },
  {
    "objectID": "overview24s.html#course-goals",
    "href": "overview24s.html#course-goals",
    "title": "Course overview",
    "section": "Course Goals",
    "text": "Course Goals\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling."
  },
  {
    "objectID": "overview24s.html#this-class",
    "href": "overview24s.html#this-class",
    "title": "Course overview",
    "section": "This class",
    "text": "This class\nIs not a math class,\n\nbut one that uses basic math toward an applied goal."
  },
  {
    "objectID": "overview24s.html#this-class-1",
    "href": "overview24s.html#this-class-1",
    "title": "Course overview",
    "section": "This class",
    "text": "This class\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here."
  },
  {
    "objectID": "overview24s.html#this-class-2",
    "href": "overview24s.html#this-class-2",
    "title": "Course overview",
    "section": "This class",
    "text": "This class\n\nis a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together."
  },
  {
    "objectID": "overview24s.html#what-do-you-need-to-know-for-501",
    "href": "overview24s.html#what-do-you-need-to-know-for-501",
    "title": "Course overview",
    "section": "What do you need to know for 501?",
    "text": "What do you need to know for 501?\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra"
  },
  {
    "objectID": "overview24s.html#matrix-algebra",
    "href": "overview24s.html#matrix-algebra",
    "title": "Course overview",
    "section": "Matrix algebra",
    "text": "Matrix algebra\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\)."
  },
  {
    "objectID": "overview24s.html#probability-theory",
    "href": "overview24s.html#probability-theory",
    "title": "Course overview",
    "section": "Probability theory",
    "text": "Probability theory\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?"
  },
  {
    "objectID": "overview24s.html#understanding-regression-models",
    "href": "overview24s.html#understanding-regression-models",
    "title": "Course overview",
    "section": "Understanding Regression models",
    "text": "Understanding Regression models\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term."
  },
  {
    "objectID": "overview24s.html#regression-models",
    "href": "overview24s.html#regression-models",
    "title": "Course overview",
    "section": "Regression models",
    "text": "Regression models\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later."
  },
  {
    "objectID": "overview24s.html#regression-models-1",
    "href": "overview24s.html#regression-models-1",
    "title": "Course overview",
    "section": "Regression models",
    "text": "Regression models\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "overview24s.html#models-to-understand-politics",
    "href": "overview24s.html#models-to-understand-politics",
    "title": "Course overview",
    "section": "Models to understand politics",
    "text": "Models to understand politics\n\\(~\\)\nThis a methods class, but the methods are only meaningful to us insofar as they help us understand politics. So let’s motivate the class with a question about politics."
  },
  {
    "objectID": "overview24s.html#careful-thinking",
    "href": "overview24s.html#careful-thinking",
    "title": "Course overview",
    "section": "Careful thinking",
    "text": "Careful thinking\nCorrelation does not imply causation."
  },
  {
    "objectID": "probability24s.html#why-probability-distributions",
    "href": "probability24s.html#why-probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns."
  },
  {
    "objectID": "probability24s.html#probability-distributions",
    "href": "probability24s.html#probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Probability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?"
  },
  {
    "objectID": "probability24s.html#things-we-want-to-know-about-x",
    "href": "probability24s.html#things-we-want-to-know-about-x",
    "title": "Basic probability",
    "section": "",
    "text": "\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value)."
  },
  {
    "objectID": "probability24s.html#types-of-variables",
    "href": "probability24s.html#types-of-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Variables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "probability24s.html#discrete-variables",
    "href": "probability24s.html#discrete-variables",
    "title": "Basic probability",
    "section": "",
    "text": "May be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "probability24s.html#continuous-variables",
    "href": "probability24s.html#continuous-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Can be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "probability24s.html#levels-of-measurement",
    "href": "probability24s.html#levels-of-measurement",
    "title": "Basic probability",
    "section": "",
    "text": "These four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "probability24s.html#levels-of-measurement-and-models",
    "href": "probability24s.html#levels-of-measurement-and-models",
    "title": "Basic probability",
    "section": "",
    "text": "In general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "probability24s.html#pdf-and-cdf",
    "href": "probability24s.html#pdf-and-cdf",
    "title": "Basic probability",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nProbability distributions can be described by\n\nProbability Density Function (PDF) which maps \\(X\\) onto the probability space describing the probabilities of every value of \\(X\\), \\(x\\).\nCumulative Distribution Function (CDF) which maps \\(X\\) onto the probability space describing the probability \\(X\\) is less than some value, \\(x\\)."
  },
  {
    "objectID": "probability24s.html#pdf-density",
    "href": "probability24s.html#pdf-density",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nDefinition\n\n\n\nThe Probability Density Function or PDF describes the probability a random variable takes on a particular value, and it does so for all values of that random variable."
  },
  {
    "objectID": "probability24s.html#pdf-density-1",
    "href": "probability24s.html#pdf-density-1",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nExample\n\n\n\nSuppose we toss a fair coin 1 time and want to describe the probability distribution of the outcome, \\(Y\\) which is a random variable. The possible outcomes are heads and tails, and the probability of each is .5. This is the PDF because it describes the probabilities associated with all possible outcomes of \\(Y\\)."
  },
  {
    "objectID": "probability24s.html#pdf-density-2",
    "href": "probability24s.html#pdf-density-2",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\nWhile establishing the probability of a value of a discrete variable is possible, establishing the probability of a particular value of a continuous variable is not.\nInstead, the continuous PDF describes the instantaneous rate of change in \\(Pr(X=x)\\) for every value of \\(x\\)."
  },
  {
    "objectID": "probability24s.html#pdf-plots",
    "href": "probability24s.html#pdf-plots",
    "title": "Basic probability",
    "section": "PDF plots",
    "text": "PDF plots"
  },
  {
    "objectID": "probability24s.html#cdf",
    "href": "probability24s.html#cdf",
    "title": "Basic probability",
    "section": "CDF",
    "text": "CDF\n\n\n\n\n\n\nDefinition\n\n\n\nFor a random variable, \\(Y\\), the probability \\(Y\\) is less than or equal to some particular value, \\(y\\) in the range of \\(Y\\) defines the cumulative density function.\nFor a continuous variable:\n\\[P(Y \\leq j) =\\int\\limits_{-\\infty}^{j} f(X)dX\\]\nFor a discrete variable:\n\\[P(Y \\leq j) = \\sum\\limits_{y\\leq j} P(Y=y)\n= 1- \\sum\\limits_{y&gt; j} P(Y=y)\\]"
  },
  {
    "objectID": "probability24s.html#cdf-plots",
    "href": "probability24s.html#cdf-plots",
    "title": "Basic probability",
    "section": "CDF plots",
    "text": "CDF plots"
  },
  {
    "objectID": "probability24s.html#notation",
    "href": "probability24s.html#notation",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\\(f(x)\\) denotes the PDF of \\(x\\).\n\\(F(x)\\) denotes the CDF of \\(x\\).\nSubstitute either the appropriate name, symbol, or function for \\(F\\) or \\(f\\) to indicate the distribution.\nLower case Greek letters denote PDFs: e.g. \\(f(x)= \\phi(x)\\) denotes the normal PDF.\nUpper case Greek letters denote CDFs: e.g. \\(F(x)=\\Phi(x)\\) denotes the normal CDF."
  },
  {
    "objectID": "probability24s.html#notation-1",
    "href": "probability24s.html#notation-1",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nExample\n\n\n\n\\(x\\) is distributed Normal, \\(N(\\mu, \\sigma^{2})\\):\n\\[F(x) = \\Phi_{\\mu, \\sigma^{2}}(x)  =    \\int  \\phi_{\\mu, \\sigma^{2}} (x) f(x)d(x) \\]\n\\[ f(x) = \\phi_{\\mu, \\sigma^{2}}(x) =\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{(x-\\mu)^{2}}{2 \\sigma^{2}} \\right) \\]\nNote the first derivative of the CDF is the PDF; the integral of the PDF is the CDF."
  },
  {
    "objectID": "probability24s.html#bernoulli",
    "href": "probability24s.html#bernoulli",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nSuppose a binary variable \\(x\\) that takes on only the values of zero and one (\\(x \\in {0, 1}\\)):\n\\[\nPr(X=1)=\\pi\\nonumber \\\\\nPr(x=0)= 1-Pr(x=1) \\\\\n= 1-\\pi   \n\\]"
  },
  {
    "objectID": "probability24s.html#bernoulli-1",
    "href": "probability24s.html#bernoulli-1",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe PDF is:\n\\[ f(x) = \\\\\n          \\pi    ~~~~~~~~ ~ ~ ~~~~ \\text{if } x=1\\\\\n         1-\\pi   ~ ~ ~ ~~~~\\text{if } x=0\n     \\]\nor\n\\[f(x) = \\pi^{x}(1-\\pi)^{1-x} \\]"
  },
  {
    "objectID": "probability24s.html#bernoulli-2",
    "href": "probability24s.html#bernoulli-2",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe CDF is:\n\\[F(x) =\\sum_{x} f(x) \\]\nand the expected value is\n\\[\nE(x) =\\sum_{x}x f(x)    \\\\\n= (1)(\\pi)+(0)(1-\\pi)   \\\\\n= \\pi\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial",
    "href": "probability24s.html#binomial",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nBernoulli is important because it is the foundation for a lot of other distributions, including the binomial distribution. The binomial describes the success probability function (where \\(x=1\\) is a “success”) from a set of \\(n\\) independent Bernoulli trials. So, the binomial is the probability of successes (“ones”) in \\(n\\) independent Bernoulli trials with identical probabilities, \\(\\pi\\).\n\\(~\\)\nThere are two essential parts to the binomial PDF - the probability of success, and the number of ways a success can occur."
  },
  {
    "objectID": "probability24s.html#binomial-1",
    "href": "probability24s.html#binomial-1",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe probability of success is the Bernoulli probability:\n\\[\nf(x) = \\pi^{x}(1-\\pi)^{1-x}\n\\]\nand the number of ways success can occur (called “n-tuples”) is\n\\[  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} = \\frac{n!}{x!(n-x)!}\n\\]\nNotice the notation for the n-tuple."
  },
  {
    "objectID": "probability24s.html#binomial-2",
    "href": "probability24s.html#binomial-2",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe PDF combines these:\n\\[\nf(x)=\n  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} \\pi^{x}(1-\\pi)^{n-x}\n\\]\nThere are \\(n\\) trials, \\(x\\) is the number of successes, and \\(n-x\\) is the number of failures. Each event in the n-tuple arises with the Bernoulli probability."
  },
  {
    "objectID": "probability24s.html#binomial-3",
    "href": "probability24s.html#binomial-3",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are going to toss a fair coin 4 times and want to know how many ways we can have heads come up twice.\n\\[\n\\frac{4!}{2!(4-2)!} = 6\n\\] Now, suppose we want to know the probability exactly 2 of those 4 tosses is heads.\n\\[\nPr(x=2) =\\frac{4!}{2!(4-2)!} (.5)^{2}(.5)^{2} \\\\\n= 6(0.0625) \\\\\n= 0.375\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial-4",
    "href": "probability24s.html#binomial-4",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nHere’s another example: suppose we flip 5 fair coins once each; what is the PDF of the number of heads we expect? In other words, what is the probability associated with each possible number of successes (heads), from 0 to 5?\n\\[\nP(x=0)  =  \\begin{pmatrix}\n    5  \\\\\n    0\n  \\end{pmatrix} \\cdot  (.5)^{0} \\cdot (.5) ^{5} = 1 \\cdot 1 \\cdot 0.03125=0.03125 \\\\\nP(x=1) =  \\begin{pmatrix}\n    5  \\\\\n    1\n  \\end{pmatrix} \\cdot (.5)^{1}\\cdot (.5)^{4} = 5 \\cdot .5 \\cdot 0.0625=0.15625 \\\\\nP(x=2) =  \\begin{pmatrix}\n    5  \\\\\n    2\n  \\end{pmatrix} \\cdot (.5)^{2}\\cdot (.5)^{3} = 10 \\cdot .25 \\cdot 0.125=0.3125 \\\\\n\\vdots \\vdots \\vdots\n\\]"
  },
  {
    "objectID": "probability24s.html#binomial-family-distributions",
    "href": "probability24s.html#binomial-family-distributions",
    "title": "Basic probability",
    "section": "Binomial family distributions",
    "text": "Binomial family distributions\n\nthe geometric distribution describes repeated Bernoulli trials with probability of success \\(\\pi\\), until the first success.\nthe negative binomial describes the number of Bernoulli failures prior to the first success - it can be thought of as a counting process up to the first success.\nthe poisson describes Bernoulli trials where \\(\\pi\\) for any particular trial is very small."
  },
  {
    "objectID": "probability24s.html#the-normal-distribution",
    "href": "probability24s.html#the-normal-distribution",
    "title": "Basic probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe Normal distribution is the most widely used distribution in the social sciences.\n\nThe normal seems to “fit” a lot of the variables social scientists measure and use in models.\nEstimation techniques we often employ assume the disturbance term is normally distributed; one result is the coefficients are either normally distributed or t-distributed."
  },
  {
    "objectID": "probability24s.html#normal-pdf",
    "href": "probability24s.html#normal-pdf",
    "title": "Basic probability",
    "section": "Normal PDF",
    "text": "Normal PDF\nThe normal PDF: \\[\nPr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]\n\\]\nwhere two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\) describe the location and shape of the distribution, the mean and variance respectively; we indicate a normally distributed variable and its parameters as\n\\[\nY \\sim \\text{Normal}(\\mu,\\sigma^{2}) \\nonumber\n\\]\n\n\nDescribe these dataNormal?\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "probability24s.html#standard-normal",
    "href": "probability24s.html#standard-normal",
    "title": "Basic probability",
    "section": "Standard Normal",
    "text": "Standard Normal\nA useful special case of the Normal is the standard normal, \\(z \\sim \\text{Normal}(0,1)\\). The PDF for the standard normal is given by\n\\[\n\\phi(z)=\\frac{1}{\\sqrt{2 \\pi }} exp \\left[\\frac{-z^{2}}{2}\\right] \\nonumber\n\\]\nwhere the parameters themselves drop out since we’d be subtracting a mean of zero and dividing/multiplying by a variance of 1. The standard normal CDF is denoted \\(\\Phi(z)\\); the standard normal PDF is denoted \\(\\phi(z)\\)."
  },
  {
    "objectID": "probability24s.html#normal-pdfs-different-moments",
    "href": "probability24s.html#normal-pdfs-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs different moments",
    "text": "Normal PDFs different moments"
  },
  {
    "objectID": "probability24s.html#models",
    "href": "probability24s.html#models",
    "title": "Basic probability",
    "section": "Models",
    "text": "Models\n\nProbability models include unobserved disturbances; we make assumptions about the distributions of those errors, \\(\\epsilon\\).\nWhat we assume about the unobservables is always informed by what we know about the observables, mainly \\(y\\).\nA useful way to describe or summarize \\(y\\) is to characterize its observed distribution.\nThe distribution of \\(y\\) informs our assumption about the distribution of \\(\\epsilon\\)."
  },
  {
    "objectID": "probability24s.html#why-this-matters-to-ols",
    "href": "probability24s.html#why-this-matters-to-ols",
    "title": "Basic probability",
    "section": "Why this matters to OLS",
    "text": "Why this matters to OLS\nLinear regression is such an inferential model; we have a number of sources of uncertainty. We represent that uncertainty in the model via the disturbance term, \\(\\epsilon\\).\n\\(~\\)\nIn order to know things about \\(\\epsilon\\) and other parts of the model, we assume that the disturbances are normally distributed; \\(y\\) should be normal too."
  },
  {
    "objectID": "probability24s.html#normality-and-centrality",
    "href": "probability24s.html#normality-and-centrality",
    "title": "Basic probability",
    "section": "Normality and Centrality",
    "text": "Normality and Centrality\nWe rely on stuff being normally distributed, and central tendency being meaningful. The Central Limit Theorem facilitates this."
  },
  {
    "objectID": "probability24s.html#central-limit-theorem",
    "href": "probability24s.html#central-limit-theorem",
    "title": "Basic probability",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ {\\sum\\limits_{n \\rightarrow \\infty} (\\bar{X_i})} / {n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "probability24s.html#clt---why-is-this-valuable",
    "href": "probability24s.html#clt---why-is-this-valuable",
    "title": "Basic probability",
    "section": "CLT - Why is this valuable?",
    "text": "CLT - Why is this valuable?\n\nIf we assume repeated sampling and/or infinitely large samples, we can assume normality.\nWe know the properties of the Normal intimately well. We can use this knowledge to evaluate where some value of \\(x\\) lies on the Normal CDF, what the probability less than that value is - we can figure out what the probability of observing that value is (on the PDF).\nAs we’ll see, \\(\\widehat{\\beta}\\) is normally distributed in the OLS model (it is in MLE as well). The CLT and what we know about normality facilitate inference."
  },
  {
    "objectID": "probability24s.html#inference",
    "href": "probability24s.html#inference",
    "title": "Basic probability",
    "section": "Inference",
    "text": "Inference\nInference is our effort to measure and characterize our uncertainty about the model and its parameters.\n\nuncertainty is the most important thing we estimate in inferential models.\ncharacterizing uncertainty relies on probability theory."
  },
  {
    "objectID": "matrix24s.html#matrix-notation",
    "href": "matrix24s.html#matrix-notation",
    "title": "Matrix algebra basics",
    "section": "Matrix notation",
    "text": "Matrix notation\nFor our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model."
  },
  {
    "objectID": "matrix24s.html#matrices",
    "href": "matrix24s.html#matrices",
    "title": "Matrix algebra basics",
    "section": "Matrices",
    "text": "Matrices\nA single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A."
  },
  {
    "objectID": "matrix24s.html#dimensions",
    "href": "matrix24s.html#dimensions",
    "title": "Matrix algebra basics",
    "section": "Dimensions",
    "text": "Dimensions\nThe dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar."
  },
  {
    "objectID": "matrix24s.html#symmetric-matrices",
    "href": "matrix24s.html#symmetric-matrices",
    "title": "Matrix algebra basics",
    "section": "Symmetric matrices",
    "text": "Symmetric matrices\n\nA is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix."
  },
  {
    "objectID": "matrix24s.html#rectangular-matrices",
    "href": "matrix24s.html#rectangular-matrices",
    "title": "Matrix algebra basics",
    "section": "Rectangular matrices",
    "text": "Rectangular matrices\nA rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3)."
  },
  {
    "objectID": "matrix24s.html#vectors",
    "href": "matrix24s.html#vectors",
    "title": "Matrix algebra basics",
    "section": "Vectors",
    "text": "Vectors\nA vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "matrix24s.html#transposition",
    "href": "matrix24s.html#transposition",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nThe transpose of a matrix A is the matrix flipped on its side such that its rows become columns and columns become rows; the transpose of A is denoted \\({\\mathbf A'}\\) and is referred to as “\\({\\mathbf A}\\) transpose.”\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right]\n~~~~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5 & 2\\\\\n\\end{array} \\right] \\]\nso \\({\\mathbf X}\\), a (3,2) matrix, becomes \\({\\mathbf X'}\\), a (2,3) matrix."
  },
  {
    "objectID": "matrix24s.html#transposition-1",
    "href": "matrix24s.html#transposition-1",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nIf \\({\\mathbf A}\\) is symmetric, then \\({\\mathbf A}={\\mathbf A'}\\) – take a look at the following symmetric matrix and you’ll see why:\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#transposition-2",
    "href": "matrix24s.html#transposition-2",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nAlso note that the transpose of a transposed matrix results in the original matrix: \\((\\mathbf{X}')'=\\mathbf{X}\\).\nTransposing a row vector results in a column vector and vice versa:\n\\[\\mathbf{x} =  \\left[\n\\begin{array}{c}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n\\end{array} \\right]\n~~~~~~~~~\n\\mathbf{x'} =  \\left[\n\\begin{array}{cccc}\n-1 & -9 &4 &16\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#trace-of-a-matrix",
    "href": "matrix24s.html#trace-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nThe trace of a matrix is the sum of the diagonal elements of a square matrix, and is denoted \\(tr(\\mathbf{A})=a_{11}+a_{22}+a_{33}\\ldots+a_{nn} = \\sum\\limits_{i=1}^{n}a_{ii}\\)\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nsuch that\n\\(tr(\\mathbf{A})=2+5+2=9\\)"
  },
  {
    "objectID": "matrix24s.html#addition-subtraction",
    "href": "matrix24s.html#addition-subtraction",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\n\nAddition and subtraction of matrices is analogous to the same scalar operations, but depend on the conformatibility of the matrices to be added or subtracted.\nTwo matrices are conformable for addition or subtraction iff they are of the same order.\nNote that conformability means something different for addition/subtraction than for multiplication."
  },
  {
    "objectID": "matrix24s.html#addition-subtraction-1",
    "href": "matrix24s.html#addition-subtraction-1",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider the following (2,2) matrices and the problem \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{C}\\):\n\\[ \\left[\n\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21}& a_{22} \\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{cc}\nb_{11} & b_{12}\\\\\nb_{21}& b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21}& a_{22}+b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\nc_{11} & c_{12}\\\\\nc_{21}& c_{22} \\\\\n\\end{array} \\right] \\]\nAddition and subtraction are only possible among matrices that share the same order; otherwise, they are nonconformable."
  },
  {
    "objectID": "matrix24s.html#addition-subtraction-2",
    "href": "matrix24s.html#addition-subtraction-2",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider a second example of addition, and note that subtraction would follow directly:\n\\[ \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{ccc}\n1 &5 &7\\\\\n3& 7& 1\\\\\n2& 4& 9\\\\\n\\end{array} \\right]\n=\n  \\left[\n\\begin{array}{ccc}\n3 &9 &15\\\\\n7& 12& 4\\\\\n10& 7& 11\\\\\n\\end{array} \\right] \\]\nMatrix addition adheres to the commutative and associative properties such that:\n\\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) (Commutative), and \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}= \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) (Associative)."
  },
  {
    "objectID": "matrix24s.html#multiplication",
    "href": "matrix24s.html#multiplication",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\nLet’s begin by multiplying a scalar and a matrix - the product is a matrix whose elements are the scalar multiplied by each element of the original matrix, so:\n\\[ \\beta\n\\left[\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\n\\beta x_{11} & \\beta x_{12}\\\\\n\\beta x_{21}&  \\beta x_{22} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "matrix24s.html#multiplication-1",
    "href": "matrix24s.html#multiplication-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\n\nIn order to multiply two matrices (or vectors), they must be conformable for multiplication.\nTwo matrices are conformable for multiplication iff the number of columns in the first matrix is equal to the number of rows in the second matrix. That is, \\(\\mathbf{A_{i,j}}\\) and \\(\\mathbf{B_{j,k}}\\).\nAn easy way to think of this is to write the dimensions of the two matrices, (i,j),(j,k) - the inner dimensions are the same, so the matrix is conformable for multiplication - moreover, the outer dimensions (i,k) give the dimension of the product matrix."
  },
  {
    "objectID": "matrix24s.html#multiplication---inner-product",
    "href": "matrix24s.html#multiplication---inner-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\n\nTo illustrate multiplication, let’s start with a column vector, \\(\\mathbf{e}\\) whose order is (N,1) and take its inner product.\nThe inner product of a column vector is the transpose of the column vector (thus, a row vector) post-multiplied by the column vector, so \\({\\mathbf e'\\mathbf e}\\). The inner product is a scalar."
  },
  {
    "objectID": "matrix24s.html#multiplication---inner-product-1",
    "href": "matrix24s.html#multiplication---inner-product-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\nWhen we transpose the column vector \\({\\mathbf e}\\) of (N,1) order, we get a row vector \\({\\mathbf e'}\\) of (1,N) order. Inner dimensions match, so they are conformable.\n\\[\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n~~~~~\n\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right] \\] \\[=\n(-1 \\cdot -1)+(-9 \\cdot -9)+(4 \\cdot 4)+ (16 \\cdot 16)+ (-10 \\cdot -10) = 454\\]"
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing",
    "href": "matrix24s.html#least-squares-foreshadowing",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nNote the inner product of a column vector is a scalar; the inner product of \\(\\mathbf{e}\\), is the sum of the squares of \\(\\mathbf{e}\\).\n\\[\\mathbf{e'e}= e_{1}e_{1}+e_{2}e_{2}+\\ldots e_{N}e_{N} = \\sum\\limits_{i=1}^{N}e_{i}^{2}\\]"
  },
  {
    "objectID": "matrix24s.html#multiplication---outer-product",
    "href": "matrix24s.html#multiplication---outer-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - outer product",
    "text": "Multiplication - outer product\nThe outer product of a column vector is the transpose of the column vector (thus, a row vector) pre-multiplied by the column vector, so \\({\\mathbf e\\mathbf e'}\\). The outer product is an (N,N) matrix.\n\\[\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right]\n\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n\\]\nLet’s multiply \\({\\mathbf e\\mathbf e'}\\), a column vector of (5,1) by a row vector of (1,5); we’ll obtain a square matrix of (5,5)."
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing-1",
    "href": "matrix24s.html#least-squares-foreshadowing-1",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nLet \\(\\mathbf{e}\\) represent the residuals or errors in our regression. The outer product\n\\[{\\mathbf e\\mathbf e'} =  \\left[\n\\begin{array}{rrrrr}\n1 & 9 &-4 &-16 &10\\\\\n9 & 81 &-36 &-144 &90\\\\\n-4 & -36 &16 &64 &-40\\\\\n-16 & -144 &64 &256 &-160\\\\\n10 & 90 &-40 &-160 &100\\\\\n\\end{array} \\right] \\]\nis the variance-covariance matrix of \\(\\mathbf{e}\\). The squares on the main diagonal (which sums to the sum of squares) and the symmetry of the off-diagonal elements."
  },
  {
    "objectID": "matrix24s.html#inverting-matrices-1",
    "href": "matrix24s.html#inverting-matrices-1",
    "title": "Matrix algebra basics",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\n\nYou’ll have noticed that division has been conspicuously absent so far. In scalar algebra, we sometimes represent division in fractions, \\(\\frac{1}{2}\\).\nAnother way to represent the same quantity is \\(2^{-1}\\), or the inverse of 2. Of course, in scalar algebra, a number multiplied by its inverse equals 1, e.g., \\(2 \\cdot 2^{-1}=1\\) or \\(2 \\cdot \\frac{1}{2}=1\\). So, if we are given \\(4x=1\\) and want to solve for \\(x\\) what we really want to know is “what is the inverse of 4 such that \\(4 \\cdot 4^{-1}=1\\)?” We consider matrices in a similar manner."
  },
  {
    "objectID": "matrix24s.html#inverting-square-matrices",
    "href": "matrix24s.html#inverting-square-matrices",
    "title": "Matrix algebra basics",
    "section": "Inverting Square Matrices",
    "text": "Inverting Square Matrices\nImagine a square matrix, \\(\\mathbf{X}\\) and its inverse, denoted \\({\\mathbf{X^{-1}}}\\) (read as “X inverse”). Analogous to scalar algebra, a matrix multiplied by its inverse is equal to the identity matrix, \\(\\mathbf{I}\\).\nSo in order to find \\({\\mathbf{X^{-1}}}\\), we must find the matrix that, when multiplied by \\(\\mathbf{X}\\), produces \\(\\mathbf{I}\\). Thus, for a square matrix, its inverse (if it exists) is such that\n\\[\\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X^{-1}} \\mathbf{X}=\\mathbf{I}\\]\nNotice the commutative property at work here - this is because \\(\\mathbf{X}\\) is square, such that \\(n=m\\), so \\(\\mathbf{X}\\) and \\({\\mathbf{X^{-1}}}\\) have the same dimensions."
  },
  {
    "objectID": "matrix24s.html#determinant",
    "href": "matrix24s.html#determinant",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nAn important characteristic of square matrices is the determinant. The determinant, denoted \\(|X|\\), is a scalar; every square matrix has one. We evaluate the determinant by examining the cross-products of the matrice’s elements. This is simple in the 2x2 matrix, less so in larger matrices.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right| = x_{11}x_{22}-x_{12}x_{21}\\]"
  },
  {
    "objectID": "matrix24s.html#determinant-1",
    "href": "matrix24s.html#determinant-1",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nIn the 2x2 matrix, the determinant is the cross-product of the main diagonals minus the cross-product of the off-diagonals.\n\\[ |\\mathbf{X_{1}}|=\\left|\n\\begin{array}{cc}\n2 & 4\\\\\n6& 3 \\\\\n\\end{array} \\right| = 2 \\cdot 3-4 \\cdot 6 = -18\n\\]\nThe determinant is -18; \\(|\\mathbf{X_{1}}|=-18\\)."
  },
  {
    "objectID": "matrix24s.html#determinant-2",
    "href": "matrix24s.html#determinant-2",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nThis matrix has a determinant of zero - this is an important case.\n\\[ |\\mathbf{X_{2}}|=\\left|\n\\begin{array}{cc}\n3 & 6\\\\\n2& 4 \\\\\n\\end{array} \\right| = 3 \\cdot 4-6 \\cdot 2 =0\n\\] A matrix with determinant zero is singular. A singluar matrix has no inverse."
  },
  {
    "objectID": "matrix24s.html#singular-matrices",
    "href": "matrix24s.html#singular-matrices",
    "title": "Matrix algebra basics",
    "section": "Singular matrices",
    "text": "Singular matrices\nThere are some important conditions that will produce a determinant of zero and thus a singular matrix:\n\nIf all the elements of any row or column of a matrix are equal to zero, then the determinant is zero.\nIf two rows or columns of a matrix are identical, the determinant is zero.\nIf a row or column of a matrix is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24s.html#least-squares-foreshadowing-2",
    "href": "matrix24s.html#least-squares-foreshadowing-2",
    "title": "Matrix algebra basics",
    "section": "Least Squares Foreshadowing",
    "text": "Least Squares Foreshadowing\nFor the linear model in least squares, this is important because computing estimates of \\(\\beta\\) requires inverting a matrix. Let’s derive \\(\\beta\\) in matrix notation to see how:\nThe estimated model is:\n\\[\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}}\\] Minimizing \\(\\widehat{\\mathbf{\\epsilon}}'\\widehat{\\mathbf{\\epsilon}}\\) (skipping the math for now), we get\n\\[(\\mathbf{X'X}) \\widehat{\\beta}=\\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares",
    "href": "matrix24s.html#foreshadowing-least-squares",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThe matrix \\((\\mathbf{X'X})\\) gives the sums of squares (on the main diagonal) and the sums of cross products (in the off-diagonals) of all the \\(\\mathbf{X}\\) variables; the matrix is symmetric. Since \\(\\beta\\) is the unknown vector in this equation, solve for \\(\\beta\\) by dividing both sides by \\((\\mathbf{X'X})\\) - in matrix terms, we premultiply each side by \\((\\mathbf{X'X)^{-1}}\\):\n\\[(\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) \\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]"
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares-1",
    "href": "matrix24s.html#foreshadowing-least-squares-1",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nAs we know, \\((\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) = \\mathbf{I}\\). So,\n\\[\\mathbf{I}\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\] or \\[\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]\nEstimating \\(\\widehat{\\beta}\\) requires inverting \\(\\mathbf{(X'X)}\\) If the matrix \\(\\mathbf{(X'X)}\\) is singular, then \\(\\mathbf{(X'X)^{-1}}\\) does not exist, and we cannot estimate \\(\\widehat{\\beta}\\). Least Squares fails."
  },
  {
    "objectID": "matrix24s.html#foreshadowing-least-squares-2",
    "href": "matrix24s.html#foreshadowing-least-squares-2",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThinking specifically of the \\(\\mathbf{X}\\) variables in the model, any of these conditions produce perfect collinearity - estimation fails because the matrix can’t invert.\n\nIf all the elements of any row or column of \\(\\mathbf{X}\\) are equal to zero, then the determinant is zero.\nIf two rows or columns of \\(\\mathbf{X}\\) are identical, the determinant is zero.\nIf a row or column of \\(\\mathbf{X}\\) is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "matrix24s.html#examples",
    "href": "matrix24s.html#examples",
    "title": "Matrix algebra basics",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2Example 3\n\n\n\\({\\mathbf X}\\) below has a row of zeros; compute the determinant using the difference of cross-products.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n0 & 0\\\\\n19& 12 \\\\\n\\end{array} \\right| = 0 \\cdot 12-0 \\cdot 19 = 0 \\]\nYou can see in \\({\\mathbf X}\\) that because all the cross-products involve multiplying by zero, the determinant will always be zero; this applies to larger matrices as well.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n5 & 7\\\\\n5& 7 \\\\\n\\end{array} \\right| = 5 \\cdot 7-5 \\cdot 7 =0 \\]\nIn \\({\\mathbf X}\\) it’s easy to see that the cross-products will always be equal to one another if the rows or columns are identical, and the difference between identical values is zero.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n64 & 48\\\\\n16& 12 \\\\\n\\end{array} \\right| = 64 \\cdot 12-48 \\cdot 16 = 768-768=0\\]\nFinally \\({\\mathbf X}\\) shows that if one row or column is a linear combination of other rows or columns, the determinant will be zero; in that matrix, the top row is 4 times the bottom row."
  },
  {
    "objectID": "matrix24s.html#inversion",
    "href": "matrix24s.html#inversion",
    "title": "Matrix algebra basics",
    "section": "Inversion",
    "text": "Inversion\nThe matrix we need to invert, \\(\\mathbf{X'X}\\) is rectangular, so inversion is more complex. I’m going to illustrate the method of cofactor expansion - the purpose here is to give you an idea what software does every time you estimate an OLS model."
  },
  {
    "objectID": "matrix24s.html#minor-of-a-matrix",
    "href": "matrix24s.html#minor-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Minor of a matrix",
    "text": "Minor of a matrix\nThe minor of a matrix is the determinant of a submatrix created by deleting the \\(i\\)th row and the \\(j\\)th column of the full matrix, where the minor is denoted by the element where the deleted rows intersect.\n\\[ \\mathbf{A} =  \\left[\n\\begin{array}{ccc}\na_{11} & a_{12} &a_{13}\\\\\na_{21}& a_{22} &a_{23}\\\\\na_{31} &a_{32} & a_{33}\\\\\n\\end{array} \\right] \\] \\[\\text{so the minor of } a_{11} \\text{ is }\n|\\mathbf{M_{11}}| =  \\left|\n\\begin{array}{cc}\na_{22} &a_{23}\\\\\na_{32} & a_{33}\\\\\n\\end{array} \\right|  =  a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\]"
  },
  {
    "objectID": "matrix24s.html#cofactor-matrix",
    "href": "matrix24s.html#cofactor-matrix",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nThe cofactor of an element (\\(c_{ij}\\)) is the minor with a positive or negative sign depending on whether \\(i+j\\) is odd (negative) or even (positive). This is given by:\n\\[\\theta_{ij}=(-1)^{i+j}|\\mathbf{M_{ij}}|\\]\nso, from the example finding the minor of \\(a_{11}\\), \\(i\\)=1 and \\(j=1\\); their sum is 2 which is even, so the cofactor is \\(+a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\). If we find the cofactor for every element, \\(a_{ij}\\) in a matrix and replace each element with its cofactor, the new matrix is called the cofactor matrix of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24s.html#cofactor-matrix-1",
    "href": "matrix24s.html#cofactor-matrix-1",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nWhat we have so far is the signed matrix of minors:\n\\[\\mathbf{A} = \\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#cofactor-expansion",
    "href": "matrix24s.html#cofactor-expansion",
    "title": "Matrix algebra basics",
    "section": "Cofactor Expansion",
    "text": "Cofactor Expansion\nFind the signed determinant of each 2x2 submatrix.\n\\[\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} = \\left[\n\\begin{array}{rrr}\n10&-10&0\\\\\n-12&0&6\\\\\n-2&5&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#find-the-determinant",
    "href": "matrix24s.html#find-the-determinant",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nUsing the cofactor matrix, \\(\\mathbf{\\Theta_{A}}\\) we perform cofactor expansion in order to find the determinant, \\(|\\mathbf{A}|\\). Cofactor expansion is given by:\n\\[|\\mathbf{A}|=\\sum\\limits_{i,j=1}^{n}a_{ij}\\Theta_{ij}\\]\nwhich means that we take two corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) and sum the products of their elements - this gives us the determinant of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "matrix24s.html#find-the-determinant-1",
    "href": "matrix24s.html#find-the-determinant-1",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nSo taking the first row from \\(\\mathbf{A}\\) (1,4,2) and from \\(\\mathbf{\\Theta_{A}}\\) (10,-10,0) above, we compute\n\\[a_{11} \\cdot \\Theta_{A11}+a_{12} \\cdot \\Theta_{A12}+a_{13} \\cdot \\Theta_{A13}\\]\n\\[1 \\cdot 10+ 4 \\cdot -10 + 2 \\cdot 0 = -30\\] This is the determinant of \\(\\mathbf{A}\\) , \\(|\\mathbf{A}|\\). Note that any corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) will produce the same result."
  },
  {
    "objectID": "matrix24s.html#adjoint-matrix",
    "href": "matrix24s.html#adjoint-matrix",
    "title": "Matrix algebra basics",
    "section": "Adjoint Matrix",
    "text": "Adjoint Matrix\nIf we transpose the cofactor matrix, (cof \\(\\mathbf{A'}\\)), the new matrix is called the adjoint matrix, denoted adj\\(\\mathbf{A}\\)=(cof \\(\\mathbf{A'}\\)).\n\\[adj\\mathbf{A} = \\mathbf{\\Theta_{A}'} = \\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#inverting-the-matrix",
    "href": "matrix24s.html#inverting-the-matrix",
    "title": "Matrix algebra basics",
    "section": "Inverting the matrix",
    "text": "Inverting the matrix\nBelieve it or not, this all leads us to inverting the matrix, provided it is nonsingular.\n\\[\\mathbf{A^{-1}}=\\frac{1}{|\\mathbf{A}|}(adj {\\mathbf A})\\] The inverse of A is equal to one over the determinant of A times the adjoint matrix of A. So we’re pre-multiplying \\(adj {\\mathbf A}\\) by the (scalar) determinant, -30."
  },
  {
    "objectID": "matrix24s.html#inverse-of-matrix-a",
    "href": "matrix24s.html#inverse-of-matrix-a",
    "title": "Matrix algebra basics",
    "section": "Inverse of Matrix A",
    "text": "Inverse of Matrix A\n\\[{\\mathbf A^{-1}}=-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n=\\left[\n\\begin{array}{rrr}\n-\\frac{1}{3}&\\frac{2}{5}&\\frac{1}{15}\\\\\n\\frac{1}{3}&0&-\\frac{1}{6}\\\\\n0&-\\frac{1}{5}&\\frac{3}{10}\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#checking-our-work",
    "href": "matrix24s.html#checking-our-work",
    "title": "Matrix algebra basics",
    "section": "Checking our work",
    "text": "Checking our work\nWe can check our work to be sure we’ve inverted correctly by making sure that \\(\\mathbf{A^{-1}A}=\\mathbf{I_{3}}\\); so,\n\\[-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n\\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n=\n-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n-30&0&0\\\\\n0&-30&0\\\\\n0&0&-30\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols",
    "href": "matrix24s.html#connecting-data-matrices-to-ols",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\nLet’s examine these matrices a little to get a feel for what the computation of \\(\\widehat{\\beta}\\) involves: \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\n\\[\n\\mathbf{X}= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \n\\end{matrix}  \\right]\n\\] The dimensions of \\(\\mathbf{X}\\) are \\(n x k\\); the sample size by the number of regressors (including the constant)."
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols-1",
    "href": "matrix24s.html#connecting-data-matrices-to-ols-1",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \n\\end{matrix}  \\right] \\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "matrix24s.html#connecting-data-matrices-to-ols-2",
    "href": "matrix24s.html#connecting-data-matrices-to-ols-2",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i}\n\\end{matrix}  \\right] \\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationships among \\(X_k\\)."
  },
  {
    "objectID": "overview24s.html",
    "href": "overview24s.html",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise\n\n\n\n\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding.\n\n\n\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling.\n\n\n\n\nIs not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal.\n\n\n\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here.\n\n\n\n\n\nis a collaborative effort among you, and with me and Kathleen. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together.\n\n\n\n\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra\n\n\n\n\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^-1\\) and \\(X'y\\).\n\n\n\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?\n\n\n\n\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term.\n\n\n\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later.\n\n\n\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "syllabus24.html",
    "href": "syllabus24.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dave Clark\n   LNG 57; LN2430\n   dclark@binghamton.edu\n   clavedark\noffice hours: M 1:30-3:30pm\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL"
  },
  {
    "objectID": "syllabus24.html#seminar-description",
    "href": "syllabus24.html#seminar-description",
    "title": "Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret an informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "syllabus24.html#course-purpose",
    "href": "syllabus24.html#course-purpose",
    "title": "Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "syllabus24.html#learning-objectives",
    "href": "syllabus24.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "syllabus24.html#reading",
    "href": "syllabus24.html#reading",
    "title": "Syllabus",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways.\nIn addition, I’ll assign a small number of articles over the course of the term."
  },
  {
    "objectID": "syllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "syllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it.\n\n\nHow to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#on-reading-or-how-to-read",
    "href": "syllabus24.html#on-reading-or-how-to-read",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "On reading, or How to Read",
    "text": "On reading, or How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever. \\\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing. \\\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#course-requirements",
    "href": "syllabus24.html#course-requirements",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Requirements",
    "text": "Course Requirements\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in \\(\\LaTeX\\) or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n“D”, “60-69%”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;70%"
  },
  {
    "objectID": "syllabus24.html#how-to-read",
    "href": "syllabus24.html#how-to-read",
    "title": "Syllabus",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24.html#course-requirements-and-grades",
    "href": "syllabus24.html#course-requirements-and-grades",
    "title": "Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "syllabus24.html#course-schedule",
    "href": "syllabus24.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013\nWeek 1. 17 Jan Introduction, Regression Discussion\n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "pdf.html",
    "href": "pdf.html",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "",
    "text": "Prof. Dave Clark\n   LNG 57\n   dclark@binghamton.edu\n   clavedark\n\n\n\n\n\n   Kathleen Bannon\n   kbannon1@binghamton.edu\n\n\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL\n\n\n\n\n\n   Thursday\n   1:15-2:15\n   CW 309"
  },
  {
    "objectID": "pdf.html#seminar-description",
    "href": "pdf.html#seminar-description",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret and informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "pdf.html#course-purpose",
    "href": "pdf.html#course-purpose",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "pdf.html#learning-objectives",
    "href": "pdf.html#learning-objectives",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "pdf.html#reading",
    "href": "pdf.html#reading",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways."
  },
  {
    "objectID": "pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it."
  },
  {
    "objectID": "pdf.html#how-to-read",
    "href": "pdf.html#how-to-read",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "pdf.html#course-requirements-and-grades",
    "href": "pdf.html#course-requirements-and-grades",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in \\(\\LaTeX\\) or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "pdf.html#course-schedule",
    "href": "pdf.html#course-schedule",
    "title": "PLSC 501 Syllabus, spring 2024",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013}\nWeek 1. 17 Jan Introduction, Regression Discussion \n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "syllabus24.html#attendance",
    "href": "syllabus24.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class."
  },
  {
    "objectID": "syllabus24.html#academic-integrity",
    "href": "syllabus24.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "syllabus24.html#course-policies",
    "href": "syllabus24.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Font Awesome Quarto Extension",
    "section": "",
    "text": "This extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; fa &lt;icon-name&gt; &gt;}}\nOptional &lt;group&gt;, &lt;size=...&gt;, and &lt;title=...&gt;:\n{{&lt; fa &lt;group&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;title=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n{{&lt; fa brands twitter size=2xl &gt;}} (HTML only)\n\n\n\n{{&lt; fa brands github size=5x &gt;}} (HTML only)\n\n\n\n{{&lt; fa battery-half size=Huge &gt;}}\n\n\n\n{{&lt; fa envelope title=\"An envelope\" &gt;}}\n\n\n\n\nNote that the icon sets are currently not perfectly interchangeable across formats:\n\nhtml uses FontAwesome 6.4.2\npdf uses the fontawesome5 package, based on FontAwesome 5.\nOther formats are currently not supported, but PRs are always welcome!"
  },
  {
    "objectID": "syllabus24pdf.html",
    "href": "syllabus24pdf.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dave Clark\n   LNG 57\n   dclark@binghamton.edu\n   clavedark\n\n\n\n\n\n   Spring 2024\n   Wednesday\n   9:40-12:40\n   LNG 332 Bing SSEL"
  },
  {
    "objectID": "syllabus24pdf.html#seminar-description",
    "href": "syllabus24pdf.html#seminar-description",
    "title": "Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret and informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "syllabus24pdf.html#course-purpose",
    "href": "syllabus24pdf.html#course-purpose",
    "title": "Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "syllabus24pdf.html#learning-objectives",
    "href": "syllabus24pdf.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "syllabus24pdf.html#reading",
    "href": "syllabus24pdf.html#reading",
    "title": "Syllabus",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways.\nIn addition, I’ll assign a small number of articles over the course of the term."
  },
  {
    "objectID": "syllabus24pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "syllabus24pdf.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it.\n\n\nHow to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus24pdf.html#course-requirements-and-grades",
    "href": "syllabus24pdf.html#course-requirements-and-grades",
    "title": "Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Kathleen will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "syllabus24pdf.html#course-policies",
    "href": "syllabus24pdf.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "syllabus24pdf.html#course-schedule",
    "href": "syllabus24pdf.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013\nWeek 1. 17 Jan Introduction, Regression Discussion\n\nmatrix notes, preview materials\n\nWeek 2, 24 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 31 Jan – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 7 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 14 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 21 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 28 Feb – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 6 March – Spring Break\nWeek 9, 13 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 20 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 27 March – Panels, Fixed & Random Effects - Wooldridge, chapters 13, 14\nWeek 12, 3 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 10 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 14, 17 April – IV models\n\nWooldridge, chapter 15\n\nWeek 15, 24 April – Passover, no classes\nWeek 16, 1 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "data.html#get-to-know-your-data-explore-etc",
    "href": "data.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "data.html#data-generating-process",
    "href": "data.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\nA Terrible Map\n\nWar outcomes\n\nwar outcome/duration data\n\nObservability\n\nAsking the right question"
  },
  {
    "objectID": "data.html#data-in-the-regression-context",
    "href": "data.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "data.html#data",
    "href": "data.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "data.html#types-of-variables",
    "href": "data.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "data.html#discrete-variables",
    "href": "data.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "data.html#continuous-variables",
    "href": "data.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "data.html#levels-of-measurement",
    "href": "data.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "data.html#levels-of-measurement-and-models",
    "href": "data.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "data.html#describe-these-data",
    "href": "data.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "data.html#describe-these-data-1",
    "href": "data.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "data.html#overstaying-terms",
    "href": "data.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata.html",
    "href": "thinkingdata.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata.html#data-generating-process",
    "href": "thinkingdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\n\nA Terrible Map\n\n\n\nWar outcomes\n\nwar outcome/duration data\n\n\n\nObservability\n\n\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata.html#data-in-the-regression-context",
    "href": "thinkingdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata.html#data",
    "href": "thinkingdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata.html#types-of-variables",
    "href": "thinkingdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata.html#discrete-variables",
    "href": "thinkingdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata.html#continuous-variables",
    "href": "thinkingdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata.html#levels-of-measurement",
    "href": "thinkingdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata.html#levels-of-measurement-and-models",
    "href": "thinkingdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata.html#describe-these-data",
    "href": "thinkingdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata.html#describe-these-data-1",
    "href": "thinkingdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata.html#overstaying-terms",
    "href": "thinkingdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html",
    "href": "thinkingdata/thinking-data24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata/thinking-data24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data-generating-process",
    "href": "thinkingdata/thinking-data24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\n\nA Terrible Map\n\n\n\nWar outcomes\n\nwar outcome/duration data\n\n\n\nObservability\n\n\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data-in-the-regression-context",
    "href": "thinkingdata/thinking-data24.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#data",
    "href": "thinkingdata/thinking-data24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#types-of-variables",
    "href": "thinkingdata/thinking-data24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#discrete-variables",
    "href": "thinkingdata/thinking-data24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#continuous-variables",
    "href": "thinkingdata/thinking-data24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#levels-of-measurement",
    "href": "thinkingdata/thinking-data24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#levels-of-measurement-and-models",
    "href": "thinkingdata/thinking-data24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#describe-these-data",
    "href": "thinkingdata/thinking-data24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#describe-these-data-1",
    "href": "thinkingdata/thinking-data24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata/thinking-data24.html#overstaying-terms",
    "href": "thinkingdata/thinking-data24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinkingdata/tdata.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata/tdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetAnscombe’s Quartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n# kable(anscombe, format=\"html\", row.names = FALSE, align=\"cccccccc\", caption=\"Anscombe's Quartet\")\n\nkable(\n  list(B, anscombe),\n  caption=\"Anscombe's data, Relationships of x, y\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE\n)\n\n\n\n\n\nAnscombe’s data, Relationships of x, y\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#data-generating-process",
    "href": "thinkingdata/tdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\nWhy do we observe the data we see and not the data we don’t?\n\nA Terrible Map\n\nWar outcomes\n\nwar outcome/duration data\n\nObservability\n\nAsking the right question"
  },
  {
    "objectID": "thinkingdata/tdata.html#data-in-the-regression-context",
    "href": "thinkingdata/tdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinkingdata/tdata.html#data",
    "href": "thinkingdata/tdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata/tdata.html#types-of-variables",
    "href": "thinkingdata/tdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata/tdata.html#discrete-variables",
    "href": "thinkingdata/tdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata/tdata.html#continuous-variables",
    "href": "thinkingdata/tdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata/tdata.html#levels-of-measurement",
    "href": "thinkingdata/tdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata/tdata.html#levels-of-measurement-and-models",
    "href": "thinkingdata/tdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata/tdata.html#describe-these-data",
    "href": "thinkingdata/tdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#describe-these-data-1",
    "href": "thinkingdata/tdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata/tdata.html#overstaying-terms",
    "href": "thinkingdata/tdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "tdata.html#get-to-know-your-data-explore-etc",
    "href": "tdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "tdata.html#data-generating-process",
    "href": "tdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "tdata.html#data-in-the-regression-context",
    "href": "tdata.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "tdata.html#data",
    "href": "tdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "tdata.html#types-of-variables",
    "href": "tdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "tdata.html#discrete-variables",
    "href": "tdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "tdata.html#continuous-variables",
    "href": "tdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "tdata.html#levels-of-measurement",
    "href": "tdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "tdata.html#levels-of-measurement-and-models",
    "href": "tdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "tdata.html#describe-these-data",
    "href": "tdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "tdata.html#describe-these-data-1",
    "href": "tdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "tdata.html#overstaying-terms",
    "href": "tdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "thinking-data24.html",
    "href": "thinking-data24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinking-data24.html#get-to-know-your-data-explore-etc",
    "href": "thinking-data24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinking-data24.html#data-generating-process",
    "href": "thinking-data24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "thinking-data24.html#data-in-the-regression-context",
    "href": "thinking-data24.html#data-in-the-regression-context",
    "title": "Thinking About Data",
    "section": "Data in the regression context",
    "text": "Data in the regression context\n\nmatrix"
  },
  {
    "objectID": "thinking-data24.html#data",
    "href": "thinking-data24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinking-data24.html#types-of-variables",
    "href": "thinking-data24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinking-data24.html#discrete-variables",
    "href": "thinking-data24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinking-data24.html#continuous-variables",
    "href": "thinking-data24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinking-data24.html#levels-of-measurement",
    "href": "thinking-data24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinking-data24.html#levels-of-measurement-and-models",
    "href": "thinking-data24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinking-data24.html#describe-these-data",
    "href": "thinking-data24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinking-data24.html#describe-these-data-1",
    "href": "thinking-data24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinking-data24.html#overstaying-terms",
    "href": "thinking-data24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "tdata.html#a-terrible-map",
    "href": "tdata.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "tdata.html#war-outcomes",
    "href": "tdata.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "tdata.html#observability",
    "href": "tdata.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "tdata.html#asking-the-right-question",
    "href": "tdata.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinking-data24.html#a-terrible-map",
    "href": "thinking-data24.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "thinking-data24.html#war-outcomes",
    "href": "thinking-data24.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "thinking-data24.html#observability",
    "href": "thinking-data24.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "thinking-data24.html#asking-the-right-question",
    "href": "thinking-data24.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinking-data24.html#ooof",
    "href": "thinking-data24.html#ooof",
    "title": "Thinking About Data",
    "section": "ooof",
    "text": "ooof\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1"
  },
  {
    "objectID": "tdata.html#ooof",
    "href": "tdata.html#ooof",
    "title": "Thinking About Data",
    "section": "ooof",
    "text": "ooof\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1"
  },
  {
    "objectID": "thinking-data24.html#data-generating-process-1",
    "href": "thinking-data24.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinking-data24.html#war-outcomes-lake92-p.-31",
    "href": "thinking-data24.html#war-outcomes-lake92-p.-31",
    "title": "Thinking About Data",
    "section": "War outcomes (Lake (1992) p. 31)",
    "text": "War outcomes (Lake (1992) p. 31)\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74\n47\n12 1\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "bivariate24.html",
    "href": "bivariate24.html",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Regression in any form is based on the conditional expectation of \\(Y\\) - the expected value of \\(Y\\) is conditional on some \\(X\\)s, but we don’t know the actual conditions or effects of those \\(X\\)s. So we can write the regression like this:\n\\[E[y|X_{1}, \\ldots,X_{k}]= \\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\]\n\n\nLet \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances.\n\n\n\nThe predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\).\n\n\n\nThe differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\).\n\n\n\nRestating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#regression",
    "href": "bivariate24.html#regression",
    "title": "The Bivariate Model",
    "section": "Regression",
    "text": "Regression\nRegression in any form is based on the conditional expectation of \\(Y\\) - the expected value of \\(Y\\) is conditional on some \\(X\\)s, but we don’t know the actual conditions or effects of those \\(X\\)s. So we can write the regression like this:\n\\[E[y|X_{1}, \\ldots,X_{k}]= \\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\]"
  },
  {
    "objectID": "bivariate24.html#regression-1",
    "href": "bivariate24.html#regression-1",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Let \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "bivariate24.html#linear-predictions-residuals",
    "href": "bivariate24.html#linear-predictions-residuals",
    "title": "The Bivariate Model",
    "section": "Linear predictions, Residuals",
    "text": "Linear predictions, Residuals\nThe predicted points that form the line are \\(\\widehat{Y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\).\nThe differences between those predicted points, \\(\\widehat{Y_{i}}\\) and the observed values \\(Y_i\\) are:\n\\[Y_{i}-\\widehat{Y_{i}} = e  \\\\\n= Y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i} \\nonumber \\\\\n= Y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\]\nThese are the residuals, \\(e\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#in-matrix",
    "href": "bivariate24.html#in-matrix",
    "title": "The Bivariate Model",
    "section": "in matrix",
    "text": "in matrix\nRestating in matrix notation:\n\\[\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{e_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{e_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-i",
    "href": "bivariate24.html#getting-to-ols-i",
    "title": "The Bivariate Model",
    "section": "Getting to OLS I",
    "text": "Getting to OLS I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-1",
    "href": "bivariate24.html#ols-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nTo minimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]\n\\(\\ldots\\) illustrating explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24.html#ols-1-1",
    "href": "bivariate24.html#ols-1-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-ii",
    "href": "bivariate24.html#getting-to-ols-ii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS II",
    "text": "Getting to OLS II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero.\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling that the mean of a variable\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]\nSo, substituting these equations into\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\\]\nand solving for \\(\\hat{\\beta}_1\\) gives:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24.html#getting-to-ols-iii",
    "href": "bivariate24.html#getting-to-ols-iii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS III",
    "text": "Getting to OLS III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24.html#matrices",
    "href": "bivariate24.html#matrices",
    "title": "The Bivariate Model",
    "section": "Matrices",
    "text": "Matrices\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#covariation-xy",
    "href": "bivariate24.html#covariation-xy",
    "title": "The Bivariate Model",
    "section": "Covariation X’y",
    "text": "Covariation X’y\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\) and \\(X_{2}\\)."
  },
  {
    "objectID": "bivariate24.html#matrices-and-regression",
    "href": "bivariate24.html#matrices-and-regression",
    "title": "The Bivariate Model",
    "section": "Matrices and Regression",
    "text": "Matrices and Regression\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \\nonumber \\\\\n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#check-this-out",
    "href": "bivariate24.html#check-this-out",
    "title": "The Bivariate Model",
    "section": "check this out",
    "text": "check this out\nagain: \\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \\nonumber \\\\\n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \\nonumber \\\\\n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#matrices-and-regression-1",
    "href": "bivariate24.html#matrices-and-regression-1",
    "title": "The Bivariate Model",
    "section": "Matrices and Regression",
    "text": "Matrices and Regression\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \\nonumber \\\\\n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \\nonumber \\\\\n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \\nonumber \\\\\n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\n\\[\n\\begin{align}\nE(\\mathbf{\\epsilon\\epsilon'})= E \\left[\n\\begin{array}{c}\n\\epsilon_{1} \\\\\n\\vdots \\\\\n\\epsilon_{n}\\\\\n\\end{array} \\right]\nE \\left[\n\\begin{array}{cccc}\n\\epsilon_{1} & \\cdots &\\epsilon_{n}\\\\\n\\end{array} \\right] \\\\ ~\\\\\n= E\n\\left[\n\\begin{array}{cccc}\n\\epsilon_{1}^{2} & \\epsilon_{1}\\epsilon_{2} &\\cdots &\\epsilon_{1}\\epsilon_{n}\\\\\n\\epsilon_{2}\\epsilon_{1}& \\epsilon_{2}^{2} &\\cdots &\\epsilon_{2}\\epsilon_{n}\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n\\epsilon_{n}\\epsilon_{1} &\\epsilon_{n}\\epsilon_{2} &\\cdots & \\epsilon_{n}^{2}\\\\\n\\end{array} \\right]\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon-1",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[\nE(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0\\\\\n0&\\sigma^{2} &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & \\sigma^{2}\\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0\\\\\n0&1 &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & 1\\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I}\n\\]\nThis is the variance-covariance matrix of the disturbances (var-cov(e)); it is symmetric, the main diagonal containing the variances, the off-diagonal containing the covariances."
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\nThe variance-covariance of the estimate, \\(\\widehat{\\beta}\\) derives as follows:\n\\[\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'y}\\]\nsubstituting \\(\\mathbf{X\\beta}+\\epsilon\\) for \\(\\mathbf{y}\\), we get\n\\[\n\\begin{align}\n\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'(X\\beta + \\epsilon)}  \\nonumber \\\\  \n= \\mathbf{(X'X)^{-1}}\\mathbf{X'X\\beta}+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\  \n=\\beta+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\\n\\widehat{\\mathbf{\\beta}}-\\beta=\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta-1",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-of-widehatbeta-2",
    "href": "bivariate24.html#variance-covariance-of-widehatbeta-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] = \\] \\(~\\)\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k)\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "bivariate24.html#standard-errors-of-beta_k",
    "href": "bivariate24.html#standard-errors-of-beta_k",
    "title": "The Bivariate Model",
    "section": "Standard Errors of \\(\\beta_k\\)",
    "text": "Standard Errors of \\(\\beta_k\\)\n\\[\n\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & \\sqrt{var(\\beta_k)}\\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#property-1",
    "href": "bivariate24.html#property-1",
    "title": "The Bivariate Model",
    "section": "Property 1",
    "text": "Property 1\nIf \\(X' \\widehat{\\epsilon} = 0\\) holds, then the following properties exist:\n\n\n\n\n\n\nProposition\n\n\n\nEach \\(x\\) variable (each column vector of \\(\\mathbf{X}\\)) is uncorrelated with \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#property-2",
    "href": "bivariate24.html#property-2",
    "title": "The Bivariate Model",
    "section": "Property 2",
    "text": "Property 2\nAssuming a constant in the matrix \\(\\mathbf{X}\\),\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\sum\\limits_{i-1}^{n} \\epsilon_i = 0\\)\nbecause each element of the matrix \\(X' \\widehat{\\epsilon}\\) would be nonzero due to the constant; only by multiplying by \\(\\epsilon_i=0\\) would each element equal zero, and then the sum must be zero."
  },
  {
    "objectID": "bivariate24.html#property-3",
    "href": "bivariate24.html#property-3",
    "title": "The Bivariate Model",
    "section": "Property 3",
    "text": "Property 3\n\n\n\n\n\n\nProposition\n\n\n\nThe mean of the residuals is zero.\nIf the sum of the residuals is zero, that sum divided by \\(N\\) must also be zero."
  },
  {
    "objectID": "bivariate24.html#property-4",
    "href": "bivariate24.html#property-4",
    "title": "The Bivariate Model",
    "section": "Property 4",
    "text": "Property 4\n\n\n\n\n\n\nProposition\n\n\n\nThe regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, \\(\\mathbf{X}\\) and \\(y\\).\n\\[\n\\begin{align}\n\\epsilon = y - X\\widehat{\\beta} \\nonumber \\\\ \\\\\n\\text{multiplying by} ~  N^{-1} \\nonumber \\\\ \\\\\n\\bar{\\epsilon} = \\bar{y} - \\bar{X} \\widehat{\\beta} = 0  \\nonumber\n\\end{align}\n\\]\n\\(\\bar{y} - \\bar{X} \\widehat{\\beta}=0\\) implies \\(\\bar{y} = \\bar{X} \\widehat{\\beta}\\), and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane \\(\\bar{y} - \\bar{X} \\widehat{\\beta}\\), the mean of the residuals equals zero, implying the regression line or hyperplane passes through it."
  },
  {
    "objectID": "bivariate24.html#linear-predictions",
    "href": "bivariate24.html#linear-predictions",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\)."
  },
  {
    "objectID": "bivariate24.html#residuals",
    "href": "bivariate24.html#residuals",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24.html#in-matrix-notation",
    "href": "bivariate24.html#in-matrix-notation",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Restating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#the-equation",
    "href": "bivariate24.html#the-equation",
    "title": "The Bivariate Model",
    "section": "The equation",
    "text": "The equation\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#mathbfxx",
    "href": "bivariate24.html#mathbfxx",
    "title": "The Bivariate Model",
    "section": "\\(\\mathbf{X'X}\\)",
    "text": "\\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#the-components",
    "href": "bivariate24.html#the-components",
    "title": "The Bivariate Model",
    "section": "The components",
    "text": "The components\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24.html#components---covariation-mathbfxx",
    "href": "bivariate24.html#components---covariation-mathbfxx",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'X}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24.html#components---covariation-mathbfxy",
    "href": "bivariate24.html#components---covariation-mathbfxy",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'y}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'y}\\)\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation",
    "href": "bivariate24.html#ols-matrix-derivation",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation-1",
    "href": "bivariate24.html#ols-matrix-derivation-1",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\   \n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "tdata.html#data-generating-process-1",
    "href": "tdata.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinkingdata24.html",
    "href": "thinkingdata24.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata24.html#get-to-know-your-data-explore-etc",
    "href": "thinkingdata24.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "Anscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "thinkingdata24.html#data-generating-process",
    "href": "thinkingdata24.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "thinkingdata24.html#data-generating-process-1",
    "href": "thinkingdata24.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "thinkingdata24.html#a-terrible-map",
    "href": "thinkingdata24.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "thinkingdata24.html#war-outcomes",
    "href": "thinkingdata24.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "thinkingdata24.html#observability",
    "href": "thinkingdata24.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "thinkingdata24.html#asking-the-right-question",
    "href": "thinkingdata24.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "thinkingdata24.html#data",
    "href": "thinkingdata24.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "thinkingdata24.html#types-of-variables",
    "href": "thinkingdata24.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "thinkingdata24.html#discrete-variables",
    "href": "thinkingdata24.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "thinkingdata24.html#continuous-variables",
    "href": "thinkingdata24.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "thinkingdata24.html#levels-of-measurement",
    "href": "thinkingdata24.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "thinkingdata24.html#levels-of-measurement-and-models",
    "href": "thinkingdata24.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "thinkingdata24.html#describe-these-data",
    "href": "thinkingdata24.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "thinkingdata24.html#describe-these-data-1",
    "href": "thinkingdata24.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "thinkingdata24.html#overstaying-terms",
    "href": "thinkingdata24.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "probability24s.html",
    "href": "probability24s.html",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns.\n\n\n\n\nProbability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value).\n\n\n\n\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita.\n\n\n\n\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely.\n\n\n\n\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war.\n\n\n\n\nThese four levels or measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information.\n\n\n\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "multivariate24.html",
    "href": "multivariate24.html",
    "title": "The Multivariate Model",
    "section": "",
    "text": "The Multivariate Model\nThe key issue in the multivariate regression is statistical control - our effort to mimic experimental conditions.\n\n\nExperimental Control\nIn an experiment, the research randomizes on all variables except the variable of interest (the treatment). This means the sample and all conditions the subjects in the sample face are random, or are explicitly fixed by the researcher. For example, in a diet pill study, the sample will be randomized with respect to demographics, health, weight, etc. The subjects’ diets during the study will be fixed or controlled by the researcher so they all eat the same. The lone exception will the administration of the diet pill - the treatment group will get the pill, the control group will get a placebo.\n\n\nIn the regression setting\nOne of the reasons we use models like the linear regression is to mimic the conditions of an experiment. The model allows us to isolate the effect of \\(x_1\\) while accounting for the independent effect of \\(x_2\\) on \\(y\\). So we’re able to say ``the effect of \\(x_1\\), controlling for the influence of \\(x_2\\) is \\(\\beta_1\\).’’\n\n\nHolding constant at mean\nThis linear regression :\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2} x_2 + \\epsilon_i\\]\nis equivalent to this one ::\n\\[y_i = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\widehat{\\beta_2^*}( x_2-\\bar{x_2}) + \\epsilon_i\\]\nOnly the constant shifts (think back to linear transformations; \\(\\widehat{\\beta_0}\\) shifts by a factor of \\(-k\\widehat{\\beta}\\)).\n\n\nConstant at mean\nWhat this shows is that even without transforming \\(x_2\\), we are holding its effect constant at its mean while we estimate the effect of \\(x_1\\).\n\n\nConditional Means\nLet \\(D_1\\) be a binary variable:\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}D_1 + \\hat{\\beta_2} x_1 + \\epsilon_i\\]\nThe partial effect of \\(D_1\\) measures the difference between the means for \\(D_1=0,1\\), given the mean effect of \\(x_1\\).\nSuppose we estimate a model predicting anti-government protests, and we think the main predictor will be liberal political institutions, controlling for per capita wealth. We think as liberalism increases, so do protests; they decrease with authoritarianism. So the regression looks like this:\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nWhat is the effect of liberal political institutions controlling for GDP per capita?\nWhen we ``control’’ for a variable like GDP per capita, we are trying to evaluate two things:\n\nthe direct or immediate effect of institutions on protests -this is because liberalism might promote free assembly.\nthe indirect of effect of institutions on GDP per capita, and then on protests - this is because liberalism might promote economic growth and development, and thereby influence protests.\n\nIn our regression\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nthis means \\(\\hat{\\beta_1}\\) is the effect of liberalism that does not go through GDP (or any other control variable).\nFrom our regression,\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i + \\hat{\\beta_2} \\text{GDPpc}_i + \\epsilon_i\\]\nPolitical institutions influence GDP per capita:\n\\[\\text{GDPpc}_i = \\hat{\\gamma_0} + \\hat{\\gamma_1}\\text{Politics}_i  + \\upsilon_i\\]\nSubstituting,\n\\[\\text{Protests}_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Politics}_i +\\hat{\\beta_2}(\\hat{\\gamma_0} + \\hat{\\gamma_1}\\text{Politics}_i  + \\upsilon_i )+ \\epsilon_i\\]\nso, the partial effect of liberal institutions is\n\\[\\frac{\\partial(protests)}{\\partial(politics)} = \\hat{\\beta_1} \\]\nThe effect is partial because it excludes the effect of politics that runs through GDP per capita. Here’s the total effect:\n\\[\\frac{\\partial(protests)}{\\partial(politics)} = \\hat{\\beta_1} + \\hat{\\beta_2}\\hat{\\gamma_1}\\]\nNotice that if liberalism has {} on GDP, so \\(\\hat{\\gamma_1}=0\\), then there is no indirect effect and the partial and total effects are the same.\n\n\nCounterfactuals\nAnother way to think about statistical control is to think about counterfactuals we’d want to evaluate:\n\ndemocracy promotes protests; but what if the state is rich?\ndemocracy promotes protests; but what if the state is poor?\n\n\n\nRandomization\nThe intuition is the same as in the experimental ideal. In the experiment the control and treatment groups are randomized with respect to all things but the treatment itself. In the regression, we want observations randomized over dimensions like wealth (i.e., we have rich and poor, etc) and all other things except the treatment itself - liberal institutions. If it were possible, we might collect data on protests and GDP but for states that are otherwise exactly the same.\n\n\nOther ways to exert control\nCollecting data on protests and GDP for states that are otherwise exactly the same - this the foundation of what are called methods. Matching is aimed at mimicking experimental design, exerting control via sampling. is a related technique that estimates weights for the control so it more closely resembles the treatment group.\n\n\nFrisch-Waugh-Lovell Theorem\nIn the linear least squares regression \\(y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\epsilon\\) produces an estimate for \\(\\hat{\\beta_1}\\) that is the same as the estimate of \\(\\hat{\\beta_1^*}\\) produced by estimating the regression \\(y\\) on \\(x_1\\), saving the residuals \\(\\hat{r_1}\\), then regressing \\(x_1\\) on \\(x_2\\), computing the residuals, \\(\\hat{r_2}\\), and then regressing \\(\\hat{r_1}\\) on \\(\\hat{r_2}\\).\n\n\\(\\hat{r_1}\\) measures the part of \\(y\\) that is unrelated to \\(x_1\\).\nThe regression \\(x_1=\\beta_0+ \\beta_2 x_2\\) measures the overlapping (correlated) parts of \\(x_1\\) and \\(x_2\\).\n\\(\\hat{r_2}\\) measures the unrelated parts of \\(x_1\\) and \\(x_2\\), the part of \\(x_1\\) unrelated to \\(x_2\\).\n\\(\\hat{r_1}\\) measures the part of \\(x_2\\) that is totally independent of \\(x_1\\)\nthe regression \\(\\hat{r_1}=\\beta_0+ \\beta_1^* \\hat{r_2}\\) estimates \\(\\beta_1^*\\) which measures the effect of the part of \\(x_1\\) that is unrelated to \\(x_2\\).\n\\(\\beta_1^*\\) is the partial effect of \\(x_1\\); we measured that part of \\(x_1\\) using \\(\\hat{r_1}\\).\n\n\n\nRegression Anatomy\nOne final way to think about this is a variant on FWL called ``regression anatomy’’ by Angrist and Pischke in their terrific book Mostly Harmless Econometrics.\n\n\nAnatomy\nIn the bivariate case, the slope on \\(\\beta_1\\) is:\n\\[\\beta_1 = \\frac{cov(y_i, x_i)}{V(x_i)}\\]\nIn the multivariate case, the slope on \\(\\beta_1\\) is:\n\\[\\beta_1 = \\frac{cov(y_i, \\tilde{x_{k,i}})}{V(\\tilde{x_{k,i}}) }\\]\nwhere \\(\\tilde{x_{k,i}}\\) refers to the residuals from the regression of \\(x_{k,i}\\) on \\(\\mathbf{X}\\), where \\(X\\) is all other right-side variables.\n\nThe residual from that regression measures the part of \\(x\\) that is unrelated to \\(X\\).\nThe estimate of \\(\\beta_1\\) then is the correlation of \\(y\\) and the part of \\(x\\) that is unrelated to \\(X\\).\nSince \\(\\tilde{x_{k,i}}\\) is unrelated to \\(X\\), its correlation to \\(y\\) is purged of any part through \\(X\\), and is therefore partial.\n\n\n\nOmitted Variable Bias\nIn the population, the true regression is:\n\\[y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\widehat{\\beta_2}x_2 + \\varepsilon \\]\nIn our sample, the regression we estimate is:\n\\[y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_1 + \\varepsilon \\]\nSo the model omits a variable. What’s the effect?\n\n\nOmitted Variable Bias\n\nIn the bivariate regression, the estimate is \\(\\widehat{\\beta_1}\\)\nIn the multivariate regression, the estimate is \\(\\widetilde{\\beta_1} = \\widehat{\\beta_1} + \\widehat{\\beta_2}\\widetilde{\\delta_1}\\). That is, the estimate for \\(\\widetilde{\\beta_1}\\) now accounts for the relationship of \\(x_1,x_2\\), measured in \\(\\widetilde{\\delta_1}\\). \\(\\widetilde{\\beta_1}\\) now measures the partial effect of \\(x_1\\) on \\(y\\).\nIf we exclude \\(x_2\\), we misestimate \\(\\widehat{\\beta_1}\\) by \\(\\pm \\widehat{\\beta_2}\\widetilde{\\delta_1}\\)\n\n\n\n\nEvaluating an Estimator\nWhat criteria make for a “good” estimator?\n\nUnbiasedness:\n\n\nIs the expected value of \\(\\hat{\\beta}\\) equal to \\(\\beta\\)?\nHow far away do we expect \\(\\hat{\\beta}\\) to be from \\(\\beta\\)?\n\\(E[\\hat{\\beta} - \\beta]\\)\nThe estimator for which this quantity is smallest is the least biased.\n\nBias will always exist, but we want it to be as small as possible, and random (not systematic).\n\nEfficiency:\n\n\nEfficiency measures how close to the true \\(\\beta\\) we are {}.\nEfficiency describes the average size of bias; the average distance of \\(E[\\hat{\\beta} - \\beta]\\).\nThis is about the size of the variance; large variance estimates will, on average, miss the true \\(\\beta\\) by a lot. Small variance estimators will miss the true \\(\\beta\\) by a little, on average.\n\n\nConsistency:\n\n\nConsistency can be thought of as large-sample-unbiasedness.\n\\(E[\\hat{\\beta} - \\beta] \\rightarrow\\) as \\(N \\rightarrow \\infty\\)\nThis is considerably less important to us than bias and efficiency. In part, this is due to the reality that our (non experimental) data are small samples in many cases. It’s also true OLS has good small sample properties.\nMLE does not have good small sample properties, and relies strongly on consistency.\n\n\n\nAssumptions\nUnder the following four assumptions, the OLS estimator \\(\\hat{\\beta}\\) is an unbiased estimator of \\(\\beta\\):\n\nLinear in parameters.\nRandom Sampling.\nZero Conditional Mean of Disturbances.\nNo Perfect Collinearity.\n\n\n\n\n\n\n\nTheorem :Unbiasedness of OLS:\n\n\n\nUnder these four assumptions, OLS estimators are unbiased estimators of the population parameters:\n\\[\nE[{\\widehat{\\beta_{j}}]= \\beta_{j} ~\\forall ~ j} \\nonumber\n\\]\n\n\n\n\nOLS\nIf the model also meets these two assumptions, then the OLS estimator has the smallest variance of all estimators:\n\nHomoskedastic disturbances; \\(Var(u|x_1,x_2,\\ldots,x_k)=\\sigma^2\\).\nUncorrelated disturbances; \\(cov(u_i,u_j|x_1,x_2,\\ldots,x_k)=0\\).\n\nIf Assumptions 1-6 are all met, then the model is BLUE and thus satisfies the Gauss-Markov Theorem. Additionally, these assumptions provide us the first two moments of the sampling distribution for the \\(\\hat{\\beta}\\)s.\n\n\n\n\n\n\nTheorem: Gauss Markov\n\n\n\nUnder these assumptions, the OLS estimator is unbiased and has the smallest variance among all linear unbiased estimators.\n\n\n\n\nOLS\nHowever, in order to talk precisely about the uncertainty surrounding the point estimates, we need to make one more assumption about the error term:\n\nThe disturbances, \\(u_i\\) are independent of the \\(X\\)s and are normally distributed: \\(u \\sim N(0,\\sigma^{2})\\).\n\n\n\nIf assumptions fail \\(\\ldots\\)\n\nsample is not random - bias.\n\\(E[u | X] \\neq 0\\) - endogeneity; bias and standard errors are too small.\nperfect collinearity - matrix is singular, regression fails.\ncorrelated \\(X\\) variables (imperfect collinearity) - unbiased estimates, inefficient standard errors.\nerrors are not identically distributed (heteroskedastic) - inefficiency.\nerrors are not independently distributed (correlated errors) - inefficiency."
  },
  {
    "objectID": "bivariate24s.html#regression-1",
    "href": "bivariate24s.html#regression-1",
    "title": "The Bivariate Model",
    "section": "Regression",
    "text": "Regression\nLet \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "bivariate24s.html#linear-predictions",
    "href": "bivariate24s.html#linear-predictions",
    "title": "The Bivariate Model",
    "section": "Linear predictions",
    "text": "Linear predictions\nThe predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\)."
  },
  {
    "objectID": "bivariate24s.html#residuals",
    "href": "bivariate24s.html#residuals",
    "title": "The Bivariate Model",
    "section": "Residuals",
    "text": "Residuals\nThe differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24s.html#in-matrix-notation",
    "href": "bivariate24s.html#in-matrix-notation",
    "title": "The Bivariate Model",
    "section": "In matrix notation",
    "text": "In matrix notation\nRestating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-i",
    "href": "bivariate24s.html#getting-to-ols-i",
    "title": "The Bivariate Model",
    "section": "Getting to OLS I",
    "text": "Getting to OLS I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-1",
    "href": "bivariate24s.html#ols-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nTo minimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]\n\\(\\ldots\\) illustrating explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-1-1",
    "href": "bivariate24s.html#ols-1-1",
    "title": "The Bivariate Model",
    "section": "OLS 1",
    "text": "OLS 1\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-ii",
    "href": "bivariate24s.html#getting-to-ols-ii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS II",
    "text": "Getting to OLS II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero.\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling that the mean of a variable\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]\nSo, substituting these equations into\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\\]\nand solving for \\(\\hat{\\beta}_1\\) gives:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24s.html#getting-to-ols-iii",
    "href": "bivariate24s.html#getting-to-ols-iii",
    "title": "The Bivariate Model",
    "section": "Getting to OLS III",
    "text": "Getting to OLS III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24s.html#the-components",
    "href": "bivariate24s.html#the-components",
    "title": "The Bivariate Model",
    "section": "The components",
    "text": "The components\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]"
  },
  {
    "objectID": "bivariate24s.html#components---covariation-mathbfxx",
    "href": "bivariate24s.html#components---covariation-mathbfxx",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'X}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'X}\\)\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "bivariate24s.html#components---covariation-mathbfxy",
    "href": "bivariate24s.html#components---covariation-mathbfxy",
    "title": "The Bivariate Model",
    "section": "Components - Covariation \\(\\mathbf{X'y}\\)",
    "text": "Components - Covariation \\(\\mathbf{X'y}\\)\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation",
    "href": "bivariate24s.html#ols-matrix-derivation",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nNow that we’ve seen all the algebra and what the data matrix looks like, let’s revisit the derivation of \\(\\beta\\), beginning with the estimating equation:\n\\[\n\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation-1",
    "href": "bivariate24s.html#ols-matrix-derivation-1",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nand solve for \\(\\beta\\) by minimizing the sum of the squared errors:\n\\[\n\\begin{align}\n\\sum\\limits_{i=1}^{n} \\epsilon^2= \\widehat{\\epsilon}'\\widehat{\\epsilon} \\nonumber \\\\  \n= (\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})'(\\mathbf{y}-\\mathbf{X}\\widehat{\\beta})  \\nonumber \\\\   \n=\\mathbf{y'y}-\\mathbf{y}'\\mathbf{X}\\widehat{\\beta}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}-\\widehat{\\beta}'\\mathbf{X}'\\mathbf{y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta}  \\nonumber \\\\  \n=\\mathbf{y'y}-2\\widehat{\\beta}'\\mathbf{X'y}+\\widehat{\\beta}'\\mathbf{X'X}\\widehat{\\beta} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\n\\[\n\\begin{align}\nE(\\mathbf{\\epsilon\\epsilon'})= E \\left[\n\\begin{array}{c}\n\\epsilon_{1} \\\\\n\\vdots \\\\\n\\epsilon_{n}\\\\\n\\end{array} \\right]\nE \\left[\n\\begin{array}{cccc}\n\\epsilon_{1} & \\cdots &\\epsilon_{n}\\\\\n\\end{array} \\right] \\\\ ~\\\\\n= E\n\\left[\n\\begin{array}{cccc}\n\\epsilon_{1}^{2} & \\epsilon_{1}\\epsilon_{2} &\\cdots &\\epsilon_{1}\\epsilon_{n}\\\\\n\\epsilon_{2}\\epsilon_{1}& \\epsilon_{2}^{2} &\\cdots &\\epsilon_{2}\\epsilon_{n}\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n\\epsilon_{n}\\epsilon_{1} &\\epsilon_{n}\\epsilon_{2} &\\cdots & \\epsilon_{n}^{2}\\\\\n\\end{array} \\right]\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon-1",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nBecause we assume constant variance and no serial correlation, we know\n\\[\nE(\\mathbf{\\epsilon\\epsilon'})=\n\\left[\n\\begin{array}{cccc}\n\\sigma^{2} & 0 &\\cdots &0\\\\\n0&\\sigma^{2} &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & \\sigma^{2}\\\\\n\\end{array} \\right]\n= \\sigma^{2}\n\\left[\n\\begin{array}{cccc}\n1 & 0 &\\cdots &0\\\\\n0&1 &\\cdots &0\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\n0 &0 &\\cdots & 1\\\\\n\\end{array} \\right]\n= \\sigma^{2} \\mathbf{I}\n\\]\nThis is the variance-covariance matrix of the disturbances (var-cov(e)); it is symmetric, the main diagonal containing the variances, the off-diagonal containing the covariances."
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\nThe variance-covariance of the estimate, \\(\\widehat{\\beta}\\) derives as follows:\n\\[\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'y}\\]\nsubstituting \\(\\mathbf{X\\beta}+\\epsilon\\) for \\(\\mathbf{y}\\), we get\n\\[\n\\begin{align}\n\\widehat{\\mathbf{\\beta}}=\\mathbf{(X'X)^{-1}} \\mathbf{X'(X\\beta + \\epsilon)}  \\nonumber \\\\  \n= \\mathbf{(X'X)^{-1}}\\mathbf{X'X\\beta}+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\  \n=\\beta+\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber \\\\\n\\widehat{\\mathbf{\\beta}}-\\beta=\\mathbf{(X'X)^{-1}}\\mathbf{X'\\epsilon} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta-1",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta-1",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[\n\\begin{align}\nE[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)']  \n= E[(\\mathbf{(X'X)^{-1}X'\\epsilon})(\\mathbf{(X'X)^{-1}X'\\epsilon})']  \\nonumber \\\\ \\nonumber \\\\\n= E[\\mathbf{(X'X)^{-1}X'\\epsilon \\epsilon'\\mathbf{X(X'X)^{-1}}}]  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} E(\\epsilon \\epsilon')\\mathbf{X(X'X)^{-1}}  \\nonumber \\\\ \\nonumber \\\\\n= \\mathbf{(X'X)^{-1}X'} \\sigma^{2}\\mathbf{I} \\mathbf{X(X'X)^{-1}}   \\nonumber \\\\ \\nonumber \\\\\n= \\sigma^{2} \\mathbf{(X'X)^{-1}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-of-widehatbeta-2",
    "href": "bivariate24s.html#variance-covariance-of-widehatbeta-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] = \\] \\(~\\)\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k)\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "bivariate24s.html#standard-errors-of-beta_k",
    "href": "bivariate24s.html#standard-errors-of-beta_k",
    "title": "The Bivariate Model",
    "section": "Standard Errors of \\(\\beta_k\\)",
    "text": "Standard Errors of \\(\\beta_k\\)\n\\[\n\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & \\sqrt{var(\\beta_k)}\\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "bivariate24s.html#property-1",
    "href": "bivariate24s.html#property-1",
    "title": "The Bivariate Model",
    "section": "Property 1",
    "text": "Property 1\nIf \\(X' \\widehat{\\epsilon} = 0\\) holds, then the following properties exist:\n\n\n\nProposition\n\n\nEach \\(x\\) variable (each column vector of \\(\\mathbf{X}\\)) is uncorrelated with \\(\\epsilon\\)."
  },
  {
    "objectID": "bivariate24s.html#property-2",
    "href": "bivariate24s.html#property-2",
    "title": "The Bivariate Model",
    "section": "Property 2",
    "text": "Property 2\nAssuming a constant in the matrix \\(\\mathbf{X}\\),\n\n\n\nProposition\n\n\n\\(\\sum\\limits_{i-1}^{n} \\epsilon_i = 0\\)\nbecause each element of the matrix \\(X' \\widehat{\\epsilon}\\) would be nonzero due to the constant; only by multiplying by \\(\\epsilon_i=0\\) would each element equal zero, and then the sum must be zero."
  },
  {
    "objectID": "bivariate24s.html#property-3",
    "href": "bivariate24s.html#property-3",
    "title": "The Bivariate Model",
    "section": "Property 3",
    "text": "Property 3\n\n\n\nProposition\n\n\nThe mean of the residuals is zero.\nIf the sum of the residuals is zero, that sum divided by \\(N\\) must also be zero."
  },
  {
    "objectID": "bivariate24s.html#property-4",
    "href": "bivariate24s.html#property-4",
    "title": "The Bivariate Model",
    "section": "Property 4",
    "text": "Property 4\n\n\n\nProposition\n\n\nThe regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, \\(\\mathbf{X}\\) and \\(y\\).\n\\[\n\\begin{align}\n\\epsilon = y - X\\widehat{\\beta} \\nonumber \\\\ \\\\\n\\text{multiplying by} ~  N^{-1} \\nonumber \\\\ \\\\\n\\bar{\\epsilon} = \\bar{y} - \\bar{X} \\widehat{\\beta} = 0  \\nonumber\n\\end{align}\n\\]\n\\(\\bar{y} - \\bar{X} \\widehat{\\beta}=0\\) implies \\(\\bar{y} = \\bar{X} \\widehat{\\beta}\\), and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane \\(\\bar{y} - \\bar{X} \\widehat{\\beta}\\), the mean of the residuals equals zero, implying the regression line or hyperplane passes through it."
  },
  {
    "objectID": "bivariate24.html#derivation-i",
    "href": "bivariate24.html#derivation-i",
    "title": "The Bivariate Model",
    "section": "Derivation I",
    "text": "Derivation I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i",
    "href": "bivariate24.html#ols-i",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nMinimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-1",
    "href": "bivariate24.html#ols-i-1",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-2",
    "href": "bivariate24.html#ols-i-2",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nThis shows explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24.html#derivation-ii",
    "href": "bivariate24.html#derivation-ii",
    "title": "The Bivariate Model",
    "section": "Derivation II",
    "text": "Derivation II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii",
    "href": "bivariate24.html#ols-ii",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-1",
    "href": "bivariate24.html#ols-ii-1",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-2",
    "href": "bivariate24.html#ols-ii-2",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero."
  },
  {
    "objectID": "bivariate24.html#ols-ii-3",
    "href": "bivariate24.html#ols-ii-3",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-4",
    "href": "bivariate24.html#ols-ii-4",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling the mean of a variable is\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-5",
    "href": "bivariate24.html#ols-ii-5",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-6",
    "href": "bivariate24.html#ols-ii-6",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24.html#properties",
    "href": "bivariate24.html#properties",
    "title": "The Bivariate Model",
    "section": "Properties",
    "text": "Properties\nNote that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about \\(\\widehat{\\beta}\\), and inferences about those estimates."
  },
  {
    "objectID": "bivariate24s.html#derivation-i",
    "href": "bivariate24s.html#derivation-i",
    "title": "The Bivariate Model",
    "section": "Derivation I",
    "text": "Derivation I\n\\[\ny_{i}=\\beta_{0}+\\beta_{1}X_{1} + u \\nonumber\n\\]\n\\(\\ldots\\) over the sample, \\(N\\), sum of squares of the deviations, \\(\\widehat{S}\\) \\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})^{2} \\nonumber\n\\]\n\\[\n\\widehat{S} = \\sum \\limits_{i=1}^{n} (y_{i}^{2}-2y_i \\widehat{\\beta_{0}}- 2y_i\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{0}}^{2}  \\nonumber \\\\\n+2\\widehat{\\beta_{0}}\\widehat{\\beta_{1}}X_{i} + \\widehat{\\beta_{1}^{2}}X_{i}^{2} ) \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i",
    "href": "bivariate24s.html#ols-i",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nMinimize \\(S\\) with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{0}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})  \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-1",
    "href": "bivariate24s.html#ols-i-1",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd do the same with respect to \\(\\beta_{1}\\):\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{S}}}{\\partial{\\widehat{\\beta_{1}}}} = -2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}) X_{i} \\nonumber \\\\\n=-2 \\sum \\limits_{i=1}^{n} (y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{i})X_{i} \\nonumber \\\\\n= -2 \\sum \\limits_{i=1}^{n}  \\hat{u_i}  X_i\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-2",
    "href": "bivariate24s.html#ols-i-2",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nThis shows explicitly that the minimum of the sum of squares is a function of the deviations of predictions from observed:\n\\[y_{i}-\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{i}\\]\n\\[\\mathbf{y} - \\mathbf{X}\\widehat{\\beta}\\]\n\\[\\mathbf{y} - \\mathbf{\\widehat{y}}\\]\n\\[\\mathbf{\\widehat{u}}\\]"
  },
  {
    "objectID": "bivariate24s.html#derivation-ii",
    "href": "bivariate24s.html#derivation-ii",
    "title": "The Bivariate Model",
    "section": "Derivation II",
    "text": "Derivation II\nHere’s another way to achieve the same thing, but for illustration purposes, a second time around might be useful. Suppose in the population, we have the equation\n\\[\ny= \\beta_{0}+\\beta_{1}x_{i} + e_{i} \\nonumber\n\\]\nAssume the mean of \\(e\\) to be zero and that \\(x\\) and \\(e\\) are uncorrelated, so\n\\[E[e]=0 \\nonumber\n\\]\n\\[\nCov[x,e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii",
    "href": "bivariate24s.html#ols-ii",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThis is equivalent to saying the expected value of the product of \\(x\\) and \\(e\\) is zero:\n\\[E[x \\cdot e]=0 \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-1",
    "href": "bivariate24s.html#ols-ii-1",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, let’s solve the original population function for \\(e\\):\n\\[\n\\begin{align}\ny=\\beta_0 + \\beta_1 x_i+ e_i \\nonumber \\\\ \\nonumber \\\\\ne_i = y-\\beta_0 - \\beta_1 x_i  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-2",
    "href": "bivariate24s.html#ols-ii-2",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand then restate these assumptions in terms of \\(y\\):\n\\[\n\\begin{align}\nE[y-\\beta_0 - \\beta_1 x_i]=0    \\nonumber \\\\  \\nonumber \\\\\nCov[x,(y-\\beta_0 - \\beta_1 x_i)]=0 \\nonumber\n\\end{align}\n\\]\nwhere the first expects the mean of the disturbances to be zero and second expects the covariance of the \\(X\\)s and the disturbances to be zero."
  },
  {
    "objectID": "bivariate24s.html#ols-ii-3",
    "href": "bivariate24s.html#ols-ii-3",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSince we want estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for a sample (so \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), state these expectations with respect to a sample (of size \\(n\\)):\n\\[\n\\begin{align}\n\\frac{\\sum\\limits_{i=1}^{n}(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0   \\nonumber \\\\ \\nonumber \\\\\n\\frac{\\sum\\limits_{i=1}^{n}x(y-\\hat{\\beta_0} - \\hat{\\beta_1} x_i)}{n}=0 \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-4",
    "href": "bivariate24s.html#ols-ii-4",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nThese first order conditions are equivalent to those in the derivation above where we solved them by taking the partial derivatives of the sum of squares with respect to the unknowns.\nRecalling the mean of a variable is\n\\[\\bar{y}=\\frac{\\sum\\limits_{i=1}^{n}y_i}{n}\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-5",
    "href": "bivariate24s.html#ols-ii-5",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nwe can write each of these in terms of the means of \\(x\\) and \\(y\\) - first, here is the mean zero statement (from above):\n\\[\n\\begin{align}\n\\bar{y}-\\hat{\\beta_0} - \\hat{\\beta_1} \\bar{x}=0 \\nonumber \\\\ \\nonumber \\\\\n\\bar{y}=\\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-6",
    "href": "bivariate24s.html#ols-ii-6",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nand we can rearrange to solve for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta_1} \\bar{x} \\nonumber\n\\]"
  },
  {
    "objectID": "bivariate24s.html#properties",
    "href": "bivariate24s.html#properties",
    "title": "The Bivariate Model",
    "section": "Properties",
    "text": "Properties\nNote that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about \\(\\widehat{\\beta}\\), and inferences about those estimates."
  },
  {
    "objectID": "bivariate24.html#ols-i-3",
    "href": "bivariate24.html#ols-i-3",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-3",
    "href": "bivariate24s.html#ols-i-3",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSubstituting the unknowns back in, setting equal to zero, and rearranging gives the “normal equations”:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\beta_{0}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-4",
    "href": "bivariate24.html#ols-i-4",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-4",
    "href": "bivariate24s.html#ols-i-4",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nSolving, we get the estimating equations, here for \\(\\widehat{\\beta_0}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}y_{i}= n \\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\nn \\widehat{\\beta_0} =  \\sum \\limits_{i=1}^{n}y_{i} - \\widehat{\\beta_{1}}\\sum \\limits_{i=1}^{n}X_{i} \\nonumber \\\\\n\\widehat{\\beta_0} =  \\bar{y} - \\widehat{\\beta_{1}}\\bar{X_{i}} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-5",
    "href": "bivariate24.html#ols-i-5",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-i-5",
    "href": "bivariate24s.html#ols-i-5",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nAnd here, for \\(\\widehat{\\beta_1}\\):\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}X_{i}y_{i} = \\beta_{0}\\sum \\limits_{i=1}^{n}X_{i}+\\beta_{1}\\sum \\limits_{i=1}^{n}X_{i}^{2}\\nonumber \\\\\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-i-6",
    "href": "bivariate24.html#ols-i-6",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24.html#ols-ii-7",
    "href": "bivariate24.html#ols-ii-7",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-8",
    "href": "bivariate24.html#ols-ii-8",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-9",
    "href": "bivariate24.html#ols-ii-9",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSo, substituting and solving for \\(\\hat{\\beta}_1\\) gives:\n\n\n\n\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#ols-ii-10",
    "href": "bivariate24.html#ols-ii-10",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24.html#derivation-iii",
    "href": "bivariate24.html#derivation-iii",
    "title": "The Bivariate Model",
    "section": "Derivation III",
    "text": "Derivation III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24.html#ols-matrix-derivation-2",
    "href": "bivariate24.html#ols-matrix-derivation-2",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24.html#variance-covariance-matrix-of-epsilon-2",
    "href": "bivariate24.html#variance-covariance-matrix-of-epsilon-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nThis matrix is important because it gives us our estimate of \\(\\sigma^2\\). This is important enough to merit its own slide."
  },
  {
    "objectID": "bivariate24s.html#ols-i-6",
    "href": "bivariate24s.html#ols-i-6",
    "title": "The Bivariate Model",
    "section": "OLS I",
    "text": "OLS I\nNote the parts of \\(\\widehat{\\beta_1}\\):\n\\[\n\\widehat{\\beta_{1}} = \\frac{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(X_{i}-\\bar{X})^2} \\nonumber\n\\]\nThe numerator is the covariance of \\(X\\) and \\(y\\) - the denominator is the variance of \\(X\\). The estimated coefficient \\(\\widehat{\\beta_{1}}\\) is the covariance of (X,y) weighted by the variance in \\(X\\). \\(\\widehat{\\beta_{1}}\\) measures the estimated change in \\(y\\) given a one-unit change in \\(X\\)."
  },
  {
    "objectID": "bivariate24s.html#ols-ii-7",
    "href": "bivariate24s.html#ols-ii-7",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nNow, rewrite the second assumption (\\(E[x \\cdot e]=0\\)) in the same way:\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i[y_i-(\\bar{y}-\\hat{\\beta_{1}}\\bar{x})-\\hat{\\beta_{1}}x_{i}] =0  \\nonumber \\\\ \\text{and rearrange,} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-8",
    "href": "bivariate24s.html#ols-ii-8",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nBecause of the properties of the summation operator,\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2  \\nonumber \\\\ \\text{and} \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) = \\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-9",
    "href": "bivariate24s.html#ols-ii-9",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nSo, substituting and solving for \\(\\hat{\\beta}_1\\) gives:\n\n\n\n\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}x_i(y_i-\\bar{y}) =\\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}x_i(x_i-\\bar{x}) \\nonumber \\\\\n\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})= \\hat{\\beta_{1}}\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2 \\nonumber \\\\\n\\hat{\\beta_{1}}=\\frac{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum \\limits_{i=1}^{n}(x_i-\\bar{x})^2} \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-ii-10",
    "href": "bivariate24s.html#ols-ii-10",
    "title": "The Bivariate Model",
    "section": "OLS II",
    "text": "OLS II\nWhy can we drop \\(n\\) from these calculations? Recall that\n\\[\n\\begin{align}\n\\sum \\limits_{i=1}^{n}\\widehat{e_{i}} = 0 \\nonumber \\\\\n\\text{so} \\nonumber \\\\\n\\frac{\\sum \\limits_{i=1}^{n}\\widehat{e_{i}}}{n} = 0 \\nonumber\n\\end{align}\n\\]\nThe same is true for the covariance assumption."
  },
  {
    "objectID": "bivariate24s.html#derivation-iii",
    "href": "bivariate24s.html#derivation-iii",
    "title": "The Bivariate Model",
    "section": "Derivation III",
    "text": "Derivation III\nOne more time, but now in matrix form:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nSkipping a lot of steps we’ll cover soon, solve for\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\]"
  },
  {
    "objectID": "bivariate24s.html#ols-matrix-derivation-2",
    "href": "bivariate24s.html#ols-matrix-derivation-2",
    "title": "The Bivariate Model",
    "section": "OLS matrix derivation",
    "text": "OLS matrix derivation\nDifferentiating with respect to \\(\\widehat{\\beta}\\), and setting equal to zero,\n\\[\n\\begin{align}\n\\frac{\\partial{\\widehat{\\epsilon}'\\widehat{\\epsilon}}}{\\partial{\\widehat{\\beta}}} = -2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X}'\\mathbf{X}\\widehat{\\beta} \\nonumber \\\\  \n-2\\mathbf{X}'\\mathbf{y}+2\\mathbf{X'X}\\widehat{\\beta}=0  \\nonumber \\\\  \n\\mathbf{X'X}\\widehat{\\beta}=\\mathbf{X'y}  \\nonumber \\\\\n(\\mathbf{X'X})^{-1}\\mathbf{X'X}\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}   \\nonumber \\\\  \n\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "bivariate24s.html#variance-covariance-matrix-of-epsilon-2",
    "href": "bivariate24s.html#variance-covariance-matrix-of-epsilon-2",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix of \\(\\epsilon\\)",
    "text": "Variance-Covariance Matrix of \\(\\epsilon\\)\nThis matrix is important because it gives us our estimate of \\(\\sigma^2\\). This is important enough to merit its own slide."
  },
  {
    "objectID": "bivariate24.html#aside-on-sigma2",
    "href": "bivariate24.html#aside-on-sigma2",
    "title": "The Bivariate Model",
    "section": "Aside on \\(\\sigma^2\\)",
    "text": "Aside on \\(\\sigma^2\\)\n\\(\\widehat{\\sigma^2}\\) is our estimate of the error variance.\n\\[\\widehat{\\sigma^2} = (u'u) \\frac{1}{n-k-1}\\] This is the sum squared error (inner product of the residuals) distributed across our degrees of freedom. You should see that small samples will have smaller denominators, so potentially larger error variance."
  },
  {
    "objectID": "bivariate24.html#average-error",
    "href": "bivariate24.html#average-error",
    "title": "The Bivariate Model",
    "section": "Average error",
    "text": "Average error\n\\[\\widehat{\\sigma} = \\sqrt{\\widehat{\\sigma^2}}\\] The square root of \\(\\widehat{\\sigma^2}\\) gives us the the residual standard error also called the root mean squared error (RMSE) - it is in the same metric as \\(y\\), so is useful as an indicator of the average error in the regression."
  },
  {
    "objectID": "bivariate24s.html#aside-on-sigma2",
    "href": "bivariate24s.html#aside-on-sigma2",
    "title": "The Bivariate Model",
    "section": "Aside on \\(\\sigma^2\\)",
    "text": "Aside on \\(\\sigma^2\\)\n\\(\\widehat{\\sigma^2}\\) is our estimate of the error variance.\n\\[\\widehat{\\sigma^2} = (u'u) \\frac{1}{n-k-1}\\] This is the sum squared error (inner product of the residuals) distributed across our degrees of freedom. You should see that small samples will have smaller denominators, so potentially larger error variance."
  },
  {
    "objectID": "bivariate24s.html#average-error",
    "href": "bivariate24s.html#average-error",
    "title": "The Bivariate Model",
    "section": "Average error",
    "text": "Average error\n\\[\\widehat{\\sigma} = \\sqrt{\\widehat{\\sigma^2}}\\] The square root of \\(\\widehat{\\sigma^2}\\) gives us the the residual standard error also called the root mean squared error (RMSE) - it is in the same metric as \\(y\\), so is useful as an indicator of the average error in the regression."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "exercise #1 answers\nexercise #2 answers\nbasic linear model"
  },
  {
    "objectID": "ex1answers2024.html",
    "href": "ex1answers2024.html",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\npdf-cdf"
  },
  {
    "objectID": "ex1answers2024.html#q1",
    "href": "ex1answers2024.html#q1",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\npdf-cdf"
  },
  {
    "objectID": "ex1answers2024.html#q2",
    "href": "ex1answers2024.html#q2",
    "title": "exercise #1 answers",
    "section": "Q2",
    "text": "Q2\n\n\ncode\nxb &lt;- runif(1000, min=-4, max=4)\npdf1 &lt;- dnorm(xb, mean=0, sd=1)\npdf2 &lt;- dnorm(xb, mean=0, sd=sqrt(.5))\npdf3 &lt;- dnorm(xb, mean=-1, sd=sqrt(1.5))\npdf4 &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, pdf1, pdf2, pdf3, pdf4)\n\n\n\nggplot(data=df, aes(x=xb, y=pdf1)) +\n  geom_line() +\n  geom_line(aes(y=pdf2), linetype=\"dotted\") +\n  geom_line(aes(y=pdf3), linetype=\"longdash\" ) +\n  annotate(\"text\", x = 2.5, y = .1, label = \"Normal (0,1)\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"Normal (0, .5)\") +\n  annotate(\"text\", x = -3.3, y = .2, label = \"Normal (-1, 1.5)\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal PDFs\")"
  },
  {
    "objectID": "ex1answers2024.html#dance-model",
    "href": "ex1answers2024.html#dance-model",
    "title": "exercise #1 answers",
    "section": "Dance model",
    "text": "Dance model\nSimple regression - is one artist’s music more danceable than the other - dummy variable for the Rolling Stones.\n\n\ncode\nd &lt;- summary(lm(data=bsum, danceability ~ as.factor(artist_name)))\nmodelsummary(d, stars=TRUE)\n\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n0.593***\n\n\n\n(0.004)\n\n\nas.factor(artist_name)The Rolling Stones\n−0.124***\n\n\n\n(0.005)\n\n\nNum.Obs.\n2875\n\n\nR2\n0.186\n\n\nR2 Adj.\n0.185\n\n\nRMSE\n0.13\n\n\n\n + p\n\n\n\n\n\n\n\n\n\nThe Stones are statistically less danceable than Taylor Swift is.\n\nDanceability over time\nDoes danceability change over time? Let’s use dummmies for years to relax linearity.\n\n\ncode\n# modeling danceability over time for each artist\n\n#taylor swift\ntsreg &lt;- summary(lm(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), danceability ~ as.factor(album_release_year)))\nmodelsummary(tsreg, stars=TRUE)\n\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n0.571***\n\n\n\n(0.011)\n\n\nas.factor(album_release_year)2008\n0.016\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2010\n−0.015\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2012\n0.062***\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2014\n0.066***\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2015\n0.044\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2017\n0.064***\n\n\n\n(0.016)\n\n\nas.factor(album_release_year)2018\n0.031\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2019\n0.087**\n\n\n\n(0.027)\n\n\nas.factor(album_release_year)2020\n−0.026+\n\n\n\n(0.014)\n\n\nas.factor(album_release_year)2021\n−0.015\n\n\n\n(0.015)\n\n\nas.factor(album_release_year)2022\n0.062***\n\n\n\n(0.017)\n\n\nNum.Obs.\n1265\n\n\nR2\n0.115\n\n\nR2 Adj.\n0.108\n\n\nRMSE\n0.10\n\n\n\n + p\n\n\n\n\n\n\n\n\n\nCompared to her first release, it appears TS has gotten more danceable since 2012, though her music varies considerably.\n\n\ncode\n#stones\nrsreg &lt;- summary(lm(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), danceability ~ as.factor(album_release_year)))\nmodelsummary(rsreg, stars=TRUE)\n\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n0.599***\n\n\n\n(0.016)\n\n\nas.factor(album_release_year)1965\n−0.033+\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)1966\n−0.105***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)1967\n−0.085***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)1968\n−0.101**\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1969\n−0.105***\n\n\n\n(0.028)\n\n\nas.factor(album_release_year)1970\n−0.135***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)1971\n−0.150***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)1972\n−0.139***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)1973\n−0.148***\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1974\n−0.067*\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1976\n0.006\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)1977\n−0.168***\n\n\n\n(0.026)\n\n\nas.factor(album_release_year)1978\n−0.026\n\n\n\n(0.023)\n\n\nas.factor(album_release_year)1980\n0.006\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1981\n−0.050\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1982\n−0.240***\n\n\n\n(0.039)\n\n\nas.factor(album_release_year)1983\n−0.017\n\n\n\n(0.032)\n\n\nas.factor(album_release_year)1986\n−0.052+\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1989\n−0.069*\n\n\n\n(0.030)\n\n\nas.factor(album_release_year)1991\n−0.176***\n\n\n\n(0.026)\n\n\nas.factor(album_release_year)1994\n−0.113**\n\n\n\n(0.035)\n\n\nas.factor(album_release_year)1995\n−0.069*\n\n\n\n(0.028)\n\n\nas.factor(album_release_year)1997\n−0.080**\n\n\n\n(0.029)\n\n\nas.factor(album_release_year)2004\n−0.250***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)2005\n−0.044\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)2011\n−0.308***\n\n\n\n(0.034)\n\n\nas.factor(album_release_year)2012\n−0.210***\n\n\n\n(0.024)\n\n\nas.factor(album_release_year)2016\n−0.224***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)2017\n−0.137***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2018\n−0.197***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2019\n−0.140***\n\n\n\n(0.020)\n\n\nas.factor(album_release_year)2020\n−0.213***\n\n\n\n(0.022)\n\n\nas.factor(album_release_year)2021\n−0.179***\n\n\n\n(0.021)\n\n\nas.factor(album_release_year)2022\n−0.233***\n\n\n\n(0.024)\n\n\nNum.Obs.\n1610\n\n\nR2\n0.272\n\n\nR2 Adj.\n0.256\n\n\nRMSE\n0.12\n\n\n\n + p\n\n\n\n\n\n\n\n\n\nThe Stones have generally been as danceable or less so since their first album.\n\n\ncode\n#plot avg dancability \n\nggplot() +\n  geom_density(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), aes(x=danceability))+\n  geom_density(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), aes(x=danceability), linetype=\"dotted\") +\n  annotate(\"text\", x=.18, y=2, label=\"Stones\")+\n  annotate(\"text\", x=.45, y=3.5, label=\"Taylor\") +\n  labs(x=\"Danceability\" , y=\"Density\")\n\n\n\n\n\n\n\n\n\n\n\nArtists over time\nBorrowing Mary L’s idea:\n\n\ncode\n#plot  dancability over time\n\nggplot() +\n  geom_smooth(data=bsum%&gt;%filter(artist_name==\"Taylor Swift\"), aes(y=danceability, x=album_release_year)) +\n  geom_smooth(data=bsum%&gt;%filter(artist_name==\"The Rolling Stones\"), aes(y=danceability, x=album_release_year)) +\n  annotate(\"text\", x=1980, y=.6, label=\"Stones\")+\n  annotate(\"text\", x=2017, y=.55, label=\"Taylor\") +\n  labs(x=\"Years\" , y=\"Danceability\")"
  }
]